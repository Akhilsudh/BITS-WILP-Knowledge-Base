{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Hello \ud83d\udc4b\ud83c\udffd # Welcome to my BITS WILP Knowledge Base. This is a personal note book for all the notes I take as part of my journey here at BITS. To know more about what a knowledge base is check this out. So what is the purpose of this? \ud83e\udd37\ud83c\udffd\u200d\u2642\ufe0f # My learning method includes taking a lot of notes through a local note taking tool called Obsidian . This website simply hosts that vault of Obsidian notes. The purpose of this page is to make the notes taken throughout my coursework accessible to myself and others in the same boat as me. Maybe this can help you too \ud83e\udd18\ud83c\udffd To know more about Obsidian and what it is, you can read about it in my blog article So what now? \ud83d\udc81\ud83c\udffd\u200d\u2642\ufe0f # Now that you know what this page is all about, dive into the !KBIndex page to enter the first index page to this knowledge base. :)","title":"Hello \ud83d\udc4b\ud83c\udffd"},{"location":"index.html#hello-","text":"Welcome to my BITS WILP Knowledge Base. This is a personal note book for all the notes I take as part of my journey here at BITS. To know more about what a knowledge base is check this out.","title":"Hello \ud83d\udc4b\ud83c\udffd"},{"location":"index.html#so-what-is-the-purpose-of-this-","text":"My learning method includes taking a lot of notes through a local note taking tool called Obsidian . This website simply hosts that vault of Obsidian notes. The purpose of this page is to make the notes taken throughout my coursework accessible to myself and others in the same boat as me. Maybe this can help you too \ud83e\udd18\ud83c\udffd To know more about Obsidian and what it is, you can read about it in my blog article","title":"So what is the purpose of this? \ud83e\udd37\ud83c\udffd\u200d\u2642\ufe0f"},{"location":"index.html#so-what-now-","text":"Now that you know what this page is all about, dive into the !KBIndex page to enter the first index page to this knowledge base. :)","title":"So what now? \ud83d\udc81\ud83c\udffd\u200d\u2642\ufe0f"},{"location":"about.html","text":"About Me \ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb # I am Akhil Sudhakaran, a full time software developer based in Bangalore, currently pursuing a masters program through BITS WILP. This Knowledge Base is hosted via . If you are interested in contributing, do check the above link out. My online presence #","title":"About Me \ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb"},{"location":"about.html#about-me-","text":"I am Akhil Sudhakaran, a full time software developer based in Bangalore, currently pursuing a masters program through BITS WILP. This Knowledge Base is hosted via . If you are interested in contributing, do check the above link out.","title":"About Me \ud83d\udc68\ud83c\udffd\u200d\ud83d\udcbb"},{"location":"about.html#my-online-presence","text":"","title":"My online presence"},{"location":"Knowledge%20Base/%21KBIndex.html","text":"Knowledge Base Index # This page has links to important resources. Title Link Induction Orientation InductionOrientation All Semesters Index !AllSemestersIndex All Question Papers QuestionPapers All Assignments Assignments","title":"Knowledge Base Index"},{"location":"Knowledge%20Base/%21KBIndex.html#knowledge-base-index","text":"This page has links to important resources. Title Link Induction Orientation InductionOrientation All Semesters Index !AllSemestersIndex All Question Papers QuestionPapers All Assignments Assignments","title":"Knowledge Base Index"},{"location":"Knowledge%20Base/InductionOrientation.html","text":"WILP Induction Orientation Details # The BITS WILP (Work Integrated earning Program) is exclusively meant for the working professionals to make them remain relevant, employable and grow in their chosen profession while contributing to their organization, sector and society. Platforms and services for accessing content # The BITS Work Integrated Learning Program has the following services for access to content from emails, to courses and other resources Service URL What it is for Impartus http://a.impartus.com/login/#/ Platform to join and watch live lectures that are conducted ELearn Portal https://elearn.bits-pilani.ac.in/ Single platform for Taxila courses, Old Materials and important announcements Email @wilp.bits-pilani.ac.in All Institution Communications Lab Platform http://bitscsis.vlabs.platifi.com Lab platform for running lab modules Apart from the above services, students also have access to Experimental Learning through Virtual Labs and Remote Labs. Students also have access to an eLibrary where one have access to academic publications (Through Open VPN) General Orientation # Grading Procedure # Students are graded with relative grading Letter grades Range from \\(A\\) , \\(A-\\) , \\(B\\) , \\(B-\\) , \\(C\\) , \\(C-\\) , \\(D\\) and \\(E\\) where each letter corresponds to grade points \\(10\\) , \\(9\\) , \\(8\\) , \\(7\\) , \\(6\\) , \\(5\\) , \\(4\\) and \\(2\\) respectively For project work a non-letter grade is awarded such as \\(EXCELLENT\\) , \\(GOOD\\) , \\(FAIR\\) and \\(POOR\\) . If a student is absent for any of the written examination, the student will be declared as a RRA (Required to Register Again) for that course. The CGPA awarded in a \\(10\\) point scale is calculated as: \\[ CGPA = {(U_1G_1 + U_2G_2 + U_3G_3 + U_4G_4)\\over(U1 + U2 + U3 + U4)} \\] Where \\(U_1\\) , \\(U_2\\) , \\(U_3\\) and \\(U_4\\) denote the Units associated with the courses taken \\(G_1\\) , \\(G_2\\) , \\(G_3\\) and \\(G_4\\) denote the Grade Points awarded in the respective courses. Non Letter Grades DO NOT go into CGPA Computation General Examination Schedules and Evaluative Components # Two exams are conducted throughout the semester. A mid semester Test A Comprehensive Exam Each of these two exams are conducted twice as a regular mode and a make up mode where one can take an examination at a separate date if one was not able to join the regular exam. Evaluative Components: # Evaluative Component Name Type Duration Session Weightage EC-1 Quizzes/Assignments/Labs Online As per course handouts As per course page ~20% EC-2 Mid-Semester Test Online Open Book 2 Hours Friday, Saturday and Sunday ~30% EC-3 Quizzes/Assignments/Labs Online Open Book 2 Hours Friday, Saturday and Sunday ~50% Program Specific Orientation (Software Systems with Data Analytics Specialization) # Course Structure # The MTech courses consists of 4 Core courses and 8 elective courses along with a dissertation. The Core courses are as follows: 1. Data Structures and Algorithms Design 2. Database Design and Applications 3. Distributed Computing 4. Software Architectures The General pool of electives are as follows: 1. Artifical Intelligence 2. Computer Organization 3. Distributed Data Systems 4. Software Engineering and Mangement 5. Usability Engineering 6. Object OPriented Analysis and Design The Specialization pool of electives for Data Analytics are as follows: 1. Adv. Statistical Techniques for Analytics 2. Applied Machine Learning 3. Metaheuristics for Optimization 4. Data Mining 5. Data Warehousing 6. Deep Learning 7. Information Retrieval 8. Mathematical Foundations for Data Science (This is a mandatory course for this specialization) 9. Natural Language Processing Semester Organization for Software Systems # Year Semester I Units Semester II Units I Data Structures & Algorithms Design 5 Software Architectures 5 DataBase Design & Applications 5 Elective 2 3(min) Distributed Computing 5 Elective 3 3(min) Elective 1 3(min) Elective 4 4(min) TOTAL 18(min) TOTAL 15(min) II Elective 5 3(min) Elective 6 3(min) Disertation 16 Elective 7 3(min) Elective 8 3(min) TOTAL 14(min) TOTAL 16","title":"WILP Induction Orientation Details"},{"location":"Knowledge%20Base/InductionOrientation.html#wilp-induction-orientation-details","text":"The BITS WILP (Work Integrated earning Program) is exclusively meant for the working professionals to make them remain relevant, employable and grow in their chosen profession while contributing to their organization, sector and society.","title":"WILP Induction Orientation Details"},{"location":"Knowledge%20Base/InductionOrientation.html#platforms-and-services-for-accessing-content","text":"The BITS Work Integrated Learning Program has the following services for access to content from emails, to courses and other resources Service URL What it is for Impartus http://a.impartus.com/login/#/ Platform to join and watch live lectures that are conducted ELearn Portal https://elearn.bits-pilani.ac.in/ Single platform for Taxila courses, Old Materials and important announcements Email @wilp.bits-pilani.ac.in All Institution Communications Lab Platform http://bitscsis.vlabs.platifi.com Lab platform for running lab modules Apart from the above services, students also have access to Experimental Learning through Virtual Labs and Remote Labs. Students also have access to an eLibrary where one have access to academic publications (Through Open VPN)","title":"Platforms and services for accessing content"},{"location":"Knowledge%20Base/InductionOrientation.html#general-orientation","text":"","title":"General Orientation"},{"location":"Knowledge%20Base/InductionOrientation.html#grading-procedure","text":"Students are graded with relative grading Letter grades Range from \\(A\\) , \\(A-\\) , \\(B\\) , \\(B-\\) , \\(C\\) , \\(C-\\) , \\(D\\) and \\(E\\) where each letter corresponds to grade points \\(10\\) , \\(9\\) , \\(8\\) , \\(7\\) , \\(6\\) , \\(5\\) , \\(4\\) and \\(2\\) respectively For project work a non-letter grade is awarded such as \\(EXCELLENT\\) , \\(GOOD\\) , \\(FAIR\\) and \\(POOR\\) . If a student is absent for any of the written examination, the student will be declared as a RRA (Required to Register Again) for that course. The CGPA awarded in a \\(10\\) point scale is calculated as: \\[ CGPA = {(U_1G_1 + U_2G_2 + U_3G_3 + U_4G_4)\\over(U1 + U2 + U3 + U4)} \\] Where \\(U_1\\) , \\(U_2\\) , \\(U_3\\) and \\(U_4\\) denote the Units associated with the courses taken \\(G_1\\) , \\(G_2\\) , \\(G_3\\) and \\(G_4\\) denote the Grade Points awarded in the respective courses. Non Letter Grades DO NOT go into CGPA Computation","title":"Grading Procedure"},{"location":"Knowledge%20Base/InductionOrientation.html#general-examination-schedules-and-evaluative-components","text":"Two exams are conducted throughout the semester. A mid semester Test A Comprehensive Exam Each of these two exams are conducted twice as a regular mode and a make up mode where one can take an examination at a separate date if one was not able to join the regular exam.","title":"General Examination Schedules and Evaluative Components"},{"location":"Knowledge%20Base/InductionOrientation.html#evaluative-components","text":"Evaluative Component Name Type Duration Session Weightage EC-1 Quizzes/Assignments/Labs Online As per course handouts As per course page ~20% EC-2 Mid-Semester Test Online Open Book 2 Hours Friday, Saturday and Sunday ~30% EC-3 Quizzes/Assignments/Labs Online Open Book 2 Hours Friday, Saturday and Sunday ~50%","title":"Evaluative Components:"},{"location":"Knowledge%20Base/InductionOrientation.html#program-specific-orientation-software-systems-with-data-analytics-specialization","text":"","title":"Program Specific Orientation (Software Systems with Data Analytics Specialization)"},{"location":"Knowledge%20Base/InductionOrientation.html#course-structure","text":"The MTech courses consists of 4 Core courses and 8 elective courses along with a dissertation. The Core courses are as follows: 1. Data Structures and Algorithms Design 2. Database Design and Applications 3. Distributed Computing 4. Software Architectures The General pool of electives are as follows: 1. Artifical Intelligence 2. Computer Organization 3. Distributed Data Systems 4. Software Engineering and Mangement 5. Usability Engineering 6. Object OPriented Analysis and Design The Specialization pool of electives for Data Analytics are as follows: 1. Adv. Statistical Techniques for Analytics 2. Applied Machine Learning 3. Metaheuristics for Optimization 4. Data Mining 5. Data Warehousing 6. Deep Learning 7. Information Retrieval 8. Mathematical Foundations for Data Science (This is a mandatory course for this specialization) 9. Natural Language Processing","title":"Course Structure"},{"location":"Knowledge%20Base/InductionOrientation.html#semester-organization-for-software-systems","text":"Year Semester I Units Semester II Units I Data Structures & Algorithms Design 5 Software Architectures 5 DataBase Design & Applications 5 Elective 2 3(min) Distributed Computing 5 Elective 3 3(min) Elective 1 3(min) Elective 4 4(min) TOTAL 18(min) TOTAL 15(min) II Elective 5 3(min) Elective 6 3(min) Disertation 16 Elective 7 3(min) Elective 8 3(min) TOTAL 14(min) TOTAL 16","title":"Semester Organization for Software Systems"},{"location":"Knowledge%20Base/All%20Semesters/%21AllSemestersIndex.html","text":"All Semesters Index # This page is an index for all semesters Semester Index Semster 1 !Semester1Index Semster 2 !Semester2Index Semster 3 !Semester3Index Semster 4","title":"All Semesters Index"},{"location":"Knowledge%20Base/All%20Semesters/%21AllSemestersIndex.html#all-semesters-index","text":"This page is an index for all semesters Semester Index Semster 1 !Semester1Index Semster 2 !Semester2Index Semster 3 !Semester3Index Semster 4","title":"All Semesters Index"},{"location":"Knowledge%20Base/All%20Semesters/Assignments.html","text":"Assignments # The list of all assignments of subjects I took as part of my WILP program Semester 1 # Distributed Computing DCAssignment Mathematical Foundation for Data Science MFDSAssignment1 MFDSAssignment2 Semester 2 # Advanced Statistics ASAssignment1 Applied Machine Learning AMLAssignment1 [[AMLAssignment2]] Object Oriented Analysis and Design OOADAssignment Software Architectures SAAssignment1 SAAssignment2 Semester 3 # Data Mining DMAssignment Deep Learning DLAssignment Natural Language Processing NLPAssignment tags: !KBIndex","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Assignments.html#assignments","text":"The list of all assignments of subjects I took as part of my WILP program","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Assignments.html#semester-1","text":"Distributed Computing DCAssignment Mathematical Foundation for Data Science MFDSAssignment1 MFDSAssignment2","title":"Semester 1"},{"location":"Knowledge%20Base/All%20Semesters/Assignments.html#semester-2","text":"Advanced Statistics ASAssignment1 Applied Machine Learning AMLAssignment1 [[AMLAssignment2]] Object Oriented Analysis and Design OOADAssignment Software Architectures SAAssignment1 SAAssignment2","title":"Semester 2"},{"location":"Knowledge%20Base/All%20Semesters/Assignments.html#semester-3","text":"Data Mining DMAssignment Deep Learning DLAssignment Natural Language Processing NLPAssignment tags: !KBIndex","title":"Semester 3"},{"location":"Knowledge%20Base/All%20Semesters/QuestionPapers.html","text":"Question Papers # The list of all question papers of subjects I took as part of my WILP program Semester 1 # Database Design and Applications Mid Sem Paper DDA End Sem Paper DDA Data-Structures and Algorithms Mid Sem Paper DSA End Sem Paper DSA Distributed Computing Mid Sem Paper DC End Sem Paper DC Mathematical Foundation for Data Science Mid Sem Paper MFDS End Sem Paper MFDS Semester 2 # Advanced Statistics Mid Sem Paper AS End Sem Paper AS Applied Machine Learning Mid Sem Paper AML End Sem Paper AML Object Oriented Analysis and Design Mid Sem Paper OOAD End Sem Paper OOAD Software Architectures Mid Sem Paper SA End Sem Paper SA Semester 3 # Data Mining Mid Sem Paper DM End Sem Paper DM Deep Learning Mid Sem Paper DL End Sem Paper DL Natural Language Processing Mid Sem Paper NLP End Sem Paper NLP tags: !KBIndex","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/QuestionPapers.html#question-papers","text":"The list of all question papers of subjects I took as part of my WILP program","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/QuestionPapers.html#semester-1","text":"Database Design and Applications Mid Sem Paper DDA End Sem Paper DDA Data-Structures and Algorithms Mid Sem Paper DSA End Sem Paper DSA Distributed Computing Mid Sem Paper DC End Sem Paper DC Mathematical Foundation for Data Science Mid Sem Paper MFDS End Sem Paper MFDS","title":"Semester 1"},{"location":"Knowledge%20Base/All%20Semesters/QuestionPapers.html#semester-2","text":"Advanced Statistics Mid Sem Paper AS End Sem Paper AS Applied Machine Learning Mid Sem Paper AML End Sem Paper AML Object Oriented Analysis and Design Mid Sem Paper OOAD End Sem Paper OOAD Software Architectures Mid Sem Paper SA End Sem Paper SA","title":"Semester 2"},{"location":"Knowledge%20Base/All%20Semesters/QuestionPapers.html#semester-3","text":"Data Mining Mid Sem Paper DM End Sem Paper DM Deep Learning Mid Sem Paper DL End Sem Paper DL Natural Language Processing Mid Sem Paper NLP End Sem Paper NLP tags: !KBIndex","title":"Semester 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/%21Semester1Index.html","text":"Semester 1 Index # Linked pages to subjects # Distributed Computing: !DistributedComputingIndex Datastructures and Algorithms: !DatastructuresAndAlgorithmsIndex Database Design and Application: !DatabaseDesignAndApplicationIndex Mathematical Foundation for Data Science: !MathematicalFoundationsIndex Lecturer Contact Details # Distributed Computing : Barsha Mitra Datastuctures and Algorithms : Pritam Bhattacharya Database Design and Application : Uma Maheswari Mathematical Foundation for Data Science : Instructor : G Venkiteswaran Instructor : TSL Radhika TA : Aparna Ramesh Kumar Tags: !AllSemestersIndex","title":"Semester 1 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/%21Semester1Index.html#semester-1-index","text":"","title":"Semester 1 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/%21Semester1Index.html#linked-pages-to-subjects","text":"Distributed Computing: !DistributedComputingIndex Datastructures and Algorithms: !DatastructuresAndAlgorithmsIndex Database Design and Application: !DatabaseDesignAndApplicationIndex Mathematical Foundation for Data Science: !MathematicalFoundationsIndex","title":"Linked pages to subjects"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/%21Semester1Index.html#lecturer-contact-details","text":"Distributed Computing : Barsha Mitra Datastuctures and Algorithms : Pritam Bhattacharya Database Design and Application : Uma Maheswari Mathematical Foundation for Data Science : Instructor : G Venkiteswaran Instructor : TSL Radhika TA : Aparna Ramesh Kumar Tags: !AllSemestersIndex","title":"Lecturer Contact Details"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/%21DatabaseDesignAndApplicationIndex.html","text":"Database Design and Application # Question Papers # Mid Sem Paper DDA End Sem Paper DDA Notes Index # This is an Index page for all Database Design and Application Content Week 1 Lecture Notes: Week1DBA Week 2 Lecture Notes: Week2DBA Week 3 Lecture Notes: Week3DBA Week 5 Lecture Notes: Week5DBA Week 6 Lecture Notes: Week6DBA Week 7 Lecture Notes: Week7DBA Week 8 Lecture Notes: Week8DBA Week 9 Lecture Notes: Week9DBA Week 10 Lecture Notes: Week10DBA Week 11 Lecture Notes: Week11DBA Note Summary: EmbeddedNotesDBA Tags: !Semester1Index","title":"Database Design and Application"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/%21DatabaseDesignAndApplicationIndex.html#database-design-and-application","text":"","title":"Database Design and Application"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/%21DatabaseDesignAndApplicationIndex.html#question-papers","text":"Mid Sem Paper DDA End Sem Paper DDA","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/%21DatabaseDesignAndApplicationIndex.html#notes-index","text":"This is an Index page for all Database Design and Application Content Week 1 Lecture Notes: Week1DBA Week 2 Lecture Notes: Week2DBA Week 3 Lecture Notes: Week3DBA Week 5 Lecture Notes: Week5DBA Week 6 Lecture Notes: Week6DBA Week 7 Lecture Notes: Week7DBA Week 8 Lecture Notes: Week8DBA Week 9 Lecture Notes: Week9DBA Week 10 Lecture Notes: Week10DBA Week 11 Lecture Notes: Week11DBA Note Summary: EmbeddedNotesDBA Tags: !Semester1Index","title":"Notes Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/EmbeddedNotesDBA.html","text":"Database Design and Application Notes #","title":"Database Design and Application Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/EmbeddedNotesDBA.html#database-design-and-application-notes","text":"","title":"Database Design and Application Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html","text":"End Sem DDA Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # tags: !DatabaseDesignAndApplicationIndex QuestionPapers","title":"End Sem DDA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#end-sem-dda-paper","text":"","title":"End Sem DDA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/End%20Sem%20Paper%20DDA.html#question-5","text":"tags: !DatabaseDesignAndApplicationIndex QuestionPapers","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html","text":"Mid Sem DDA Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !DatabaseDesignAndApplicationIndex QuestionPapers","title":"Mid Sem DDA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html#mid-sem-dda-paper","text":"","title":"Mid Sem DDA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Mid%20Sem%20Paper%20DDA.html#question-4","text":"tags: !DatabaseDesignAndApplicationIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html","text":"Week 10 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 23/Oct/2021 Topics Covered # Secondary Storage Devices # Placing File Records on Disk # Files Of Unordered Records # Files Of Ordered Records # Problem # DBMS Architecture # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 10"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#week-10","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 23/Oct/2021","title":"Week 10"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#secondary-storage-devices","text":"","title":"Secondary Storage Devices"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#placing-file-records-on-disk","text":"","title":"Placing File Records on Disk"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#files-of-unordered-records","text":"","title":"Files Of Unordered Records"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#files-of-ordered-records","text":"","title":"Files Of Ordered Records"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#problem","text":"","title":"Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week10DBA.html#dbms-architecture","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"DBMS Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html","text":"Week 11 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 30Oct/2021 2PL # Basic # All locks are obtained in growth phase and during shrinking phase all the unlocks are done Conservative # All locks are obtained before any transactions and all are unlocked after transactions Strict # Exclusive locks are unlocked after a commit, shared locks Rigourous # All locks are unlocked after a commit Problems on Locking # Basic 2PL Strict 2PL No growth or shrinking phase so not in 2PL Deadlock # Problems # No deadlocks Deadlock happens because the two processes are waiting for each other Tags: !DatabaseDesignAndApplicationIndex","title":"Week 11"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#week-11","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 30Oct/2021","title":"Week 11"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#2pl","text":"","title":"2PL"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#basic","text":"All locks are obtained in growth phase and during shrinking phase all the unlocks are done","title":"Basic"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#conservative","text":"All locks are obtained before any transactions and all are unlocked after transactions","title":"Conservative"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#strict","text":"Exclusive locks are unlocked after a commit, shared locks","title":"Strict"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#rigourous","text":"All locks are unlocked after a commit","title":"Rigourous"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#problems-on-locking","text":"Basic 2PL Strict 2PL No growth or shrinking phase so not in 2PL","title":"Problems on Locking"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#deadlock","text":"","title":"Deadlock"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week11DBA.html#problems","text":"No deadlocks Deadlock happens because the two processes are waiting for each other Tags: !DatabaseDesignAndApplicationIndex","title":"Problems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html","text":"Week 1 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 24/Jul/2021 Topics Covered # Intro to DBMS Advantages of DBMS Operations of DBMS # Database catalogue or dictionary is the metadata used by DBMS to save the information about the type of data, structures and constraints of the data. Constructing the database is the process of storing the data on some stoprage and medium that is controlled by the DBMS Manipulating a DB includes functions such as querying the DB to retrieve specidic data, updating the DB and querying the DB Sharing the db regfers to sharing it via a server access to DB - Embedded SQL ( EXEC block in C programming language) - Create a special API to call SQL commands (JDBC, ODBC) - Allow external code to be executed from within SQL How to design database # Three layer Architecture # Each layer handles two issues External Layer # The above shows two views generated Conceptual layer # shows stored data in terms of the data model of the DBMS Ina relational DBMS the conceptial schema descibes all relations that are stored in the DB This couse is aimed at this level Physical Layer: # FM -> DM -> BM All data is stored in the form of pages, and the file manager keeps track of these The Disk manager brings it to memory 1. Specify additional storage details 2. File organization 3. ETL (Extract Transform Load) # Pysicall data independence # irrespective of where the data is Mapping # Allow for external code to be executed from within SQL EXECUTE can be used to run external scripts example python protection Sys protection (Against hw and sw) sec protection (against unauth) Characteristics of DB # self describing nature : there exists a data called meta data that describes the structure database. This is used by DBMS software and DB users who need infor about db structure Insulation between programs and data : There exists a data abstraction. The interface that allows for a program to access the data in the form of functions Support for multiple views : subsets of the database, contains vcirtual data derived from databse files but is not explicitly stored. Multi User DBMS allow users to use distinct apps and must provide facilities to create multiple views Sharing of data concurrency control software(make sure that multi user changes on data is controlled), Online transaction processing (OLTP) application Transactions are rolled back since it is not commited Example architecture # Data independence : all schemas are logical and the actual data is stored in bots in disk, so they both are inherently separate Mapping : The act of mapping two levels in a 3 level DBMS System catalog # It contains information such as: - user accounts/default settings - priveledges - performance stats - object sizing - object growth - table str - index str - table constraints - user sessions - internal db settings - location of DB files CRUD # Create Read Update Delete ERP and CRM # History # DBMS WORKERS or Users # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#week-1","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 24/Jul/2021","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#topics-covered","text":"Intro to DBMS Advantages of DBMS","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#operations-of-dbms","text":"Database catalogue or dictionary is the metadata used by DBMS to save the information about the type of data, structures and constraints of the data. Constructing the database is the process of storing the data on some stoprage and medium that is controlled by the DBMS Manipulating a DB includes functions such as querying the DB to retrieve specidic data, updating the DB and querying the DB Sharing the db regfers to sharing it via a server access to DB - Embedded SQL ( EXEC block in C programming language) - Create a special API to call SQL commands (JDBC, ODBC) - Allow external code to be executed from within SQL","title":"Operations of DBMS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#how-to-design-database","text":"","title":"How to design database"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#three-layer-architecture","text":"Each layer handles two issues","title":"Three layer Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#external-layer","text":"The above shows two views generated","title":"External Layer"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#conceptual-layer","text":"shows stored data in terms of the data model of the DBMS Ina relational DBMS the conceptial schema descibes all relations that are stored in the DB This couse is aimed at this level","title":"Conceptual layer"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#physical-layer","text":"FM -> DM -> BM All data is stored in the form of pages, and the file manager keeps track of these The Disk manager brings it to memory 1. Specify additional storage details 2. File organization 3.","title":"Physical Layer:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#etl-extract-transform-load","text":"","title":"ETL (Extract Transform Load)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#pysicall-data-independence","text":"irrespective of where the data is","title":"Pysicall data independence"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#mapping","text":"Allow for external code to be executed from within SQL EXECUTE can be used to run external scripts example python protection Sys protection (Against hw and sw) sec protection (against unauth)","title":"Mapping"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#characteristics-of-db","text":"self describing nature : there exists a data called meta data that describes the structure database. This is used by DBMS software and DB users who need infor about db structure Insulation between programs and data : There exists a data abstraction. The interface that allows for a program to access the data in the form of functions Support for multiple views : subsets of the database, contains vcirtual data derived from databse files but is not explicitly stored. Multi User DBMS allow users to use distinct apps and must provide facilities to create multiple views Sharing of data concurrency control software(make sure that multi user changes on data is controlled), Online transaction processing (OLTP) application Transactions are rolled back since it is not commited","title":"Characteristics of DB"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#example-architecture","text":"Data independence : all schemas are logical and the actual data is stored in bots in disk, so they both are inherently separate Mapping : The act of mapping two levels in a 3 level DBMS","title":"Example architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#system-catalog","text":"It contains information such as: - user accounts/default settings - priveledges - performance stats - object sizing - object growth - table str - index str - table constraints - user sessions - internal db settings - location of DB files","title":"System catalog"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#crud","text":"Create Read Update Delete","title":"CRUD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#erp-and-crm","text":"","title":"ERP and CRM"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#history","text":"","title":"History"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week1DBA.html#dbms-workers-or-users","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"DBMS WORKERS or Users"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html","text":"Week 2 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 31/Jul/2021 Topics Covered # Intro to DBMS Advantages of DBMS Mapping # Traditional Multimedia dbs Geographical (GIS) Data warehouses (OLAP) (analyze useful business information from very large databses, support descision making) Real time and active DB Time series db (volatile stocks/financial data) Database design process # Database design phases # 1. Requirements specification 1. Produces data reqs and functional reqs 2. Conceptual design 1. We develop an ER model or UML class diagram 2. We describe different entities, their relation and their attributes 3. Logical design 1. Based on the ER diagrams created, the designers create 2. Primary and foreign keys are determined 3. Normalization is to eliminate redundancy and potential update anomalies. 4. Physical design 1. This phase is to develop the database 2. Decide the datatypes 3. The SQL clauses are written to ccreate the DBs Example : Conceptual database design # Domains, Atttributes, Tuples, and Relations # Entity Relationship Diagram # Entity in ER # Example of Entity # Attributes in ER # Simple and composite : Simple cannot be split, it is atomic Composite can be divided into smaller sub parts (It is a combination of simple attributes example address) Single valued and multivalued : Single valued means there is only one value for that column (Example name) Multivalued means there are more than one value for that column (Example Phone numbers) Stored attributes and Derived attribute : Attributes from which other attributes can be derived is a stored attribute (Example birth date, job join date) This will be stored in the DB Derived attribute are attributes that are derived from stored attributes (Example age from birth date, experience from job joining date) This value would not be stored in the DB Key Attributes Relationship between the physical design to the conceptual design # Table -> Entity Type Row \u2192 Entity/Tuple Column \u2192 Domain Set of all rows \u2192 Entity Set Drawing attributes # flightid: Composite id and single valued Rollno: key attribute since every STUDENT will have a unique value Tags: !DatabaseDesignAndApplicationIndex","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#week-2","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 31/Jul/2021","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#topics-covered","text":"Intro to DBMS Advantages of DBMS","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#mapping","text":"Traditional Multimedia dbs Geographical (GIS) Data warehouses (OLAP) (analyze useful business information from very large databses, support descision making) Real time and active DB Time series db (volatile stocks/financial data)","title":"Mapping"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#database-design-process","text":"","title":"Database design process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#database-design-phases","text":"1. Requirements specification 1. Produces data reqs and functional reqs 2. Conceptual design 1. We develop an ER model or UML class diagram 2. We describe different entities, their relation and their attributes 3. Logical design 1. Based on the ER diagrams created, the designers create 2. Primary and foreign keys are determined 3. Normalization is to eliminate redundancy and potential update anomalies. 4. Physical design 1. This phase is to develop the database 2. Decide the datatypes 3. The SQL clauses are written to ccreate the DBs Example :","title":"Database design phases"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#conceptual-database-design","text":"","title":"Conceptual database design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#domains-atttributes-tuples-and-relations","text":"","title":"Domains, Atttributes, Tuples, and Relations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#entity-relationship-diagram","text":"","title":"Entity Relationship Diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#entity-in-er","text":"","title":"Entity in ER"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#example-of-entity","text":"","title":"Example of Entity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#attributes-in-er","text":"Simple and composite : Simple cannot be split, it is atomic Composite can be divided into smaller sub parts (It is a combination of simple attributes example address) Single valued and multivalued : Single valued means there is only one value for that column (Example name) Multivalued means there are more than one value for that column (Example Phone numbers) Stored attributes and Derived attribute : Attributes from which other attributes can be derived is a stored attribute (Example birth date, job join date) This will be stored in the DB Derived attribute are attributes that are derived from stored attributes (Example age from birth date, experience from job joining date) This value would not be stored in the DB Key Attributes","title":"Attributes in ER"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#relationship-between-the-physical-design-to-the-conceptual-design","text":"Table -> Entity Type Row \u2192 Entity/Tuple Column \u2192 Domain Set of all rows \u2192 Entity Set","title":"Relationship between the physical design to the conceptual design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week2DBA.html#drawing-attributes","text":"flightid: Composite id and single valued Rollno: key attribute since every STUDENT will have a unique value Tags: !DatabaseDesignAndApplicationIndex","title":"Drawing attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html","text":"Week 3 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 07/Aug/2021 Topics Covered # Answer to questions from previous class Answer to questions from previous class # Q1 : Answer Name -> Simple Age -> Derived addr -> Composite pho -> Simple, Multi Valued DOB -> Stored Q2 : Answer Since there are two addresses, the attribute is multivalued. and since the address itself is a composite attribute, the Employee addr is a composite multivalued Q3 : Answer We know that \\(P_k \\in C_k \\in S_k\\) 1. Super Key \\(S_k\\) -> BookID, BookID-Name, BookID-Author, Name-Author, BookID-Name-Author 2. Candidate Key \\(C_k\\) -> BookID, Name-Author 3. Primary Key \\(P_k\\) -> BookID Q4 : Answer 1. Candidate Key \\(C_k\\) -> PAN, Licence#-address 2. Primary Key \\(P_k\\) -> PAN Q5 : Answer K1 is the Pk since it has the least composition Q6 : Answer Composite Key attributes -> Custid-orderid, Name-addr Relationship # graph LR A[Employee]---B{Works For} B---C[Department] In the above diagram, the two entities are having the relationship 'works for'. Relationship Properties # Degree of relationship # graph LR A[SALESASSIST]---B{SELLS} B---C[PRODUCT] B---D[CUSTOMER] In the above example the degree of the SELLS relationship is 3 graph LR A[Employee]---B{Works For} B---C[Department] In the above example the degree of the Works For relation is 2 graph LR A[EMPLOYEE]-- supervisor ---B{SUPERVISION} B-- supervisee ---A The above relation is a unary relationship where the relationship is with itself Role Name # In case of Unary relationship, we give a role name to the entities. For example in the supervision relationship, there are some employees who are supervisor and some will be supervisee. Here supervisor and supervisee are the role names Mapping Constraints # Cardinality Constraints # 1 to 1 relationship graph LR A[Employee]-- 1 ---B{Manages} B-- 1 ---C[Department] N to 1 relationship graph LR A[Employee]-- N ---B{Works For} B-- 1 ---C[Department] N employees work on M to N relationship graph LR A[Employee]-- M ---B{Works On} B-- N ---C[Projects] M employees can work on 1 project or 1 employee can work on N projects Participation Constraints # How many people are participating in a relationship Total Participation graph LR A[Employee]-- N ---B{Works For} B-- 1 ---C[Department] All employess participate in a department But not all departments need to have \\(Cardinality + Optionality = multiplicity\\) Relationship with attributes: # Consider the following 1 to 1 case: graph LR A[Employee]-- 1 ---B{Manages} B-- 1 ---C[Department] B---D([Start-Date]) Start date attribute can show when the employee managed a department, hence the start date can be in either of the entity set Consider the following N to 1 case: graph LR A[Employee]-- N ---B{Work For} B-- 1 ---C[Department] B---D([Start-Date]) Here start date is associated to every employee , since start date is unique for every employee but not the other way around, start date can be in employee table. Consider the following M to N case: graph LR A[Employee]-- M ---B{Work On} B-- N ---C[Project] B---D([Hours]) In the above case, the hour data must be in the relationship table since it cannot be associated to either employee or a project tables. MIN-MAX constraint # graph LR A[Employee]-- N ---B{Work For} B-- 1 ---C[Department] In the above relation min-max form for Employee is {1,N} and for department is {0,1} Entity Types # Weak and Strong # - An entity that does not have a key attribute is a weak entity (Denoted by double rectangle) - And a strong one has a key - A double diamond relationship denotes a relationship between a strong and weak entity - In the above example the Dependents entity does not have a key, hence it uses the employee id as the key - Driver license entity cannot exist without person entity ER Model # Refinement Invese refinement Binary and Ternary Relationships # Example of entities having more than 1 relationships # Removing redundant relationship # Aggregation # Questions Answered # Q1 Answer Taught during Q2 graph LR A[Employee]-- M ---B{Work On} B-- N ---C[Project] B---D([Hours]) Q5 Tags: !DatabaseDesignAndApplicationIndex","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#week-3","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 07/Aug/2021","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#topics-covered","text":"Answer to questions from previous class","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#answer-to-questions-from-previous-class","text":"Q1 : Answer Name -> Simple Age -> Derived addr -> Composite pho -> Simple, Multi Valued DOB -> Stored Q2 : Answer Since there are two addresses, the attribute is multivalued. and since the address itself is a composite attribute, the Employee addr is a composite multivalued Q3 : Answer We know that \\(P_k \\in C_k \\in S_k\\) 1. Super Key \\(S_k\\) -> BookID, BookID-Name, BookID-Author, Name-Author, BookID-Name-Author 2. Candidate Key \\(C_k\\) -> BookID, Name-Author 3. Primary Key \\(P_k\\) -> BookID Q4 : Answer 1. Candidate Key \\(C_k\\) -> PAN, Licence#-address 2. Primary Key \\(P_k\\) -> PAN Q5 : Answer K1 is the Pk since it has the least composition Q6 : Answer Composite Key attributes -> Custid-orderid, Name-addr","title":"Answer to questions from previous class"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#relationship","text":"graph LR A[Employee]---B{Works For} B---C[Department] In the above diagram, the two entities are having the relationship 'works for'.","title":"Relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#relationship-properties","text":"","title":"Relationship Properties"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#degree-of-relationship","text":"graph LR A[SALESASSIST]---B{SELLS} B---C[PRODUCT] B---D[CUSTOMER] In the above example the degree of the SELLS relationship is 3 graph LR A[Employee]---B{Works For} B---C[Department] In the above example the degree of the Works For relation is 2 graph LR A[EMPLOYEE]-- supervisor ---B{SUPERVISION} B-- supervisee ---A The above relation is a unary relationship where the relationship is with itself","title":"Degree of relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#role-name","text":"In case of Unary relationship, we give a role name to the entities. For example in the supervision relationship, there are some employees who are supervisor and some will be supervisee. Here supervisor and supervisee are the role names","title":"Role Name"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#mapping-constraints","text":"","title":"Mapping Constraints"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#cardinality-constraints","text":"1 to 1 relationship graph LR A[Employee]-- 1 ---B{Manages} B-- 1 ---C[Department] N to 1 relationship graph LR A[Employee]-- N ---B{Works For} B-- 1 ---C[Department] N employees work on M to N relationship graph LR A[Employee]-- M ---B{Works On} B-- N ---C[Projects] M employees can work on 1 project or 1 employee can work on N projects","title":"Cardinality Constraints"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#participation-constraints","text":"How many people are participating in a relationship Total Participation graph LR A[Employee]-- N ---B{Works For} B-- 1 ---C[Department] All employess participate in a department But not all departments need to have \\(Cardinality + Optionality = multiplicity\\)","title":"Participation Constraints"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#relationship-with-attributes","text":"Consider the following 1 to 1 case: graph LR A[Employee]-- 1 ---B{Manages} B-- 1 ---C[Department] B---D([Start-Date]) Start date attribute can show when the employee managed a department, hence the start date can be in either of the entity set Consider the following N to 1 case: graph LR A[Employee]-- N ---B{Work For} B-- 1 ---C[Department] B---D([Start-Date]) Here start date is associated to every employee , since start date is unique for every employee but not the other way around, start date can be in employee table. Consider the following M to N case: graph LR A[Employee]-- M ---B{Work On} B-- N ---C[Project] B---D([Hours]) In the above case, the hour data must be in the relationship table since it cannot be associated to either employee or a project tables.","title":"Relationship with attributes:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#min-max-constraint","text":"graph LR A[Employee]-- N ---B{Work For} B-- 1 ---C[Department] In the above relation min-max form for Employee is {1,N} and for department is {0,1}","title":"MIN-MAX constraint"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#entity-types","text":"","title":"Entity Types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#weak-and-strong","text":"- An entity that does not have a key attribute is a weak entity (Denoted by double rectangle) - And a strong one has a key - A double diamond relationship denotes a relationship between a strong and weak entity - In the above example the Dependents entity does not have a key, hence it uses the employee id as the key - Driver license entity cannot exist without person entity","title":"Weak and Strong"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#er-model","text":"Refinement Invese refinement","title":"ER Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#binary-and-ternary-relationships","text":"","title":"Binary and Ternary Relationships"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#example-of-entities-having-more-than-1-relationships","text":"","title":"Example of entities having more than 1 relationships"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#removing-redundant-relationship","text":"","title":"Removing redundant relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#aggregation","text":"","title":"Aggregation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week3DBA.html#questions-answered","text":"Q1 Answer Taught during Q2 graph LR A[Employee]-- M ---B{Work On} B-- N ---C[Project] B---D([Hours]) Q5 Tags: !DatabaseDesignAndApplicationIndex","title":"Questions Answered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html","text":"Week 5 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 21/Aug/2021 Topics Covered # Data Model and Schema Relational Model (Logical Data Model) Characteristics and concepts of relational model Relational Schema Diagram Example Data Model and Schema # data model captures what kind of data is stored how it must be organized etc Data model has a description of datam data semantics and consistency constraints of data There are 3 different types of data models: conceptual data models, logical data models, and physical data models and each has it's own purpose: 1. Conceptual : ER, EER and UML diagrams used to represent how data is organized. We define scope and business concepts and rules 2. Logical : How the system is implemented independent of the DBMS. Will be used by data architects and analysts. The purpose ius to develop technical map of rules and data structures (Schema, it is a plan on how the database needs to be implemented) 3. Physical : This describes how the system will be implemented using a specific DBMS system. This model is typically created by DBA and developers. The files that are used to save tables are part of this model Relational Model (Logical Data Model) # It is based upon the mathematical concepts of relations, set theory and predicate logic A relation is a set of values. Domain : Domain D is a set of atomic values By atomic we mean that each value in the domain is indivisible Method of specifying a domain is to specify a data type USA_Phone_number: The set of ten digit phone numbers valid in US SSN: Set of valid nine digit SSNs Sex: a member of the set {female, male} GPA: A real number between 0.0 to 4.0 Names: The set of character strings that represent names of person The above rules are called logical definitions if domains A domain is thus given a name, data type and a format. Attribute : Attributes are columns for a given entity type and have a name The domain for an attribute \\(A\\) is given as \\(dom(A)\\) and it holds all the values that the attribute can hold Tuple : A Row in an entity is called a tuple The mapping of attributes and its values Example {Name -> \"Akhil Sudhakaran\", Sex -> Male, IQ -> 9000} Relational Schema : Relational schema is given by \\(R\\) , and it is the relation's name. It is denoted by \\(R(A_1, A_2, A_3,...,A_n)\\) Each attribute \\(A_i\\) is identified by \\(dom(A_i)\\) The degree of a relation is the number of attributes of its relation Using data type of each attributem the definition is sometimes written as: \\[STUDENT(Name: string,\\ Ssn: string,\\ Home\\_phone: string,\\ Address: string,\\ Office\\_phone: string,\\ Age: integer,\\ Gpa: real)\\] Attribute \\(A_i\\) can be qualified with the relation name \\(R\\) to which it belongs by using the dot notation \\(R.A_i\\) , for example \\(STUDENT.name\\) Relation State : Denoted by \\(r(R)\\) , is a set if n-tuples \\(r = \\{t_1, t_2, t_3, ... , t_n\\}\\) Each tuple \\(t\\) is an ordered list of n values \\(t=<v_1, v_2, ... , v_n>\\) Each \\(v_i\\) is an element of \\(dom(A_i)\\) It is possible for several attributes to have the same domain. The domain 'USA_phone_numbers' can be used for the attributes 'Home_phone' and 'Office_phone' as well Relation : A named set of tuples all of the same form (Having same set of attributes). The term table is a loose synonym A relation state is a subset of the Cartesian product of the domains defining the relation schema Relational Database : A collection if relations, each one needs to have a logical relationship, then that collection is called relational database Characteristics and concepts of relational model # A relation is a set of tuples and have no duplicates and have no order Ordering of tuples cannot be possible Logically the tuples have no order Physically in disk there can be an order When we display a relation as a table, the rows are displated in a vertain ordeer by different attributes Ordering if attributes: A tuple is best viewed as a mapping from its attributes. The null value: Used for dont know, not applicable or value undefined etc. For example if an attribute Phone_number is not available then we can store the value NULL to denote that Composite and multivalued attributes are not allowed Values of attributes: All values must be atomic and must not contain composite or multi valued. r(R) < dom(A_1) x dom(A_2) x ... x dom(A_n) R: Schema of relation r of R: a specific value or popuilation if R R is also calledf the intension of relation Let S_1 = {0, 1} Let S_2 - {a, b, c} Let R < S1 x S2 then for example: r(R) = {<0, a>, <0, b>, <1, c>}. There are a lot of constraints or restrictions on the actual valuies in a datrabase state Constraints on DBB can generally be divided: Implicit constraints : constraints in the data model.No two tupples can have duplicate values The salary of employee cannot exceed the salary of the supervisor Maximum number of hours an employee can work in a week is 56 Triggers and assertions can be used to enforce semantic based constraints schema based : Constraints expressed in schema, typically done in DDL Domain Constraints: Within each tuple, the value of each attribute \\(A\\) must be atomic. Each attribute must be either null or must be from the \\(dom(A)\\) Some DBMS allow to impost a not null constraint as well Key constraint : Means no two tuples can have the same combination if values for all their attributes Subsets of attributes of a relation in schema \\(R\\) with the property that no two tuples in any relation state r of R is called a super key. Whenever a super key has redundant values, they are removed and a candidate key is generated. One from this candidate key is the primary key. These keys are underlined in the relational schema Relation database state \\(S\\) is a set of relational schema \\(S = \\{R_1, R_2, ... , R_m\\}\\) A snapshot of the relationship database is the data at the current relation schema Integrity constraint : This states that no primary key value can be NULL Referential Integrity Constraint : This is implemented for foreign keys Referential Integrity Violation : When foreign key is updated and that foreign key is not a PK of the foreign tuple When primary key is updated and the entities that refer to this PK as FK must also be updated application based Relational Schema Diagram Example # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#week-5","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 21/Aug/2021","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#topics-covered","text":"Data Model and Schema Relational Model (Logical Data Model) Characteristics and concepts of relational model Relational Schema Diagram Example","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#data-model-and-schema","text":"data model captures what kind of data is stored how it must be organized etc Data model has a description of datam data semantics and consistency constraints of data There are 3 different types of data models: conceptual data models, logical data models, and physical data models and each has it's own purpose: 1. Conceptual : ER, EER and UML diagrams used to represent how data is organized. We define scope and business concepts and rules 2. Logical : How the system is implemented independent of the DBMS. Will be used by data architects and analysts. The purpose ius to develop technical map of rules and data structures (Schema, it is a plan on how the database needs to be implemented) 3. Physical : This describes how the system will be implemented using a specific DBMS system. This model is typically created by DBA and developers. The files that are used to save tables are part of this model","title":"Data Model and Schema"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#relational-model-logical-data-model","text":"It is based upon the mathematical concepts of relations, set theory and predicate logic A relation is a set of values. Domain : Domain D is a set of atomic values By atomic we mean that each value in the domain is indivisible Method of specifying a domain is to specify a data type USA_Phone_number: The set of ten digit phone numbers valid in US SSN: Set of valid nine digit SSNs Sex: a member of the set {female, male} GPA: A real number between 0.0 to 4.0 Names: The set of character strings that represent names of person The above rules are called logical definitions if domains A domain is thus given a name, data type and a format. Attribute : Attributes are columns for a given entity type and have a name The domain for an attribute \\(A\\) is given as \\(dom(A)\\) and it holds all the values that the attribute can hold Tuple : A Row in an entity is called a tuple The mapping of attributes and its values Example {Name -> \"Akhil Sudhakaran\", Sex -> Male, IQ -> 9000} Relational Schema : Relational schema is given by \\(R\\) , and it is the relation's name. It is denoted by \\(R(A_1, A_2, A_3,...,A_n)\\) Each attribute \\(A_i\\) is identified by \\(dom(A_i)\\) The degree of a relation is the number of attributes of its relation Using data type of each attributem the definition is sometimes written as: \\[STUDENT(Name: string,\\ Ssn: string,\\ Home\\_phone: string,\\ Address: string,\\ Office\\_phone: string,\\ Age: integer,\\ Gpa: real)\\] Attribute \\(A_i\\) can be qualified with the relation name \\(R\\) to which it belongs by using the dot notation \\(R.A_i\\) , for example \\(STUDENT.name\\) Relation State : Denoted by \\(r(R)\\) , is a set if n-tuples \\(r = \\{t_1, t_2, t_3, ... , t_n\\}\\) Each tuple \\(t\\) is an ordered list of n values \\(t=<v_1, v_2, ... , v_n>\\) Each \\(v_i\\) is an element of \\(dom(A_i)\\) It is possible for several attributes to have the same domain. The domain 'USA_phone_numbers' can be used for the attributes 'Home_phone' and 'Office_phone' as well Relation : A named set of tuples all of the same form (Having same set of attributes). The term table is a loose synonym A relation state is a subset of the Cartesian product of the domains defining the relation schema Relational Database : A collection if relations, each one needs to have a logical relationship, then that collection is called relational database","title":"Relational Model (Logical Data Model)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#characteristics-and-concepts-of-relational-model","text":"A relation is a set of tuples and have no duplicates and have no order Ordering of tuples cannot be possible Logically the tuples have no order Physically in disk there can be an order When we display a relation as a table, the rows are displated in a vertain ordeer by different attributes Ordering if attributes: A tuple is best viewed as a mapping from its attributes. The null value: Used for dont know, not applicable or value undefined etc. For example if an attribute Phone_number is not available then we can store the value NULL to denote that Composite and multivalued attributes are not allowed Values of attributes: All values must be atomic and must not contain composite or multi valued. r(R) < dom(A_1) x dom(A_2) x ... x dom(A_n) R: Schema of relation r of R: a specific value or popuilation if R R is also calledf the intension of relation Let S_1 = {0, 1} Let S_2 - {a, b, c} Let R < S1 x S2 then for example: r(R) = {<0, a>, <0, b>, <1, c>}. There are a lot of constraints or restrictions on the actual valuies in a datrabase state Constraints on DBB can generally be divided: Implicit constraints : constraints in the data model.No two tupples can have duplicate values The salary of employee cannot exceed the salary of the supervisor Maximum number of hours an employee can work in a week is 56 Triggers and assertions can be used to enforce semantic based constraints schema based : Constraints expressed in schema, typically done in DDL Domain Constraints: Within each tuple, the value of each attribute \\(A\\) must be atomic. Each attribute must be either null or must be from the \\(dom(A)\\) Some DBMS allow to impost a not null constraint as well Key constraint : Means no two tuples can have the same combination if values for all their attributes Subsets of attributes of a relation in schema \\(R\\) with the property that no two tuples in any relation state r of R is called a super key. Whenever a super key has redundant values, they are removed and a candidate key is generated. One from this candidate key is the primary key. These keys are underlined in the relational schema Relation database state \\(S\\) is a set of relational schema \\(S = \\{R_1, R_2, ... , R_m\\}\\) A snapshot of the relationship database is the data at the current relation schema Integrity constraint : This states that no primary key value can be NULL Referential Integrity Constraint : This is implemented for foreign keys Referential Integrity Violation : When foreign key is updated and that foreign key is not a PK of the foreign tuple When primary key is updated and the entities that refer to this PK as FK must also be updated application based","title":"Characteristics and concepts of relational model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week5DBA.html#relational-schema-diagram-example","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"Relational Schema Diagram Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html","text":"Week 6 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 21/Aug/2021 Topics Covered # Logical Design Phase Logical Design Phase # A summary of design phases: Converting strong entity types: # Each entity type becomes a table single valued attributes becomes a column Derived attributes are ignored Composite attributes are represented by components Multivalued attributes are represented by a separate table The key attributes of the entry type becomes the primary key of the table Entity Example 1 # - Here address is a composite attribute - Years of service is a derived attribute so can be ignored - Skill set is a multivalued attribute The converted relational schema for the above example: Employee ( E# , Name, Door_No, Street, City, Pincode, Date_Of_Joining) Emp_Skillset( E# , Skillset) Entity Example 2 # - Weak entity types are converted into a table oif their own, with the primary key of the strong entity acting as a foreign key in the table - This foreign key along with the key of the weak entity form the composite primary key of this table The converted relational schema for the above example: Employee ( E# , ...) Dependant( E# , Dependant_ID , Name, Address) Converting relationships: # Binary 1:1 # Case 1: Combination of participation types # - The primary key of the partial participant will become the foreign key of the total participant - The manager employee for the department will be a foreign key wrt Employee table The converted relational schema: Employee( E# , Name, ....) Department( Dept# , Name, .... , MgrE#) Case 2: Uniform participation types # The converted relational schema: Employee( E# , Name, ....) Chair( Item# , Model, Location, used_by) Employee( E# , Name, .... , Sits_on) Chair( Item# , Model, Location) Binary 1:N # - Teacher in Subject table is the foreign key for Teacher entity The converted relational schema: Teacher( ID , Name, Telephone, ....) Subject( Code , Name, .... , Teacher ) Binary M:N # - The relation is moved to a separate table - This table uses the PK of the other two tables as the foreign keys The converted relational schema: Student( Sid# , Title, .... ) Enrolls( Sid# , C# ) Course( C# , CName, .... ) Self Referencing Binary 1:1 # - E# is the primary key - Spouse is the foreign key to employee table The converted relational schema: Employee( E# , Name, .... , Spouse ) Self Referencing Binary 1:N # - Manager is the FK to the table Employee The converted relational schema: Employee( E# , Name, .... , Manager ) Self Referencing Binary M:N # - Guarantor, Beneficiary are FK to the table Employee The converted relational schema: Employee( E# , Name, ....) Guarantr( Guarantor , Beneficiary ) Ternary Relationship # The converted relational schema: Prescription ( Doctor# , Patient# , Medicine# ) ER Constructs to relations: # SuperSSN : FK to Employee table that shows supervisor WorksOn: This relationship is it's own table since it is M:N Supervising_DNO : FK to the Dept table that shows the supervising department Works_For_DNO : FK to the Dept table that shows the supervising department Employee( SSN , Name, Sex, .... , SuperSSN , Works_For_DNO ) Dept( DNO , Name, ManagerSSN , start_date) Dept_locations( DNO , Location) Project( PNO , Name, Location, Supervising_DNO ) Dependent(Name, Sex, DOB, Relationship Employee_SSN , ) WorksOn( PNO , SSN , hours) Ternary relationship conversion example: # EER to Relational Mapping Steps # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#week-6","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 21/Aug/2021","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#topics-covered","text":"Logical Design Phase","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#logical-design-phase","text":"A summary of design phases:","title":"Logical Design Phase"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#converting-strong-entity-types","text":"Each entity type becomes a table single valued attributes becomes a column Derived attributes are ignored Composite attributes are represented by components Multivalued attributes are represented by a separate table The key attributes of the entry type becomes the primary key of the table","title":"Converting strong entity types:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#entity-example-1","text":"- Here address is a composite attribute - Years of service is a derived attribute so can be ignored - Skill set is a multivalued attribute The converted relational schema for the above example: Employee ( E# , Name, Door_No, Street, City, Pincode, Date_Of_Joining) Emp_Skillset( E# , Skillset)","title":"Entity Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#entity-example-2","text":"- Weak entity types are converted into a table oif their own, with the primary key of the strong entity acting as a foreign key in the table - This foreign key along with the key of the weak entity form the composite primary key of this table The converted relational schema for the above example: Employee ( E# , ...) Dependant( E# , Dependant_ID , Name, Address)","title":"Entity Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#converting-relationships","text":"","title":"Converting relationships:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#binary-11","text":"","title":"Binary 1:1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#case-1-combination-of-participation-types","text":"- The primary key of the partial participant will become the foreign key of the total participant - The manager employee for the department will be a foreign key wrt Employee table The converted relational schema: Employee( E# , Name, ....) Department( Dept# , Name, .... , MgrE#)","title":"Case 1: Combination of participation types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#case-2-uniform-participation-types","text":"The converted relational schema: Employee( E# , Name, ....) Chair( Item# , Model, Location, used_by) Employee( E# , Name, .... , Sits_on) Chair( Item# , Model, Location)","title":"Case 2: Uniform participation types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#binary-1n","text":"- Teacher in Subject table is the foreign key for Teacher entity The converted relational schema: Teacher( ID , Name, Telephone, ....) Subject( Code , Name, .... , Teacher )","title":"Binary 1:N"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#binary-mn","text":"- The relation is moved to a separate table - This table uses the PK of the other two tables as the foreign keys The converted relational schema: Student( Sid# , Title, .... ) Enrolls( Sid# , C# ) Course( C# , CName, .... )","title":"Binary M:N"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#self-referencing-binary-11","text":"- E# is the primary key - Spouse is the foreign key to employee table The converted relational schema: Employee( E# , Name, .... , Spouse )","title":"Self Referencing Binary 1:1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#self-referencing-binary-1n","text":"- Manager is the FK to the table Employee The converted relational schema: Employee( E# , Name, .... , Manager )","title":"Self Referencing Binary 1:N"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#self-referencing-binary-mn","text":"- Guarantor, Beneficiary are FK to the table Employee The converted relational schema: Employee( E# , Name, ....) Guarantr( Guarantor , Beneficiary )","title":"Self Referencing Binary M:N"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#ternary-relationship","text":"The converted relational schema: Prescription ( Doctor# , Patient# , Medicine# )","title":"Ternary Relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#er-constructs-to-relations","text":"SuperSSN : FK to Employee table that shows supervisor WorksOn: This relationship is it's own table since it is M:N Supervising_DNO : FK to the Dept table that shows the supervising department Works_For_DNO : FK to the Dept table that shows the supervising department Employee( SSN , Name, Sex, .... , SuperSSN , Works_For_DNO ) Dept( DNO , Name, ManagerSSN , start_date) Dept_locations( DNO , Location) Project( PNO , Name, Location, Supervising_DNO ) Dependent(Name, Sex, DOB, Relationship Employee_SSN , ) WorksOn( PNO , SSN , hours)","title":"ER Constructs to relations:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#ternary-relationship-conversion-example","text":"","title":"Ternary relationship conversion example:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week6DBA.html#eer-to-relational-mapping-steps","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"EER to Relational Mapping Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html","text":"Week 7 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 4/Sep/2021 Topics Covered # Query Languages Relational Algebra SELECT Operation PROJECT Operation TYPE COMPATIBILITY UNION Operation INTERSECTION Operation SET DIFFERENCE Operation Some properties for UNION, INTERSECTION and SET DIFFERENCE operations CARTESIAN (Cross Product) Operation RENAME Operation JOIN Operation OUTER UNION Operation Complete Set of Relational Operations DIVISION Operation Aggregate Functions and Grouping Operation Tutorial Session Post Lecture Notes Query Languages # - Procedural: What to do and how to do - Non Procedural: What to do Relational Algebra # REFERENCE LECTURE Consider the example schema Which is populated with these data The basic set of operations for the relational model is known as the relational algebra These operations enable a user to specify basic retrieval requests The reult of a retreival is a new relation, which may have been formed from on or more relations. A sequence of relational algebra operations forms a relational algebra expression, whose result will also be a relation SELECT Operation # Select operation is used to select a subset of the tuples from a relation that satisfy a selection condition. The select operation is denoted by a sigma letter \\(\\sigma_{DNO} = 4 (EMPLOYEE)\\) (This represents all employees in department 4) \\(\\sigma_{SALARY} \\gt 30000 (EMPLOYEE)\\) (This represents all employees in with salary greater than 30000) Properties # SELECT is commutative A cascade SEKLEECT operation may be applied in any order \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(R))\\) \\(=\\) \\(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 1>}(R))\\) A cascade SELECT operation can be \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\) \\(=\\) \\(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(\\sigma_{<condition\\ 1>}(R)))\\) A cascade can also be written as: \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\) \\(=\\) \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\) Example # PROJECT Operation # This operation selects certain columns from the table and discards the rest. This is shown as the pi letter \\(\\pi_{LNAME,\\ FNAME,\\ SALARY}(EMPLOYEE)\\) Properties # The number iof tuples in the result of projection \\(\\pi_{<list>}(R)\\) is always less or equal to the number of tuples in R If the list of attributes Example # Combination of SELECT and PROJECT TYPE COMPATIBILITY # The operand relations must have the same number of attributes and the domains of the corresponding attributes must be compatible UNION Operation # Denoted as \\(R \\cup S\\) , means that all the tuples of \\(R\\) and \\(S\\) will be accumulated together and any duplicates are thrown away Example # INTERSECTION Operation # Denoted as \\(R \\cap S\\) , means that common tuples of \\(R\\) and \\(S\\) will be accumulated together and others are thrown away Examples # Consider the same example as the UNION operation, then intersection is: SET DIFFERENCE Operation # The result of this operation denoted by \\(R - S\\) is a relation that includes all tuples that are in \\(R\\) but not in \\(S\\) Here order matters, meaning \\(R - S\\) need not be the same as \\(S - R\\) Examples # Some properties for UNION, INTERSECTION and SET DIFFERENCE operations # \\(R \\cup S = S \\cup R\\) \\(R \\cap S = S \\cap R\\) CARTESIAN (Cross Product) Operation # Cartesian product combines each and every row of the two relations. It is denoted by \\(R x S\\) Examples # RENAME Operation # Name conflicts can arise in some situations Examples # JOIN Operation # Sequence of CARTESIAN Operation followed by a SELECT Types of JOIN Operations # Conditional Join # In the employee example the condition is \\(EMPLOYEE.EMP\\_CODE = SALARY.EMP\\_CODE\\) Natural Join # denoted as \\(*\\) Join such that the tuples are equal on all the common attribute names A Complete Example: Equi Join # A join where the only comparison operator used is \\(=\\) . In the result of an EQUIJOIN we always have one or more pairs THETA Join # Apart from \\(=\\) all the other comparison operators are also used OUTER Join # Left Outer Join: Keeps every tuple in the first/left relation \\(R\\) ; if no matching tuple is found in \\(S\\) , then attributes of \\(S\\) are given null values Right Outer Join: Keeps every tuple in the second/right relation \\(S\\) ; if no matching tuple is found in \\(R\\) , then attributes of \\(R\\) are given null values Full outer join: Keeps every tuple in both the relations \\(R\\) and \\(S\\) and the rest are padded with \\(null\\) OUTER UNION Operation # Complete Set of Relational Operations # DIVISION Operation # denoted by \\(\\div\\) Examples # Another example Aggregate Functions and Grouping Operation # Tutorial Session # Post Lecture Notes # Go through the SQL slides for the next week Go through the example problems in the book for relational algebra Tags: !DatabaseDesignAndApplicationIndex","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#week-7","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 4/Sep/2021","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#topics-covered","text":"Query Languages Relational Algebra SELECT Operation PROJECT Operation TYPE COMPATIBILITY UNION Operation INTERSECTION Operation SET DIFFERENCE Operation Some properties for UNION, INTERSECTION and SET DIFFERENCE operations CARTESIAN (Cross Product) Operation RENAME Operation JOIN Operation OUTER UNION Operation Complete Set of Relational Operations DIVISION Operation Aggregate Functions and Grouping Operation Tutorial Session Post Lecture Notes","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#query-languages","text":"- Procedural: What to do and how to do - Non Procedural: What to do","title":"Query Languages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#relational-algebra","text":"REFERENCE LECTURE Consider the example schema Which is populated with these data The basic set of operations for the relational model is known as the relational algebra These operations enable a user to specify basic retrieval requests The reult of a retreival is a new relation, which may have been formed from on or more relations. A sequence of relational algebra operations forms a relational algebra expression, whose result will also be a relation","title":"Relational Algebra"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#select-operation","text":"Select operation is used to select a subset of the tuples from a relation that satisfy a selection condition. The select operation is denoted by a sigma letter \\(\\sigma_{DNO} = 4 (EMPLOYEE)\\) (This represents all employees in department 4) \\(\\sigma_{SALARY} \\gt 30000 (EMPLOYEE)\\) (This represents all employees in with salary greater than 30000)","title":"SELECT Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#properties","text":"SELECT is commutative A cascade SEKLEECT operation may be applied in any order \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(R))\\) \\(=\\) \\(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 1>}(R))\\) A cascade SELECT operation can be \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\) \\(=\\) \\(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(\\sigma_{<condition\\ 1>}(R)))\\) A cascade can also be written as: \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\) \\(=\\) \\(\\sigma_{<condition\\ 1>}(\\sigma_{<condition\\ 2>}(\\sigma_{<condition\\ 3>}(R)))\\)","title":"Properties"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#example","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#project-operation","text":"This operation selects certain columns from the table and discards the rest. This is shown as the pi letter \\(\\pi_{LNAME,\\ FNAME,\\ SALARY}(EMPLOYEE)\\)","title":"PROJECT Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#properties_1","text":"The number iof tuples in the result of projection \\(\\pi_{<list>}(R)\\) is always less or equal to the number of tuples in R If the list of attributes","title":"Properties"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#example_1","text":"Combination of SELECT and PROJECT","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#type-compatibility","text":"The operand relations must have the same number of attributes and the domains of the corresponding attributes must be compatible","title":"TYPE COMPATIBILITY"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#union-operation","text":"Denoted as \\(R \\cup S\\) , means that all the tuples of \\(R\\) and \\(S\\) will be accumulated together and any duplicates are thrown away","title":"UNION Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#example_2","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#intersection-operation","text":"Denoted as \\(R \\cap S\\) , means that common tuples of \\(R\\) and \\(S\\) will be accumulated together and others are thrown away","title":"INTERSECTION Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#examples","text":"Consider the same example as the UNION operation, then intersection is:","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#set-difference-operation","text":"The result of this operation denoted by \\(R - S\\) is a relation that includes all tuples that are in \\(R\\) but not in \\(S\\) Here order matters, meaning \\(R - S\\) need not be the same as \\(S - R\\)","title":"SET DIFFERENCE Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#examples_1","text":"","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#some-properties-for-union-intersection-and-set-difference-operations","text":"\\(R \\cup S = S \\cup R\\) \\(R \\cap S = S \\cap R\\)","title":"Some properties for UNION, INTERSECTION and SET DIFFERENCE operations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#cartesian-cross-product-operation","text":"Cartesian product combines each and every row of the two relations. It is denoted by \\(R x S\\)","title":"CARTESIAN (Cross Product) Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#examples_2","text":"","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#rename-operation","text":"Name conflicts can arise in some situations","title":"RENAME Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#examples_3","text":"","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#join-operation","text":"Sequence of CARTESIAN Operation followed by a SELECT","title":"JOIN Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#types-of-join-operations","text":"","title":"Types of JOIN Operations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#conditional-join","text":"In the employee example the condition is \\(EMPLOYEE.EMP\\_CODE = SALARY.EMP\\_CODE\\)","title":"Conditional Join"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#natural-join","text":"denoted as \\(*\\) Join such that the tuples are equal on all the common attribute names A Complete Example:","title":"Natural Join"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#equi-join","text":"A join where the only comparison operator used is \\(=\\) . In the result of an EQUIJOIN we always have one or more pairs","title":"Equi Join"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#theta-join","text":"Apart from \\(=\\) all the other comparison operators are also used","title":"THETA Join"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#outer-join","text":"Left Outer Join: Keeps every tuple in the first/left relation \\(R\\) ; if no matching tuple is found in \\(S\\) , then attributes of \\(S\\) are given null values Right Outer Join: Keeps every tuple in the second/right relation \\(S\\) ; if no matching tuple is found in \\(R\\) , then attributes of \\(R\\) are given null values Full outer join: Keeps every tuple in both the relations \\(R\\) and \\(S\\) and the rest are padded with \\(null\\)","title":"OUTER Join"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#outer-union-operation","text":"","title":"OUTER UNION Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#complete-set-of-relational-operations","text":"","title":"Complete Set of Relational Operations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#division-operation","text":"denoted by \\(\\div\\)","title":"DIVISION Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#examples_4","text":"Another example","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#aggregate-functions-and-grouping-operation","text":"","title":"Aggregate Functions and Grouping Operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#tutorial-session","text":"","title":"Tutorial Session"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week7DBA.html#post-lecture-notes","text":"Go through the SQL slides for the next week Go through the example problems in the book for relational algebra Tags: !DatabaseDesignAndApplicationIndex","title":"Post Lecture Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html","text":"Week 8 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 11/Sep/2021 Topics Covered # Tutorial RA # Relational Algebra Introduction Tuple Relational Calculus # Example 1 # Example 2 # Example 3 # Question 1 # Question 2 # Question 3 # Question 1 # Question 2 # Question 3 # SQL Programming # Introduction # Different commercial SQL versions # SQL statements # Primary Key Constraint # Foreign Key Constraint # Example 1 - Employee DB # Example 2 - Order DB # VIEWS # Example 3 Sailor DB # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#week-8","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 11/Sep/2021","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#tutorial-ra","text":"Relational Algebra Introduction","title":"Tutorial RA"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#tuple-relational-calculus","text":"","title":"Tuple Relational Calculus"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-1","text":"","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-2","text":"","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-3","text":"","title":"Example 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-1_1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-2_1","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#question-3_1","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#sql-programming","text":"","title":"SQL Programming"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#introduction","text":"","title":"Introduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#different-commercial-sql-versions","text":"","title":"Different commercial SQL versions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#sql-statements","text":"","title":"SQL statements"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#primary-key-constraint","text":"","title":"Primary Key Constraint"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#foreign-key-constraint","text":"","title":"Foreign Key Constraint"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-1---employee-db","text":"","title":"Example 1 - Employee DB"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-2---order-db","text":"","title":"Example 2 - Order DB"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#views","text":"","title":"VIEWS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week8DBA.html#example-3-sailor-db","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"Example 3 Sailor DB"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html","text":"Week 9 # Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 16/Oct/2021 Topics Covered # Normalization or Logical Design # Functional Dependencies # Here you can see that the \\(roll_no\\) is a functional dependency because it can be used to get other values Key Terms # Armstrong's axioms/properties of functional dependencies # Functional Dependency # Multivalued Functional Dependency # Trivial Functional Dependency # Non Trivial Functional Dependency # Transitive Functional Dependency # Keys # Primary Keys # Foreign Keys # Foreign Keys help in maintaining the Referential Integrity between tables Compound Keys # Composite Keys # Surrogate Key # The DBMS automatically creates a surrogate key Normalization # Tutorial # Normal Forms # 1NF # In a functional dependency if the RHS has only one attribute then it is in 1NF Another way to do this is as follows: Employee Details: EMP_ID EMP_NAME EMP_STATE Employee Phone Numbers: EMP_ID PH_NO 2NF # 3NF # Summary of 1, 2, 3 NF # BCNF # 4NF # 5NF # Spurious Tuples # Lossless Decomposition # Extraneous Attributes # Example 1 # \\(R(A, B, C)\\) \\(F: \\{A \\rightarrow B, B \\rightarrow C\\}\\) Since: 1. \\(A^+ \\implies \\{A, B, C\\}\\) , hence we can say that \\(A\\) is a \\(CK\\) 2. \\(B^+ \\implies \\{B, C\\}\\) , since all other attributes cannot be derived, \\(B\\) cannot be a \\(CK\\) 3. \\(C^+ \\implies \\{C\\}\\) , since all other attributes cannot be derived, \\(C\\) cannot be a \\(CK\\) Example 2 # \\(R(A, B, C, D, E)\\) \\(F: \\{A \\rightarrow D, BC \\rightarrow A, BC \\rightarrow D, C \\rightarrow B, E \\rightarrow A, E \\rightarrow D\\}\\) \\(B^+ \\implies \\{B\\}\\) \\(C^+ \\implies \\{C, B, A, D\\}\\) Since C covers all attributes we can remove the extraneous attributes from: \\(BC \\rightarrow A, BC \\rightarrow D\\) to \\(C \\rightarrow A, C \\rightarrow D\\) Canonical/Minimal Cover Problem # RHS must be singleton Extraneous attributes must be resolved Remove redundant FD Example # \\(R(A, B, C)\\) \\(F: \\{A \\rightarrow B, AB \\rightarrow C\\}\\) 1. Both relations have a singleton RHS 2. The second one has extraneous attributes \\(A^+ \\implies \\{ABC\\}\\) \\(B^+ \\implies \\{B\\}\\) Since A covers B we can reduce the second relation to: \\(A \\rightarrow C\\) so the minimal cover is: \\(F: \\{A \\rightarrow B, A \\rightarrow C\\}\\) Example # \\(R(A, B, C, D)\\) \\(F: \\{A \\rightarrow B, AB \\rightarrow C, D \\rightarrow AC, D \\rightarrow E\\}\\) Does \\(F\\) cover \\(G\\) ? \\(G: \\{A \\rightarrow BC, D \\rightarrow AB\\}\\) For \\(F\\) # Decomposing to singleton relations: \\(A \\rightarrow B\\) \\(AB \\rightarrow C\\) \\(D \\rightarrow A\\) \\(D \\rightarrow C\\) Resolve Extraneous attributes: \\(AB \\rightarrow C\\) \\(A^+ \\implies \\{ABC\\}\\) \\(B^+ \\implies \\{B\\}\\) new \\(F\\) : \\(\\{A \\rightarrow B, A \\rightarrow C, D \\rightarrow A, D \\rightarrow C, D \\rightarrow E\\}\\) Redundant FD Since the dependency \\(D \\rightarrow C\\) is not needed we can drop that dependency Final \\(F\\) : \\(\\{A \\rightarrow B, A \\rightarrow C, D \\rightarrow A, D \\rightarrow E\\}\\) For \\(G\\) # Normalization Problems # Tags: !DatabaseDesignAndApplicationIndex","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#week-9","text":"Lecturer : Uma Maheswari, Faculty for BITS Pilani WILP Date : 16/Oct/2021","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#normalization-or-logical-design","text":"","title":"Normalization or Logical Design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#functional-dependencies","text":"Here you can see that the \\(roll_no\\) is a functional dependency because it can be used to get other values","title":"Functional Dependencies"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#key-terms","text":"","title":"Key Terms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#armstrongs-axiomsproperties-of-functional-dependencies","text":"","title":"Armstrong's axioms/properties of functional dependencies"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#functional-dependency","text":"","title":"Functional Dependency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#multivalued-functional-dependency","text":"","title":"Multivalued Functional Dependency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#trivial-functional-dependency","text":"","title":"Trivial Functional Dependency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#non-trivial-functional-dependency","text":"","title":"Non Trivial Functional Dependency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#transitive-functional-dependency","text":"","title":"Transitive Functional Dependency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#keys","text":"","title":"Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#primary-keys","text":"","title":"Primary Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#foreign-keys","text":"Foreign Keys help in maintaining the Referential Integrity between tables","title":"Foreign Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#compound-keys","text":"","title":"Compound Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#composite-keys","text":"","title":"Composite Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#surrogate-key","text":"The DBMS automatically creates a surrogate key","title":"Surrogate Key"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#normalization","text":"","title":"Normalization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#tutorial","text":"","title":"Tutorial"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#normal-forms","text":"","title":"Normal Forms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#1nf","text":"In a functional dependency if the RHS has only one attribute then it is in 1NF Another way to do this is as follows: Employee Details: EMP_ID EMP_NAME EMP_STATE Employee Phone Numbers: EMP_ID PH_NO","title":"1NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#2nf","text":"","title":"2NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#3nf","text":"","title":"3NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#summary-of-1-2-3-nf","text":"","title":"Summary of 1, 2, 3 NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#bcnf","text":"","title":"BCNF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#4nf","text":"","title":"4NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#5nf","text":"","title":"5NF"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#spurious-tuples","text":"","title":"Spurious Tuples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#lossless-decomposition","text":"","title":"Lossless Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#extraneous-attributes","text":"","title":"Extraneous Attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#example-1","text":"\\(R(A, B, C)\\) \\(F: \\{A \\rightarrow B, B \\rightarrow C\\}\\) Since: 1. \\(A^+ \\implies \\{A, B, C\\}\\) , hence we can say that \\(A\\) is a \\(CK\\) 2. \\(B^+ \\implies \\{B, C\\}\\) , since all other attributes cannot be derived, \\(B\\) cannot be a \\(CK\\) 3. \\(C^+ \\implies \\{C\\}\\) , since all other attributes cannot be derived, \\(C\\) cannot be a \\(CK\\)","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#example-2","text":"\\(R(A, B, C, D, E)\\) \\(F: \\{A \\rightarrow D, BC \\rightarrow A, BC \\rightarrow D, C \\rightarrow B, E \\rightarrow A, E \\rightarrow D\\}\\) \\(B^+ \\implies \\{B\\}\\) \\(C^+ \\implies \\{C, B, A, D\\}\\) Since C covers all attributes we can remove the extraneous attributes from: \\(BC \\rightarrow A, BC \\rightarrow D\\) to \\(C \\rightarrow A, C \\rightarrow D\\)","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#canonicalminimal-cover-problem","text":"RHS must be singleton Extraneous attributes must be resolved Remove redundant FD","title":"Canonical/Minimal Cover Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#example","text":"\\(R(A, B, C)\\) \\(F: \\{A \\rightarrow B, AB \\rightarrow C\\}\\) 1. Both relations have a singleton RHS 2. The second one has extraneous attributes \\(A^+ \\implies \\{ABC\\}\\) \\(B^+ \\implies \\{B\\}\\) Since A covers B we can reduce the second relation to: \\(A \\rightarrow C\\) so the minimal cover is: \\(F: \\{A \\rightarrow B, A \\rightarrow C\\}\\)","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#example_1","text":"\\(R(A, B, C, D)\\) \\(F: \\{A \\rightarrow B, AB \\rightarrow C, D \\rightarrow AC, D \\rightarrow E\\}\\) Does \\(F\\) cover \\(G\\) ? \\(G: \\{A \\rightarrow BC, D \\rightarrow AB\\}\\)","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#for-f","text":"Decomposing to singleton relations: \\(A \\rightarrow B\\) \\(AB \\rightarrow C\\) \\(D \\rightarrow A\\) \\(D \\rightarrow C\\) Resolve Extraneous attributes: \\(AB \\rightarrow C\\) \\(A^+ \\implies \\{ABC\\}\\) \\(B^+ \\implies \\{B\\}\\) new \\(F\\) : \\(\\{A \\rightarrow B, A \\rightarrow C, D \\rightarrow A, D \\rightarrow C, D \\rightarrow E\\}\\) Redundant FD Since the dependency \\(D \\rightarrow C\\) is not needed we can drop that dependency Final \\(F\\) : \\(\\{A \\rightarrow B, A \\rightarrow C, D \\rightarrow A, D \\rightarrow E\\}\\)","title":"For \\(F\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#for-g","text":"","title":"For \\(G\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Database%20Design%20and%20Application/Week9DBA.html#normalization-problems","text":"Tags: !DatabaseDesignAndApplicationIndex","title":"Normalization Problems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/%21DatastructuresAndAlgorithmsIndex.html","text":"Datastructure and Algorithm # Question Papers # Mid Sem Paper DSA End Sem Paper DSA Notes Index # This is an Index page for all Distributed Computing Content Week 1 Lecture Notes: Week1DSA Week 2 Lecture Notes: Week2DSA Week 4 Lecture Notes: Week4DSA Lecture Notes: Week4DSA2 Week 5 Lecture Notes: Week5DSA Lecture Notes: Week5DSA2 Week 6 Lecture Notes: Week6DSA Lecture Notes: Week6DSA2 Week 7 Lecture Notes: Week7DSA Week 8 Lecture Notes: Week8DSA Lecture Notes: Week9DSA Week 9 Lecture Notes: Week11DSA Week 10 Lecture Notes: Week12DSA3 Note Summary: EmbeddedNotesDSA Tags: !Semester1Index","title":"Datastructure and Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/%21DatastructuresAndAlgorithmsIndex.html#datastructure-and-algorithm","text":"","title":"Datastructure and Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/%21DatastructuresAndAlgorithmsIndex.html#question-papers","text":"Mid Sem Paper DSA End Sem Paper DSA","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/%21DatastructuresAndAlgorithmsIndex.html#notes-index","text":"This is an Index page for all Distributed Computing Content Week 1 Lecture Notes: Week1DSA Week 2 Lecture Notes: Week2DSA Week 4 Lecture Notes: Week4DSA Lecture Notes: Week4DSA2 Week 5 Lecture Notes: Week5DSA Lecture Notes: Week5DSA2 Week 6 Lecture Notes: Week6DSA Lecture Notes: Week6DSA2 Week 7 Lecture Notes: Week7DSA Week 8 Lecture Notes: Week8DSA Lecture Notes: Week9DSA Week 9 Lecture Notes: Week11DSA Week 10 Lecture Notes: Week12DSA3 Note Summary: EmbeddedNotesDSA Tags: !Semester1Index","title":"Notes Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/EmbeddedNotesDSA.html","text":"Data Structures And Algorithms Notes #","title":"Data Structures And Algorithms Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/EmbeddedNotesDSA.html#data-structures-and-algorithms-notes","text":"","title":"Data Structures And Algorithms Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html","text":"End Sem DSA Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DatastructuresAndAlgorithmsIndex QuestionPapers","title":"End Sem DSA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#end-sem-dsa-paper","text":"","title":"End Sem DSA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/End%20Sem%20Paper%20DSA.html#question-7","text":"tags: !DatastructuresAndAlgorithmsIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html","text":"Mid Sem DSA Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DatastructuresAndAlgorithmsIndex QuestionPapers","title":"Mid Sem DSA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#mid-sem-dsa-paper","text":"","title":"Mid Sem DSA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Mid%20Sem%20Paper%20DSA.html#question-7","text":"tags: !DatastructuresAndAlgorithmsIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week11DSA.html","text":"Week 11 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 30/Oct/2021 Linked List # The linked list has a node where the node has a data and a pointer component that points to the next node (can be a node or null if it ends in this node) struct Node { int data struct Node * next } ADT Operations # first() -> This function returns the head of the linked list first () { return head ; } last() -> This function returns the last element of the linked list last () { if ( head == NULL ) { return NULL } struct Node * temp = head while ( temp -> next != NULL ) { temp = temp -> next } return temp } before(p) -> This function returns the previous node to a given node p before ( struct Node * p ) { if ( p == head || p == NULL ) { return NULL } struct Node * curr = head while ( curr -> next != p ) { curr = curr -> next } return curr } after(p) -> This function returns the next node after ( struct Node * p ) { if ( p == NULL ) { return NULL } return p -> next } insertBefore(p, key) -> Insert a node before p with the key value insertBefore ( struct Node * p , int key ) { if ( head == NULL || p == NULL ) { return ERROR } struct Node * newNode ; newNode -> data = key newNode -> next = pos struct node * curr = head while ( curr -> next != p && curr -> next != NULL ) { curr = curr -> next } if ( curr -> next == NULL ) { return ERROR } curr -> next = newNode } insertAfter(p, key) -> Insert a node with value key to a position after p insertAfter(struct Node* p, int key) { if(p == NULL) { return ERROR } struct Node* newNode newNode -> data = key newNode -> next = p -> next p -> next = newNode } remove(p) -> remove the node from the linked list remove(struct Node* p){ if(p == NULL || head == NULL) { return NULL } struct Node* curr = head while(curr -> next != p && curr -> next != NULL) { curr = curr -> next } if(curr -> next == NULL) { return NULL } curr -> next = p -> next return p } Double Linked list # In a doubly linked list the Node contains the next node as well as the previous node as well along with the data. The struct would look somthing like this: struct Node { int data struct Node * next struct Node * prev } The ADT functions can now utilize the previous pointer for reference in operations such as insert before and insert after and one must also make sure to correctly set the previous and next pointers as well. Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 11"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week11DSA.html#week-11","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 30/Oct/2021","title":"Week 11"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week11DSA.html#linked-list","text":"The linked list has a node where the node has a data and a pointer component that points to the next node (can be a node or null if it ends in this node) struct Node { int data struct Node * next }","title":"Linked List"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week11DSA.html#adt-operations","text":"first() -> This function returns the head of the linked list first () { return head ; } last() -> This function returns the last element of the linked list last () { if ( head == NULL ) { return NULL } struct Node * temp = head while ( temp -> next != NULL ) { temp = temp -> next } return temp } before(p) -> This function returns the previous node to a given node p before ( struct Node * p ) { if ( p == head || p == NULL ) { return NULL } struct Node * curr = head while ( curr -> next != p ) { curr = curr -> next } return curr } after(p) -> This function returns the next node after ( struct Node * p ) { if ( p == NULL ) { return NULL } return p -> next } insertBefore(p, key) -> Insert a node before p with the key value insertBefore ( struct Node * p , int key ) { if ( head == NULL || p == NULL ) { return ERROR } struct Node * newNode ; newNode -> data = key newNode -> next = pos struct node * curr = head while ( curr -> next != p && curr -> next != NULL ) { curr = curr -> next } if ( curr -> next == NULL ) { return ERROR } curr -> next = newNode } insertAfter(p, key) -> Insert a node with value key to a position after p insertAfter(struct Node* p, int key) { if(p == NULL) { return ERROR } struct Node* newNode newNode -> data = key newNode -> next = p -> next p -> next = newNode } remove(p) -> remove the node from the linked list remove(struct Node* p){ if(p == NULL || head == NULL) { return NULL } struct Node* curr = head while(curr -> next != p && curr -> next != NULL) { curr = curr -> next } if(curr -> next == NULL) { return NULL } curr -> next = p -> next return p }","title":"ADT Operations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week11DSA.html#double-linked-list","text":"In a doubly linked list the Node contains the next node as well as the previous node as well along with the data. The struct would look somthing like this: struct Node { int data struct Node * next struct Node * prev } The ADT functions can now utilize the previous pointer for reference in operations such as insert before and insert after and one must also make sure to correctly set the previous and next pointers as well. Tags: !DatastructuresAndAlgorithmsIndex","title":"Double Linked list"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html","text":"Week 12 (cont.) # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 5/Nov/2021 Hashing # Example for the hashing function taken \\(h(k) = k\\ \\%\\ 7\\) Finding an array # find ( x ) { pos = x % 7 if ( T [ pos ] == x ) { return True } else { return False } } Insert to an array # insert ( x ) { pos = x % 7 if ( T [ pos ] = EMPTY ) { T [ pos ] = x } else { # Handle Collision } } Open Addressing Hashing # Finding in open addressing # find ( x ) { pos = x % 7 currNode = T [ pos ] while ( currNode != NULL ) { if ( currNode -> data == x ) { return True } curNode = currNode -> next } return False } Insertion in open addressing # insert ( x ) { pos = x % 7 struct Node * newNode newNode -> data = x newNode -> next = NULL if ( T [ pos ] == NULL ) { T [ pos ] = newNode } else { currNode = T [ pos ] while ( currNode -> next != NULL ) { currNode = currNode -> next } currNode -> next = newNode } } Linear Probing # When a collision is reached, what we end up doing is if there is a collision, we find the next hash value as so: \\(h(k, 0) = k\\ \\%\\ 7\\) \\(h(k, 1) = (h(k) + 1 )\\ \\%\\ 7\\) \\(...\\) \\(h(k, n) = (h(k) + n )\\ \\%\\ 7\\) Insertion # insert ( k ) { i = 0 pos = (( k % 7 ) + i ) % 7 while ( T [ pos ] != EMPTY ) { i += 1 pos = (( k % 7 ) + i ) % 7 if ( pos == k % 7 ) { return \"ERROR! HASHTABLE IS FULL\" } } T [ pos ] = k } Finding # insert ( k ) { i = 0 pos = (( k % 7 ) + i ) % 7 while ( T [ pos ] != k ) { i += 1 pos = (( k % 7 ) + i ) % 7 if ( T [ pos ] == EMPTY || pos == k % 7 ) { return \"NOT FOUND IN HASHTABLE\" } } return pos } Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 12 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#week-12-cont","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 5/Nov/2021","title":"Week 12 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#hashing","text":"Example for the hashing function taken \\(h(k) = k\\ \\%\\ 7\\)","title":"Hashing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#finding-an-array","text":"find ( x ) { pos = x % 7 if ( T [ pos ] == x ) { return True } else { return False } }","title":"Finding an array"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#insert-to-an-array","text":"insert ( x ) { pos = x % 7 if ( T [ pos ] = EMPTY ) { T [ pos ] = x } else { # Handle Collision } }","title":"Insert to an array"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#open-addressing-hashing","text":"","title":"Open Addressing Hashing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#finding-in-open-addressing","text":"find ( x ) { pos = x % 7 currNode = T [ pos ] while ( currNode != NULL ) { if ( currNode -> data == x ) { return True } curNode = currNode -> next } return False }","title":"Finding in open addressing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#insertion-in-open-addressing","text":"insert ( x ) { pos = x % 7 struct Node * newNode newNode -> data = x newNode -> next = NULL if ( T [ pos ] == NULL ) { T [ pos ] = newNode } else { currNode = T [ pos ] while ( currNode -> next != NULL ) { currNode = currNode -> next } currNode -> next = newNode } }","title":"Insertion in open addressing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#linear-probing","text":"When a collision is reached, what we end up doing is if there is a collision, we find the next hash value as so: \\(h(k, 0) = k\\ \\%\\ 7\\) \\(h(k, 1) = (h(k) + 1 )\\ \\%\\ 7\\) \\(...\\) \\(h(k, n) = (h(k) + n )\\ \\%\\ 7\\)","title":"Linear Probing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#insertion","text":"insert ( k ) { i = 0 pos = (( k % 7 ) + i ) % 7 while ( T [ pos ] != EMPTY ) { i += 1 pos = (( k % 7 ) + i ) % 7 if ( pos == k % 7 ) { return \"ERROR! HASHTABLE IS FULL\" } } T [ pos ] = k }","title":"Insertion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week12DSA3.html#finding","text":"insert ( k ) { i = 0 pos = (( k % 7 ) + i ) % 7 while ( T [ pos ] != k ) { i += 1 pos = (( k % 7 ) + i ) % 7 if ( T [ pos ] == EMPTY || pos == k % 7 ) { return \"NOT FOUND IN HASHTABLE\" } } return pos } Tags: !DatastructuresAndAlgorithmsIndex","title":"Finding"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week1DSA.html","text":"Week 1 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 24/Jul/2021 Topics Covered # What is an algorithm? What is time and space complexity? What is an algorithm? # An algorithm is a finite sequence of steps to be followed to reach a pre determined goal for a predetermined set of inputs. An algorithm has the following properties: 1. Finiteness : the sequence of steps the algorithm has must be finite 2. Definiteness : Each step should be atomic and precise and cause no confusion on what it does 3. Input : There may or may not be an input passed to the algorithm to work on 4. Termination : Due to the finiteness of algorithms the algorithm must terminate and produce an output 5. Correctness : The output produces must be the right one for any given input What is time and space complexity? # It is a way to analyze the performance of algorithms. Experimental studies for analyzing algorithms have some limitations: 1. It is difficult to compare running times of two algorithms in different software/hardware configurations 2. To truly know the performance we need to implement and run the algorithms on a computer since things like type of data provided for an algorithm can greatly alter the running time of the algorithm (For example an algorithm can run really slow for a single but large value and faster for multiple values) 3. One must need to make sure that the set of inputs used to analyze the algorithms are representative, meaning that the inputs must cover some particular use case (say for a sorting algorithm, we can have one input where all values are already sorted, sorted in reverse, all values being same and all values being jumbled) Consider the following example: Algorithm arrayMax(A, n): Input: An array A storing n >= 1 integers. Output: The maximum element in A currentMax <- A[0] for i <- 1 to n - 1 do if currentMax < A[i] then currentMax <- A[i] return currentMax The above example is what we call a pseudo code . It is meant to represent the flow of logic to find the maximum value in a given array. Consider A = [3, 2, 7, 5] Step i What we are comparing it with currentMax 0 null = A[0] = 3 1 A[1] = 2 3 > 2 so 3 2 A[2] = 7 3 < 7 so 7 3 A[3] = 5 7 > 5 so 7 So going over the steps we see that at the end we can determine that the max value is A[2] = 7. We see that: \"Algorithm arrayMax runs in time proportional to n\" If we were to actually run experiments, then the running time of arrayMax given any input of size n would never exceed c.n where c is the amount of time taken by the given software to run for an input of size 1. As seen above the constant c depends on the language used to run the algorithm and the hardware used to run the algorithm. A workstation can compute a more complex algorithm faster than a slow computer would for a less complex algorithm, hence to truly compare two algorithms all the parameters (The language, software and the hardware used) must be the same. Given two algorithms has two implementations where: 1. A algorithm : that runs proportional to \\(N\\) and 2. B algorithm that runs proportional to \\(N^2\\) we need to ideally makes sure that the algorithm chosen is the one that is proportional to N since for a very large input A would perform better. Addition, Multiplication, Subtraction and Division are examples of primitive operations. Primitive operation are those that cannot be further broken down to more simpler steps. In the above algorithm we can see the analysis as follows: currentMax <- A[0] ---> 2 = 1 (for indexing) + 1 (for assignment) i <- 1 ---> 1 (for assignment) while i < n ---> n (1 for the every comparison) if currentMax < A[i] then ---> 2 = 1 (for indexing) + 1 (for comparison) currentMax <- A[i] ---> 2 = 1 (for indexing) + 1 (for assignment) i = i + 1 ---> 2 = 1 (for addition) + 1 (for assignment) return currentMax ---> 1 From the above analysis we can say that the algorithm time complexity at the worst case where every iteration goes into the if block, is: \\[TimeComplexity < (2 + 1 + n + (n - 1) * (2 + 2 + 2) + 1)\\] \\[TimeComplexity < 7n - 2\\] \\[TimeComplexity < 7n\\] We say \\(<\\) because there are cases where the statements under the if condition is not calculated. Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week1DSA.html#week-1","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 24/Jul/2021","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week1DSA.html#topics-covered","text":"What is an algorithm? What is time and space complexity?","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week1DSA.html#what-is-an-algorithm","text":"An algorithm is a finite sequence of steps to be followed to reach a pre determined goal for a predetermined set of inputs. An algorithm has the following properties: 1. Finiteness : the sequence of steps the algorithm has must be finite 2. Definiteness : Each step should be atomic and precise and cause no confusion on what it does 3. Input : There may or may not be an input passed to the algorithm to work on 4. Termination : Due to the finiteness of algorithms the algorithm must terminate and produce an output 5. Correctness : The output produces must be the right one for any given input","title":"What is an algorithm?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week1DSA.html#what-is-time-and-space-complexity","text":"It is a way to analyze the performance of algorithms. Experimental studies for analyzing algorithms have some limitations: 1. It is difficult to compare running times of two algorithms in different software/hardware configurations 2. To truly know the performance we need to implement and run the algorithms on a computer since things like type of data provided for an algorithm can greatly alter the running time of the algorithm (For example an algorithm can run really slow for a single but large value and faster for multiple values) 3. One must need to make sure that the set of inputs used to analyze the algorithms are representative, meaning that the inputs must cover some particular use case (say for a sorting algorithm, we can have one input where all values are already sorted, sorted in reverse, all values being same and all values being jumbled) Consider the following example: Algorithm arrayMax(A, n): Input: An array A storing n >= 1 integers. Output: The maximum element in A currentMax <- A[0] for i <- 1 to n - 1 do if currentMax < A[i] then currentMax <- A[i] return currentMax The above example is what we call a pseudo code . It is meant to represent the flow of logic to find the maximum value in a given array. Consider A = [3, 2, 7, 5] Step i What we are comparing it with currentMax 0 null = A[0] = 3 1 A[1] = 2 3 > 2 so 3 2 A[2] = 7 3 < 7 so 7 3 A[3] = 5 7 > 5 so 7 So going over the steps we see that at the end we can determine that the max value is A[2] = 7. We see that: \"Algorithm arrayMax runs in time proportional to n\" If we were to actually run experiments, then the running time of arrayMax given any input of size n would never exceed c.n where c is the amount of time taken by the given software to run for an input of size 1. As seen above the constant c depends on the language used to run the algorithm and the hardware used to run the algorithm. A workstation can compute a more complex algorithm faster than a slow computer would for a less complex algorithm, hence to truly compare two algorithms all the parameters (The language, software and the hardware used) must be the same. Given two algorithms has two implementations where: 1. A algorithm : that runs proportional to \\(N\\) and 2. B algorithm that runs proportional to \\(N^2\\) we need to ideally makes sure that the algorithm chosen is the one that is proportional to N since for a very large input A would perform better. Addition, Multiplication, Subtraction and Division are examples of primitive operations. Primitive operation are those that cannot be further broken down to more simpler steps. In the above algorithm we can see the analysis as follows: currentMax <- A[0] ---> 2 = 1 (for indexing) + 1 (for assignment) i <- 1 ---> 1 (for assignment) while i < n ---> n (1 for the every comparison) if currentMax < A[i] then ---> 2 = 1 (for indexing) + 1 (for comparison) currentMax <- A[i] ---> 2 = 1 (for indexing) + 1 (for assignment) i = i + 1 ---> 2 = 1 (for addition) + 1 (for assignment) return currentMax ---> 1 From the above analysis we can say that the algorithm time complexity at the worst case where every iteration goes into the if block, is: \\[TimeComplexity < (2 + 1 + n + (n - 1) * (2 + 2 + 2) + 1)\\] \\[TimeComplexity < 7n - 2\\] \\[TimeComplexity < 7n\\] We say \\(<\\) because there are cases where the statements under the if condition is not calculated. Tags: !DatastructuresAndAlgorithmsIndex","title":"What is time and space complexity?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week2DSA.html","text":"Week 2 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 31/Jul/2021 Topics Covered # Questions Answered on Previous Week Topics Concept of Recursion Questions Answered on Previous Week Topics # Question : If an algorithm for solving a problem has a loop that runs \\(n\\) times and another algorithm to solve the same problem and has a loop that runs \\(n\\over2\\) , then what is the comparative analysis for these two algorithms? Answer : Both are proportional to \\(n\\) since the only thing that changed here is the constant \\(c\\) , which is \\(1\\) in the first case and \\(1\\over2\\) in the other. Question : Do all primitives have the same constant running time? Answer : This is not the case but as a concept it is taken as to be the same since the running time for a given operation is not dependent on the number of inputs. Question : From the array max example (discussed here: Week1DSA#What is time and space complexity ) we see that the algorithm in its worst case is \\(7n - 2\\) , what is it in its best case? Answer : In the best case, the maximum value is in the starting of the array \\(A[0]\\) itself. In this case the inside of the if block will not even execute so we can reduce \\(2\\) primitive steps from the calculation, this will lead to the following analysis: \\[TimeComplexity < (2 + 1 + n + (n - 1) * (2 + 0 + 2) + 1)\\] \\[TimeComplexity < 5n\\] Concept of Recursion # Consider the algorithm for finding the factorial of a number: Algorithm factorial(n): Input: A number n > 0. Output: The factorial of n. product <- 1 for i <- 1 to n do product <- i * product return product The above algorithm is called an iterative one since it employs loops, the same can be written through recursive function calls such as below: Algorithm factorial(n): Input: A number n > 0. Output: The factorial of n. product <- 1 if n = 1 then return 1 else result = n * factorial(n - 1) return result Here you see that the loop is removed and the algorithm is called recursively, but it yields the same result. One must be careful to have at least one base case (Here \\(product \\longleftarrow 1\\) ) must exist, else recursion will go on indefinitely. Now a recursive implementation for the array max algorithm ( Week1DSA#What is time and space complexity ) would be as follows: Algorithm recursiveMax(A, n): Input: An array A storing n >= 1 integers. Output: The maximum element in A if n = 1 then return A[0] return max{recursiveMax(A, n-1), A[n-1]} The running time for the same will be shows as below: \\[T(n) = 3,\\ if\\ n=1\\] \\[T(n) = T(n-1)+7,\\ otherwise\\] Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week2DSA.html#week-2","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 31/Jul/2021","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week2DSA.html#topics-covered","text":"Questions Answered on Previous Week Topics Concept of Recursion","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week2DSA.html#questions-answered-on-previous-week-topics","text":"Question : If an algorithm for solving a problem has a loop that runs \\(n\\) times and another algorithm to solve the same problem and has a loop that runs \\(n\\over2\\) , then what is the comparative analysis for these two algorithms? Answer : Both are proportional to \\(n\\) since the only thing that changed here is the constant \\(c\\) , which is \\(1\\) in the first case and \\(1\\over2\\) in the other. Question : Do all primitives have the same constant running time? Answer : This is not the case but as a concept it is taken as to be the same since the running time for a given operation is not dependent on the number of inputs. Question : From the array max example (discussed here: Week1DSA#What is time and space complexity ) we see that the algorithm in its worst case is \\(7n - 2\\) , what is it in its best case? Answer : In the best case, the maximum value is in the starting of the array \\(A[0]\\) itself. In this case the inside of the if block will not even execute so we can reduce \\(2\\) primitive steps from the calculation, this will lead to the following analysis: \\[TimeComplexity < (2 + 1 + n + (n - 1) * (2 + 0 + 2) + 1)\\] \\[TimeComplexity < 5n\\]","title":"Questions Answered on Previous Week Topics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week2DSA.html#concept-of-recursion","text":"Consider the algorithm for finding the factorial of a number: Algorithm factorial(n): Input: A number n > 0. Output: The factorial of n. product <- 1 for i <- 1 to n do product <- i * product return product The above algorithm is called an iterative one since it employs loops, the same can be written through recursive function calls such as below: Algorithm factorial(n): Input: A number n > 0. Output: The factorial of n. product <- 1 if n = 1 then return 1 else result = n * factorial(n - 1) return result Here you see that the loop is removed and the algorithm is called recursively, but it yields the same result. One must be careful to have at least one base case (Here \\(product \\longleftarrow 1\\) ) must exist, else recursion will go on indefinitely. Now a recursive implementation for the array max algorithm ( Week1DSA#What is time and space complexity ) would be as follows: Algorithm recursiveMax(A, n): Input: An array A storing n >= 1 integers. Output: The maximum element in A if n = 1 then return A[0] return max{recursiveMax(A, n-1), A[n-1]} The running time for the same will be shows as below: \\[T(n) = 3,\\ if\\ n=1\\] \\[T(n) = T(n-1)+7,\\ otherwise\\] Tags: !DatastructuresAndAlgorithmsIndex","title":"Concept of Recursion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html","text":"Week 4 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 21/Aug/2021 Topics Covered # Deriving Big \\(\\mathcal{O}\\) From Growth Rate Significance of Growth Rate Examples Rules to Find Big \\(\\mathcal{O}\\) Notation Deriving Big \\(\\mathcal{O}\\) From Growth Rate # Significance of Growth Rate # Consider f(n) and g(n), Let us consider what a growth rate of a function is. Growth rate \\(f(n) = 1000n\\) \\(g(n) = 2n^2\\) \\(n\\) \\(f(n)\\) \\(g(n)\\) \\(f(n)/f(n-1)\\) \\(g(n)/g(n-1)\\) 1 1000 2 - - 2 2000 8 2 4 3 3000 18 1.5 2.25 4 4000 32 1.33 1.78 5 5000 50 1.2 1.56 Seeing the above pattern in the rate of growth, we see that for any particular value of \\(n\\) the \\(g(n)\\) ratio is bigger than that for \\(f(n)\\) , and this is seen as we incrementing \\(n\\) . So we can conclude that \\(g(n)\\) has a faster rate of growth. We can also find the value of n for which the value of \\(g(n)\\) will shoot up more than \\(f(n)\\) : \\(g(n) \\ge f(n)\\) \\(2.n^2 \\ge 1000n\\) \\(n \\ge 500\\) This shows that since the \\(g(n)\\) grows faster (quadratic growth), whenever \\(n\\) is greater than \\(500\\) then \\(g(n)\\) will always be greater than \\(f(n)\\) so we can conclude that the algorithm that has a run time of \\(f(n)\\) will be overall better than one that has a run time of \\(g(n)\\) . Examples # Example 1 : \\(f(n) = 20.n^3 + 10.n.log(n) + 5\\) is \\(\\mathcal{O}(n^3)\\) Proof : \\(20.n^3 + 10.n.log(n) + 5 \\le 35.n^3\\) , for \\(n \\ge 1\\) Example 2 : \\(f(n) = 2^{100}\\) is \\(\\mathcal{O}(1)\\) Proof : \\(2^{100} \\le 2^{100}.1\\) , for \\(n \\ge 1\\) Example 3 : \\(f(n) = 3^log(n) + log(log(n))\\) is \\(\\mathcal{O}(log(n))\\) Proof : \\(3^log(n) + log(log(n)) \\le log(n)\\) , for \\(n \\ge 2\\) and \\(c = 4\\) Rules to Find Big \\(\\mathcal{O}\\) Notation # Some examples: Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#week-4","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 21/Aug/2021","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#topics-covered","text":"Deriving Big \\(\\mathcal{O}\\) From Growth Rate Significance of Growth Rate Examples Rules to Find Big \\(\\mathcal{O}\\) Notation","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#deriving-bigmathcalo-from-growth-rate","text":"","title":"Deriving Big\\(\\mathcal{O}\\) From Growth Rate"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#significance-of-growth-rate","text":"Consider f(n) and g(n), Let us consider what a growth rate of a function is. Growth rate \\(f(n) = 1000n\\) \\(g(n) = 2n^2\\) \\(n\\) \\(f(n)\\) \\(g(n)\\) \\(f(n)/f(n-1)\\) \\(g(n)/g(n-1)\\) 1 1000 2 - - 2 2000 8 2 4 3 3000 18 1.5 2.25 4 4000 32 1.33 1.78 5 5000 50 1.2 1.56 Seeing the above pattern in the rate of growth, we see that for any particular value of \\(n\\) the \\(g(n)\\) ratio is bigger than that for \\(f(n)\\) , and this is seen as we incrementing \\(n\\) . So we can conclude that \\(g(n)\\) has a faster rate of growth. We can also find the value of n for which the value of \\(g(n)\\) will shoot up more than \\(f(n)\\) : \\(g(n) \\ge f(n)\\) \\(2.n^2 \\ge 1000n\\) \\(n \\ge 500\\) This shows that since the \\(g(n)\\) grows faster (quadratic growth), whenever \\(n\\) is greater than \\(500\\) then \\(g(n)\\) will always be greater than \\(f(n)\\) so we can conclude that the algorithm that has a run time of \\(f(n)\\) will be overall better than one that has a run time of \\(g(n)\\) .","title":"Significance of Growth Rate"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#examples","text":"Example 1 : \\(f(n) = 20.n^3 + 10.n.log(n) + 5\\) is \\(\\mathcal{O}(n^3)\\) Proof : \\(20.n^3 + 10.n.log(n) + 5 \\le 35.n^3\\) , for \\(n \\ge 1\\) Example 2 : \\(f(n) = 2^{100}\\) is \\(\\mathcal{O}(1)\\) Proof : \\(2^{100} \\le 2^{100}.1\\) , for \\(n \\ge 1\\) Example 3 : \\(f(n) = 3^log(n) + log(log(n))\\) is \\(\\mathcal{O}(log(n))\\) Proof : \\(3^log(n) + log(log(n)) \\le log(n)\\) , for \\(n \\ge 2\\) and \\(c = 4\\)","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA.html#rules-to-find-bigmathcalo-notation","text":"Some examples: Tags: !DatastructuresAndAlgorithmsIndex","title":"Rules to Find Big\\(\\mathcal{O}\\) Notation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html","text":"Week 4 (cont.) # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 22/Aug/2021 Topics Covered # Rules to Find Big \\(\\mathcal{O}\\) Notation (cont.) Little \\(\\mathcal{o}\\) and Little \\(\\omega\\) Bubble Sort Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity Rules to Find Big \\(\\mathcal{O}\\) Notation (cont.) # It is considered poor taste to include constant factors and lower order terms in the big \\(\\mathcal{O}\\) notation Consider a function \\(2n^2\\) is \\(\\mathcal{O}(4n^2 + 6nlog(n))\\) , although this is right it is always easier and simpler to write \\(\\mathcal{O}(n^2)\\) Logrithmic Linear Quadractic Polynomial Exponential \\(\\mathcal{O}(log(n))\\) \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(n^2)\\) \\(\\mathcal{O}(n^k)\\ (k \\ge 1)\\) \\(\\mathcal{O}(a^n)\\ (a \\gt 1)\\) Even though in general we ignore constants in the big \\(\\mathcal{O}\\) notation, we need to be careful and check if the constants have very large constants, in this case big \\(\\mathcal{O}\\) might not be the right assumption for an algorithm. Some Functions Ordered By Growth Rate Commone Name \\(\\mathcal{O}(log(n))\\) Logrithmic \\(\\mathcal{O}(log^2(n))\\) Polytlogrithmic \\(\\mathcal{O}(\\sqrt{n})\\) Square Root \\(\\mathcal{O}(n)\\) Linear \\(\\mathcal{O}(nlog(n))\\) Linearithmic \\(\\mathcal{O}(n^2)\\) Quadratic \\(\\mathcal{O}(n^3)\\) Cubic \\(\\mathcal{O}(2^n)\\) Exponential In the above chart you see that \\(\\sqrt{n}\\) crosses over much later over \\(log^2(n)\\) in comparison to other function pairs. Little \\(\\mathcal{o}\\) and Little \\(\\omega\\) # No matter what value of \\(c \\gt 0\\) we choose, we need to see if \\(g(n) \\ge f(n)\\) . For example for a functions \\(12n^2 + 6n\\) , the little \\(\\mathcal{o}\\) is \\(\\mathcal{o}(n^3)\\) . if \\(f(n)\\) is \\(\\mathcal{o}(g(n)\\) then \\(g(n)\\) is \\(\\omega(f(n))\\) Bubble Sort # Sort Function # BubbleSort ( int [] A , int n ) Input : An array A containing n >= 1 integers Output : The sorted version of the array A for i = 0 to ( n - 1 ) for j = 0 to ( n - 1 - i ) if A [ i ] > A [ j + 1 ] # swap A[j] with A[j + 1] A [ j ] <-> A [ j + 1 ] return A Optimized Sort Function # BubbleSortOptimized ( int [] A , int n ) Input : An array A containing n >= 1 integers Output : The sorted version of the array A for i = 0 to ( n - 1 ) swaps = 0 for j = 1 to ( n - 1 - i ) if A [ i ] > A [ j + 1 ] # swap A[j] with A[j + 1] A [ j ] <-> A [ j + 1 ] swaps <- swaps + 1 if swaps == 0 break return A Algorithm Analysis # Time Complexity # In the basic sort function, for every value of \\(i\\) , the inner loop indexed by \\(j\\) is done upto \\(n - 1\\) number of times and the outer loop indexed by \\(i\\) also runs \\(n - 1\\) times as well. So the worst case time complexity is \\(\\mathcal{O}((n - 1)(n - 1)) = \\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is also \\(\\mathcal{O}(n^2)\\) Now taking the optimized sort function, we see that at the worst case the function behaves exactly like the basic sort function so the worst case time complexity is still \\(\\mathcal{O}(n^2)\\) , but in the best case the outer loop will run only once, so the best case time complexity is \\(\\mathcal{O}(n)\\) since only the inner loop will run completely once. Space Complexity # Since no extra data structures were used, the space used is constant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 4 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#week-4-cont","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 22/Aug/2021","title":"Week 4 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#topics-covered","text":"Rules to Find Big \\(\\mathcal{O}\\) Notation (cont.) Little \\(\\mathcal{o}\\) and Little \\(\\omega\\) Bubble Sort Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#rules-to-find-bigmathcalo-notation-cont","text":"It is considered poor taste to include constant factors and lower order terms in the big \\(\\mathcal{O}\\) notation Consider a function \\(2n^2\\) is \\(\\mathcal{O}(4n^2 + 6nlog(n))\\) , although this is right it is always easier and simpler to write \\(\\mathcal{O}(n^2)\\) Logrithmic Linear Quadractic Polynomial Exponential \\(\\mathcal{O}(log(n))\\) \\(\\mathcal{O}(n)\\) \\(\\mathcal{O}(n^2)\\) \\(\\mathcal{O}(n^k)\\ (k \\ge 1)\\) \\(\\mathcal{O}(a^n)\\ (a \\gt 1)\\) Even though in general we ignore constants in the big \\(\\mathcal{O}\\) notation, we need to be careful and check if the constants have very large constants, in this case big \\(\\mathcal{O}\\) might not be the right assumption for an algorithm. Some Functions Ordered By Growth Rate Commone Name \\(\\mathcal{O}(log(n))\\) Logrithmic \\(\\mathcal{O}(log^2(n))\\) Polytlogrithmic \\(\\mathcal{O}(\\sqrt{n})\\) Square Root \\(\\mathcal{O}(n)\\) Linear \\(\\mathcal{O}(nlog(n))\\) Linearithmic \\(\\mathcal{O}(n^2)\\) Quadratic \\(\\mathcal{O}(n^3)\\) Cubic \\(\\mathcal{O}(2^n)\\) Exponential In the above chart you see that \\(\\sqrt{n}\\) crosses over much later over \\(log^2(n)\\) in comparison to other function pairs.","title":"Rules to Find Big\\(\\mathcal{O}\\) Notation (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#littlemathcalo-and-littleomega","text":"No matter what value of \\(c \\gt 0\\) we choose, we need to see if \\(g(n) \\ge f(n)\\) . For example for a functions \\(12n^2 + 6n\\) , the little \\(\\mathcal{o}\\) is \\(\\mathcal{o}(n^3)\\) . if \\(f(n)\\) is \\(\\mathcal{o}(g(n)\\) then \\(g(n)\\) is \\(\\omega(f(n))\\)","title":"Little\\(\\mathcal{o}\\) and Little\\(\\omega\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#bubble-sort","text":"","title":"Bubble Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#sort-function","text":"BubbleSort ( int [] A , int n ) Input : An array A containing n >= 1 integers Output : The sorted version of the array A for i = 0 to ( n - 1 ) for j = 0 to ( n - 1 - i ) if A [ i ] > A [ j + 1 ] # swap A[j] with A[j + 1] A [ j ] <-> A [ j + 1 ] return A","title":"Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#optimized-sort-function","text":"BubbleSortOptimized ( int [] A , int n ) Input : An array A containing n >= 1 integers Output : The sorted version of the array A for i = 0 to ( n - 1 ) swaps = 0 for j = 1 to ( n - 1 - i ) if A [ i ] > A [ j + 1 ] # swap A[j] with A[j + 1] A [ j ] <-> A [ j + 1 ] swaps <- swaps + 1 if swaps == 0 break return A","title":"Optimized Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#time-complexity","text":"In the basic sort function, for every value of \\(i\\) , the inner loop indexed by \\(j\\) is done upto \\(n - 1\\) number of times and the outer loop indexed by \\(i\\) also runs \\(n - 1\\) times as well. So the worst case time complexity is \\(\\mathcal{O}((n - 1)(n - 1)) = \\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is also \\(\\mathcal{O}(n^2)\\) Now taking the optimized sort function, we see that at the worst case the function behaves exactly like the basic sort function so the worst case time complexity is still \\(\\mathcal{O}(n^2)\\) , but in the best case the outer loop will run only once, so the best case time complexity is \\(\\mathcal{O}(n)\\) since only the inner loop will run completely once.","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week4DSA2.html#space-complexity","text":"Since no extra data structures were used, the space used is constant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html","text":"Week 5 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 28/Aug/2021 Topics Covered # Selection Sort Steps Algorithm Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity Selection Sort # Selection sort uses similar concepts of Bubble sort but then instead of bubbling up the max value in each pass, we select the max value in the given array and then swap that to the last position directly and do the same to the second last element and so on. Steps # Let us consider the following arrays Initial State: \\([\\ 4, 1, 7, 5, 2, 3\\ ]\\) At each pass we select the maximum value of the array and move it to the end: Pass 1: \\([\\ 4, 1, 3, 5, 2, 7\\ ]\\) Pass 2: \\([\\ 4, 1, 3, 2, 5, 7\\ ]\\) Pass 3: \\([\\ 2, 1, 3, 4, 5, 7\\ ]\\) Pass 4: \\([\\ 2, 1, 3, 4, 5, 7\\ ]\\) Pass 5: \\([\\ 1, 2, 3, 4, 5, 7\\ ]\\) Algorithm # Sort Function # sort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 ; i < n do max = A [ 0 ] maxIndex = 0 for j = 1 ; j < ( n - i ) do if A [ j ] > max then max = A [ j ] maxIndex = j A [ maxIndex ] <-> A [ n - i ] Optimized Sort Function # optimizedSort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 ; i < n do inversions = 0 for j = 0 ; j < ( n - 1 - i ) do if A [ j ] > A [ j + 1 ] then inversion = inversion + 1 if invversions == 0 then break max = A [ 0 ] maxIndex = 0 for j = 1 ; j < ( n - i ) do if A [ j ] > max then max = A [ j ] maxIndex = j A [ maxIndex ] <-> A [ n - i ] Algorithm Analysis # Time Complexity # Just like the bubble sort, here we see that the outer loop runs in the order of \\(n\\) and the inner loop is also running in the order of \\(n\\) So the worst case time complexity is \\(\\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is also \\(\\mathcal{O}(n^2)\\) Space Complexity # Since no extra data structures were used, the space used is contant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#week-5","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 28/Aug/2021","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#topics-covered","text":"Selection Sort Steps Algorithm Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#selection-sort","text":"Selection sort uses similar concepts of Bubble sort but then instead of bubbling up the max value in each pass, we select the max value in the given array and then swap that to the last position directly and do the same to the second last element and so on.","title":"Selection Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#steps","text":"Let us consider the following arrays Initial State: \\([\\ 4, 1, 7, 5, 2, 3\\ ]\\) At each pass we select the maximum value of the array and move it to the end: Pass 1: \\([\\ 4, 1, 3, 5, 2, 7\\ ]\\) Pass 2: \\([\\ 4, 1, 3, 2, 5, 7\\ ]\\) Pass 3: \\([\\ 2, 1, 3, 4, 5, 7\\ ]\\) Pass 4: \\([\\ 2, 1, 3, 4, 5, 7\\ ]\\) Pass 5: \\([\\ 1, 2, 3, 4, 5, 7\\ ]\\)","title":"Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#sort-function","text":"sort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 ; i < n do max = A [ 0 ] maxIndex = 0 for j = 1 ; j < ( n - i ) do if A [ j ] > max then max = A [ j ] maxIndex = j A [ maxIndex ] <-> A [ n - i ]","title":"Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#optimized-sort-function","text":"optimizedSort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 ; i < n do inversions = 0 for j = 0 ; j < ( n - 1 - i ) do if A [ j ] > A [ j + 1 ] then inversion = inversion + 1 if invversions == 0 then break max = A [ 0 ] maxIndex = 0 for j = 1 ; j < ( n - i ) do if A [ j ] > max then max = A [ j ] maxIndex = j A [ maxIndex ] <-> A [ n - i ]","title":"Optimized Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#time-complexity","text":"Just like the bubble sort, here we see that the outer loop runs in the order of \\(n\\) and the inner loop is also running in the order of \\(n\\) So the worst case time complexity is \\(\\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is also \\(\\mathcal{O}(n^2)\\)","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA.html#space-complexity","text":"Since no extra data structures were used, the space used is contant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html","text":"Week 5 (cont.) # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 29/Aug/2021 Topics Covered # Insertion Sort Steps Algorithm Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity Selection Sort # Steps # Let us consider the following arrays Initial State: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) At each pass we sort the array that we get by adding one element to it, So we first sort indexes \\(1, 2\\) of the array, after that we sort the indexes \\(1, 2, 3\\) and so on till \\(1, 2, 3, .... , n\\) to get the final sorted array Pass 1: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) , \\(j = 0\\) Pass 2: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) Pass 3: \\([\\ 1, 4, 5, 7, 2, 3\\ ]\\) Pass 4: \\([\\ 1, 2, 4, 5, 7, 3\\ ]\\) Pass 5: \\([\\ 1, 2, 3, 4, 5, 7\\ ]\\) Algorithm # Sort Function # sort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 to n - 1 j = i - 1 while A [ i ] < A [ j ] j = j - 1 if j < 0 break curr = A [ i ] k = i - 1 while k >= j + 1 A [ k + 1 ] = A [ k ] k = k - 1 A [ j + 1 ] = curr Algorithm Analysis # Time Complexity # Just like the selection sort, here we see that the outer loop runs in the order of \\(n\\) and the inner while loop is also running in the order of \\(n\\) So the worst case time complexity is \\(\\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is \\(\\mathcal{O}(n)\\) , since none of the loops are executed if the elements are sorted, so the order of \\(n\\) operations that is contributed by them will not happen. Space Complexity # Since no extra data structures were used, the space used is constant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 5 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#week-5-cont","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 29/Aug/2021","title":"Week 5 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#topics-covered","text":"Insertion Sort Steps Algorithm Sort Function Optimized Sort Function Algorithm Analysis Time Complexity Space Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#selection-sort","text":"","title":"Selection Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#steps","text":"Let us consider the following arrays Initial State: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) At each pass we sort the array that we get by adding one element to it, So we first sort indexes \\(1, 2\\) of the array, after that we sort the indexes \\(1, 2, 3\\) and so on till \\(1, 2, 3, .... , n\\) to get the final sorted array Pass 1: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) , \\(j = 0\\) Pass 2: \\([\\ 1, 4, 7, 5, 2, 3\\ ]\\) Pass 3: \\([\\ 1, 4, 5, 7, 2, 3\\ ]\\) Pass 4: \\([\\ 1, 2, 4, 5, 7, 3\\ ]\\) Pass 5: \\([\\ 1, 2, 3, 4, 5, 7\\ ]\\)","title":"Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#sort-function","text":"sort ( A ): Input : An array of size n Ouput : The sorted version of the array A for i = 1 to n - 1 j = i - 1 while A [ i ] < A [ j ] j = j - 1 if j < 0 break curr = A [ i ] k = i - 1 while k >= j + 1 A [ k + 1 ] = A [ k ] k = k - 1 A [ j + 1 ] = curr","title":"Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#time-complexity","text":"Just like the selection sort, here we see that the outer loop runs in the order of \\(n\\) and the inner while loop is also running in the order of \\(n\\) So the worst case time complexity is \\(\\mathcal{O}(n^2)\\) Best case time complexity (Where array is already sorted) is \\(\\mathcal{O}(n)\\) , since none of the loops are executed if the elements are sorted, so the order of \\(n\\) operations that is contributed by them will not happen.","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week5DSA2.html#space-complexity","text":"Since no extra data structures were used, the space used is constant, so the space complexity in all cases is \\(\\mathcal{O}(1)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html","text":"Week 6 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 4/Sep/2021 Topics Covered # Merge Sort Steps Algorithm Merge Function Sort Function Algorithm Analysis Space Complexity Time Complexity Merge Sort # Merge Sort is not an in-place sort like the other three that was explained above Steps # Let us consider the following arrays \\(4, 1, 7, 5, 2, 3\\) Now let us split this array into two halves: \\(4, 1, 7\\ |\\ 5, 2, 3\\) And let us sort these two parts: \\(1, 4, 7\\ |\\ 2, 3, 5\\) When merging the above two parts, we maintain two pointers that start on either parts, and we compare the elements pointed by these two and place the smaller element into a new array and advance that pointer and repeat the above procedure till all the elements are compared. Algorithm # Merge Function # merge ( S1 , S2 , S ): Input : Two arrays , S1 , S2 , of size n1 and n2 sorted in non decreasing order , and an empty array S of size n1 + n2 Ouput : S , containing the elements from S1 and S2 in sorted order i <- 0 j <- 0 while i < n1 and j < n2 do if S1 [ i ] <= S2 [ j ] then S [ i + j ] <- S1 [ i ] i <- i + 1 else S [ i + j ] <- S2 [ j ] j <- j + 1 while i <= n1 do S [ i + j ] <- S1 [ i ] i <- i + 1 while i <= n2 do S [ i + j ] <- S2 [ j ] j <- j + 1 Sort Function # The actual sorting is done in a divide and conquer fashion denoted by the below steps: 1. Divide: If \\(S\\) has zero or one element, return \\(S\\) immediately, it is already sorted, Otherwise put the elements of \\(S\\) into two sequences \\(S_1\\) and \\(S_2\\) , each containing about half of the elements of \\(S\\) 2. Recur: Recursively sort the sequences \\(S_1\\) and \\(S_2\\) 3. Conquer: Put back the elements into S by merging the sorted sequences \\(S_1\\) and \\(S_2\\) into a sorted sequence. sort ( S ): Input : An array of size n Ouput : The sorted version of the array S if n == 1 then return S if n == 2 then if S [ 0 ] > S [ 1 ] then S [ 0 ] <-> S [ 1 ] return S S1 <- { S [ 0 ], S [ 1 ], S [ 2 ], .... , S [ n / 2 ]} S1 <- { S [( n / 2 ) + 1 ], .... , S [ n - 2 ], S [ n - 1 ]} R1 <- sort ( S1 ) R2 <- sort ( S2 ) merge ( R1 , R2 , R ) # R is an empty array of size n return R Algorithm Analysis # Space Complexity # The recursive tree for a random input array would look something like this: From the above image you can see that the number of memory blocks from the root to the leaf node of the recursive tree is at max \\(n\\) , so: Space Complexity: \\(\\mathcal{O}(n)\\) Time Complexity # At every level, we do a total effort of merging in the order of \\(\\mathcal{O}(n)\\) The total height of the recursive tree is given by the order of \\(\\mathcal{O}(log\\ n)\\) So Time Complexity: \\(\\mathcal{O}(n\\ log\\ n)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#week-6","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 4/Sep/2021","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#topics-covered","text":"Merge Sort Steps Algorithm Merge Function Sort Function Algorithm Analysis Space Complexity Time Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#merge-sort","text":"Merge Sort is not an in-place sort like the other three that was explained above","title":"Merge Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#steps","text":"Let us consider the following arrays \\(4, 1, 7, 5, 2, 3\\) Now let us split this array into two halves: \\(4, 1, 7\\ |\\ 5, 2, 3\\) And let us sort these two parts: \\(1, 4, 7\\ |\\ 2, 3, 5\\) When merging the above two parts, we maintain two pointers that start on either parts, and we compare the elements pointed by these two and place the smaller element into a new array and advance that pointer and repeat the above procedure till all the elements are compared.","title":"Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#merge-function","text":"merge ( S1 , S2 , S ): Input : Two arrays , S1 , S2 , of size n1 and n2 sorted in non decreasing order , and an empty array S of size n1 + n2 Ouput : S , containing the elements from S1 and S2 in sorted order i <- 0 j <- 0 while i < n1 and j < n2 do if S1 [ i ] <= S2 [ j ] then S [ i + j ] <- S1 [ i ] i <- i + 1 else S [ i + j ] <- S2 [ j ] j <- j + 1 while i <= n1 do S [ i + j ] <- S1 [ i ] i <- i + 1 while i <= n2 do S [ i + j ] <- S2 [ j ] j <- j + 1","title":"Merge Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#sort-function","text":"The actual sorting is done in a divide and conquer fashion denoted by the below steps: 1. Divide: If \\(S\\) has zero or one element, return \\(S\\) immediately, it is already sorted, Otherwise put the elements of \\(S\\) into two sequences \\(S_1\\) and \\(S_2\\) , each containing about half of the elements of \\(S\\) 2. Recur: Recursively sort the sequences \\(S_1\\) and \\(S_2\\) 3. Conquer: Put back the elements into S by merging the sorted sequences \\(S_1\\) and \\(S_2\\) into a sorted sequence. sort ( S ): Input : An array of size n Ouput : The sorted version of the array S if n == 1 then return S if n == 2 then if S [ 0 ] > S [ 1 ] then S [ 0 ] <-> S [ 1 ] return S S1 <- { S [ 0 ], S [ 1 ], S [ 2 ], .... , S [ n / 2 ]} S1 <- { S [( n / 2 ) + 1 ], .... , S [ n - 2 ], S [ n - 1 ]} R1 <- sort ( S1 ) R2 <- sort ( S2 ) merge ( R1 , R2 , R ) # R is an empty array of size n return R","title":"Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#space-complexity","text":"The recursive tree for a random input array would look something like this: From the above image you can see that the number of memory blocks from the root to the leaf node of the recursive tree is at max \\(n\\) , so: Space Complexity: \\(\\mathcal{O}(n)\\)","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA.html#time-complexity","text":"At every level, we do a total effort of merging in the order of \\(\\mathcal{O}(n)\\) The total height of the recursive tree is given by the order of \\(\\mathcal{O}(log\\ n)\\) So Time Complexity: \\(\\mathcal{O}(n\\ log\\ n)\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html","text":"Week 6 (cont.) # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 5/Sep/2021 Topics Covered # Quick Sort Steps Done Algorithm Pivot Function Quick Sort Function Algorithm Analysis Space Complexity Time Complexity Quick Sort # Steps Done # The actual sorting is done in a divide and conquer fashion denoted by the below steps: 1. Divide: If \\(S\\) has zero or one element, return \\(S\\) immediately, it is already sorted, Otherwise pick a random element as a pivot element, and generate 3 parts: 1. \\(L\\) : The elements less than the pivot element 2. \\(E\\) : The element that is used as a pivot 3. \\(G\\) : The elements greater than the pivot element 2. Recur: Recursively sort the sequences \\(L\\) and \\(G\\) 3. Conquer: Concatenate the three sequences and return that Algorithm # Pivot Function # partition ( arr , low , high ) Input : An array of integers arr the lower index for the partition in the array and high the upper index Ouput : pi , Partitioning index , the index where the pivot is placed pivot = arr [ high ] i = ( low - 1 ) for ( j = low ; j <= high - 1 ; j ++ ) then if arr [ j ] < pivot then i ++ arr [ i ] <-> arr [ j ] arr [ i + 1 ] <-> arr [ high ] return ( i + 1 ) Quick Sort Function # sort ( arr , low , high ) Input : An array of integers arr the lower index for the partition in the array and high the upper index Ouput : pi , Partitioning index , the index where the pivot is placed if low < high then pi = partition ( arr , low , high ) sort ( arr , low , pi - 1 ) sort ( arr , pi + 1 , high ) Algorithm Analysis # Space Complexity # The in place algorithm does not take any extra space so we can say the space complexity is \\(\\mathcal{O}(1)\\) Time Complexity # The partition function performs a single loop on the array elements, so the total number of operations done on a given level is proportional to n, so the time complexity here is \\(\\mathcal{O}(n)\\) . In the best case, where the partition was balanced: The sort function recursively calls sort by dividing the array into two segments and a pivot, so the size reduces at the rate of \\(log\\ n\\) so the time complexity is \\(\\mathcal{O}(log\\ n)\\) . The total time complexity is: \\(\\mathcal{O}(n\\ log\\ n)\\) In the worst case, where the partition is not balanced The sort function would end up creating \\(n - 1\\) levels, making the total time complexity as \\(\\mathcal{O}(n^2)\\) The reason why this sort is called Quick sort is because, the worst case happens very rarely and even if it does it can be fixed with a simple check to see if an array is already sorted. Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 6 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#week-6-cont","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 5/Sep/2021","title":"Week 6 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#topics-covered","text":"Quick Sort Steps Done Algorithm Pivot Function Quick Sort Function Algorithm Analysis Space Complexity Time Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#quick-sort","text":"","title":"Quick Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#steps-done","text":"The actual sorting is done in a divide and conquer fashion denoted by the below steps: 1. Divide: If \\(S\\) has zero or one element, return \\(S\\) immediately, it is already sorted, Otherwise pick a random element as a pivot element, and generate 3 parts: 1. \\(L\\) : The elements less than the pivot element 2. \\(E\\) : The element that is used as a pivot 3. \\(G\\) : The elements greater than the pivot element 2. Recur: Recursively sort the sequences \\(L\\) and \\(G\\) 3. Conquer: Concatenate the three sequences and return that","title":"Steps Done"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#pivot-function","text":"partition ( arr , low , high ) Input : An array of integers arr the lower index for the partition in the array and high the upper index Ouput : pi , Partitioning index , the index where the pivot is placed pivot = arr [ high ] i = ( low - 1 ) for ( j = low ; j <= high - 1 ; j ++ ) then if arr [ j ] < pivot then i ++ arr [ i ] <-> arr [ j ] arr [ i + 1 ] <-> arr [ high ] return ( i + 1 )","title":"Pivot Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#quick-sort-function","text":"sort ( arr , low , high ) Input : An array of integers arr the lower index for the partition in the array and high the upper index Ouput : pi , Partitioning index , the index where the pivot is placed if low < high then pi = partition ( arr , low , high ) sort ( arr , low , pi - 1 ) sort ( arr , pi + 1 , high )","title":"Quick Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#space-complexity","text":"The in place algorithm does not take any extra space so we can say the space complexity is \\(\\mathcal{O}(1)\\)","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week6DSA2.html#time-complexity","text":"The partition function performs a single loop on the array elements, so the total number of operations done on a given level is proportional to n, so the time complexity here is \\(\\mathcal{O}(n)\\) . In the best case, where the partition was balanced: The sort function recursively calls sort by dividing the array into two segments and a pivot, so the size reduces at the rate of \\(log\\ n\\) so the time complexity is \\(\\mathcal{O}(log\\ n)\\) . The total time complexity is: \\(\\mathcal{O}(n\\ log\\ n)\\) In the worst case, where the partition is not balanced The sort function would end up creating \\(n - 1\\) levels, making the total time complexity as \\(\\mathcal{O}(n^2)\\) The reason why this sort is called Quick sort is because, the worst case happens very rarely and even if it does it can be fixed with a simple check to see if an array is already sorted. Tags: !DatastructuresAndAlgorithmsIndex","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html","text":"Week 7 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 11/Sep/2021 Topics Covered # Counting Sort Steps Done Algorithm Sort Function Algorithm Analysis Space Complexity Time Complexity Counting Sort # Steps Done # Let us say that we know that an input unsorted array has values that are within a given range. Then we maintain another array that keeps track of the frequency of the elements that occur in this list. We parse the array and whenever we come across the element we increment the frequency of that element by one. Value: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Frequency: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] A: { 6, 1, 8, 3, 7, 2, 3, 9, 7 } New_Frequency: [0, 1, 1, 2, 0, 0, 1, 2, 1, 1] Using the above newly calculated frequency array we can insert elements into the sorted array: S: { 1, 2, 3, 3, 6, 7, 7, 8, 9 } Algorithm # Sort Function # CountSort ( int [] A , int n ) Input : An array of integers of lenght n where the values in A are between [ 0 , R - 1 ] Output : The sorted version of A int F [ R ] for i = 0 to ( R - 1 ) F [ i ] = 0 for i = 0 to ( n - 1 ) tmp = A [ i ] F [ tmp ] += 1 int S [ n ] int k = 0 for i = 0 to ( R - 1 ) freq = F [ i ] for j = 1 to freq S [ k ] = i k ++ return S Algorithm Analysis # Space Complexity # Space complexity depends on the length of the frequency array, so it is: \\(\\mathcal{O}(R)\\) Time Complexity # The time complexity is max of \\(\\mathcal{O}(n)\\) or \\(\\mathcal{O}(R)\\) = \\(\\mathcal{O}(max(n, R))\\) The total complexity for the last loop is: \\(\\mathcal{O}(max(n, R))\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#week-7","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 11/Sep/2021","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#topics-covered","text":"Counting Sort Steps Done Algorithm Sort Function Algorithm Analysis Space Complexity Time Complexity","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#counting-sort","text":"","title":"Counting Sort"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#steps-done","text":"Let us say that we know that an input unsorted array has values that are within a given range. Then we maintain another array that keeps track of the frequency of the elements that occur in this list. We parse the array and whenever we come across the element we increment the frequency of that element by one. Value: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] Frequency: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] A: { 6, 1, 8, 3, 7, 2, 3, 9, 7 } New_Frequency: [0, 1, 1, 2, 0, 0, 1, 2, 1, 1] Using the above newly calculated frequency array we can insert elements into the sorted array: S: { 1, 2, 3, 3, 6, 7, 7, 8, 9 }","title":"Steps Done"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#sort-function","text":"CountSort ( int [] A , int n ) Input : An array of integers of lenght n where the values in A are between [ 0 , R - 1 ] Output : The sorted version of A int F [ R ] for i = 0 to ( R - 1 ) F [ i ] = 0 for i = 0 to ( n - 1 ) tmp = A [ i ] F [ tmp ] += 1 int S [ n ] int k = 0 for i = 0 to ( R - 1 ) freq = F [ i ] for j = 1 to freq S [ k ] = i k ++ return S","title":"Sort Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#algorithm-analysis","text":"","title":"Algorithm Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#space-complexity","text":"Space complexity depends on the length of the frequency array, so it is: \\(\\mathcal{O}(R)\\)","title":"Space Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week7DSA.html#time-complexity","text":"The time complexity is max of \\(\\mathcal{O}(n)\\) or \\(\\mathcal{O}(R)\\) = \\(\\mathcal{O}(max(n, R))\\) The total complexity for the last loop is: \\(\\mathcal{O}(max(n, R))\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Time Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html","text":"Week 8 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 18/Sep/2021 Topics Covered # Solving Recurrence Relations Methods of solving recurrence relations Substitution Method Recurrence Tree Method Masters Theorem Solving Recurrence Relations # Taking the example of binary search, we can see that the function intially takes in n inputs adn then does constant operations for checking the validity of the iteration and to calculate the mid element. Once this is done the array is divided by two again and the same happens again. This would look like: \\(T(n) = \\mathcal{O}(1) + T({n \\over 2})\\) \\(\\implies T(n) = \\mathcal{O}(log(n))\\) Methods of solving recurrence relations # Substitution Method # We guess a solution, and the check whether it is correct Example 1: Let us consider for binary search as \\(T(n) = \\mathcal{O}(log(n))\\) , which means that we must have \\(T(n) \\le c.(log(n))\\) for large enough \\(n\\) (for all \\(n \\ge n_0\\) ) \\(T(n) = \\mathcal{O}(1) + T({n \\over 2})\\) \\(T(n) \\le \\mathcal{O}(1) + \\mathcal{O}(log({n \\over 2}))\\) \\(T(n) \\le c_1 + c_2.log({n \\over 2})\\) \\(T(n) \\le c_1 + c_2.log(n) - c_2.log(2)\\) \\(T(n) \\le c_1 + c_2.log(n) - c_2\\) \\(T(n) \\le c_2.log(n) - (c_2 - c_1)\\) \\(\\implies T(n) \\le c_2.log(n)\\) \\(\\implies T(n) = \\mathcal{O}(log(n))\\) Example 2: Let us now consider merge sort. We know that merge sort works by splitting the array into two and then sorting the sub arrays and then merging it back. This would look like this \\(T(n) = 2.T({n \\over 2}) + \\mathcal{O}(n)\\) #### Recurrence Tree Method This analysis can be seen in the sorting algorithms that were explained earlier. #### Masters Theorem It is a direct method to get solutions for recurrences of the form \\(T(n) = a.T(n/b) + \\mathcal{O}(n^c)\\) , and \\(a \\ge 1\\) and \\(b \\gt 1\\) then there are the following three cases to obtain the solutions directly: - If \\(c \\lt log_b(a)\\) , then \\(T(n) = \\Theta(n^{log_b(a)})\\) - If \\(c = log_b(a)\\) , then \\(T(n) = \\Theta(n^c.log(n))\\) - If \\(c \\gt log_b(a)\\) , then \\(T(n) = \\Theta(n^c)\\) ##### Examples: Binary Search: - \\(T(n) = T({n \\over 2}) + \\mathcal{O}(1)\\) - \\(a = 1\\) , \\(b = 2\\) and \\(c = 0\\) - Falls under second case so \\(T(n) = \\Theta(n^0 log(n)) = \\Theta(log(n))\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html#week-8","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 18/Sep/2021","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html#topics-covered","text":"Solving Recurrence Relations Methods of solving recurrence relations Substitution Method Recurrence Tree Method Masters Theorem","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html#solving-recurrence-relations","text":"Taking the example of binary search, we can see that the function intially takes in n inputs adn then does constant operations for checking the validity of the iteration and to calculate the mid element. Once this is done the array is divided by two again and the same happens again. This would look like: \\(T(n) = \\mathcal{O}(1) + T({n \\over 2})\\) \\(\\implies T(n) = \\mathcal{O}(log(n))\\)","title":"Solving Recurrence Relations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html#methods-of-solving-recurrence-relations","text":"","title":"Methods of solving recurrence relations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week8DSA.html#substitution-method","text":"We guess a solution, and the check whether it is correct Example 1: Let us consider for binary search as \\(T(n) = \\mathcal{O}(log(n))\\) , which means that we must have \\(T(n) \\le c.(log(n))\\) for large enough \\(n\\) (for all \\(n \\ge n_0\\) ) \\(T(n) = \\mathcal{O}(1) + T({n \\over 2})\\) \\(T(n) \\le \\mathcal{O}(1) + \\mathcal{O}(log({n \\over 2}))\\) \\(T(n) \\le c_1 + c_2.log({n \\over 2})\\) \\(T(n) \\le c_1 + c_2.log(n) - c_2.log(2)\\) \\(T(n) \\le c_1 + c_2.log(n) - c_2\\) \\(T(n) \\le c_2.log(n) - (c_2 - c_1)\\) \\(\\implies T(n) \\le c_2.log(n)\\) \\(\\implies T(n) = \\mathcal{O}(log(n))\\) Example 2: Let us now consider merge sort. We know that merge sort works by splitting the array into two and then sorting the sub arrays and then merging it back. This would look like this \\(T(n) = 2.T({n \\over 2}) + \\mathcal{O}(n)\\) #### Recurrence Tree Method This analysis can be seen in the sorting algorithms that were explained earlier. #### Masters Theorem It is a direct method to get solutions for recurrences of the form \\(T(n) = a.T(n/b) + \\mathcal{O}(n^c)\\) , and \\(a \\ge 1\\) and \\(b \\gt 1\\) then there are the following three cases to obtain the solutions directly: - If \\(c \\lt log_b(a)\\) , then \\(T(n) = \\Theta(n^{log_b(a)})\\) - If \\(c = log_b(a)\\) , then \\(T(n) = \\Theta(n^c.log(n))\\) - If \\(c \\gt log_b(a)\\) , then \\(T(n) = \\Theta(n^c)\\) ##### Examples: Binary Search: - \\(T(n) = T({n \\over 2}) + \\mathcal{O}(1)\\) - \\(a = 1\\) , \\(b = 2\\) and \\(c = 0\\) - Falls under second case so \\(T(n) = \\Theta(n^0 log(n)) = \\Theta(log(n))\\) Tags: !DatastructuresAndAlgorithmsIndex","title":"Substitution Method"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html","text":"Week 9 # Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 16/Oct/2021 Topics Covered # Abstract Data Types Stack Array Implementation Queue Array Implementation Abstract Data Types # It is a mathematical model for a data type, it is a container for how data is stored. Stack # A stack is a Last In First Out (LIFO) data structure, it supports the following operations, both at the same end: 1. void push(<type> key) : This operation adds a new value to the top of the stack 2. <type> pop() : This operation removes the top most element from the stack and returns it to the caller of this function 3. int size() : This operation returns the current size of the stack 4. <Type> top()/peek() : Returns the value of the top most value 5. bool isEmpty() : Returns True when the stack is empty 6. bool isFull() : Returns True when the stack is full Applications # Check for proper parenthesisation. To see the validity of parenthesis in an expression Array Implementation # The following is an implementation for Stack<int> : class Stack { int stack [ 100 ]; int top ; Stack () { top = - 1 ; } void push ( int item ) { if ( isFull ()) { print ( \"Stack is full\" ) return } stack [ ++ top ] = item return } int pop () { if ( top != - 1 ) { return stack [ top -- ] } else { print ( \"Stack is empty\" ) return exception } } int size () { return top + 1 } int top () { return stack [ top ] } bool isEmpty () { return ( top == - 1 ) } bool isFull () { return ( top == stack . length - 1 ) } } Queue # A queue is a First In First Out (FIFO) data structure, it supports the following operations, both at the same end: 1. void enqueue(<type> key) : This operation adds a new value to the rear end of the queue 2. <type> dequeue() : This operation removes the front most element from the queue and returns it to the caller of this function 3. int size() : This operation returns the current size of the queue 4. <Type> front()/peek() : Returns the value of the front most value 5. bool isEmpty() : Returns True when the queue is empty 6. bool isFull() : Returns True when the queue is full Applications # Simulating anything that needs a queue such as a ticket counters, message queues and any Distributed computing algorithm that uses a FIFO message channels and any buffer. Array Implementation # The following is an implementation for Queue<int> : class Queue { int queue [ 100 ]; int front ; int rear ; Queue () { rear = - 1 ; front = 0 ; } void enqueue ( int item ) { if ( isFull ()) { return exception } rear = ( rear + 1 ) % queue . length queue [ rear ] = item } int dequeue () { if ( isEmpty ()) { return exception } temp = queue [ front ] front = ( front + 1 ) % queue . length return temp } int size () { size = rear - front + 1 return ( size >= 0 ) ? size : ( size + queue . length ) } int peek () { return queue [ front ] } bool isEmpty () { return ( size () == 0 ) } bool isFull () { return ( size () == queue . length ) } } Tags: !DatastructuresAndAlgorithmsIndex","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#week-9","text":"Lecturer : Pritam Bhattacharya , BITS Pilani, Goa Campus Date : 16/Oct/2021","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#topics-covered","text":"Abstract Data Types Stack Array Implementation Queue Array Implementation","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#abstract-data-types","text":"It is a mathematical model for a data type, it is a container for how data is stored.","title":"Abstract Data Types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#stack","text":"A stack is a Last In First Out (LIFO) data structure, it supports the following operations, both at the same end: 1. void push(<type> key) : This operation adds a new value to the top of the stack 2. <type> pop() : This operation removes the top most element from the stack and returns it to the caller of this function 3. int size() : This operation returns the current size of the stack 4. <Type> top()/peek() : Returns the value of the top most value 5. bool isEmpty() : Returns True when the stack is empty 6. bool isFull() : Returns True when the stack is full","title":"Stack"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#applications","text":"Check for proper parenthesisation. To see the validity of parenthesis in an expression","title":"Applications"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#array-implementation","text":"The following is an implementation for Stack<int> : class Stack { int stack [ 100 ]; int top ; Stack () { top = - 1 ; } void push ( int item ) { if ( isFull ()) { print ( \"Stack is full\" ) return } stack [ ++ top ] = item return } int pop () { if ( top != - 1 ) { return stack [ top -- ] } else { print ( \"Stack is empty\" ) return exception } } int size () { return top + 1 } int top () { return stack [ top ] } bool isEmpty () { return ( top == - 1 ) } bool isFull () { return ( top == stack . length - 1 ) } }","title":"Array Implementation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#queue","text":"A queue is a First In First Out (FIFO) data structure, it supports the following operations, both at the same end: 1. void enqueue(<type> key) : This operation adds a new value to the rear end of the queue 2. <type> dequeue() : This operation removes the front most element from the queue and returns it to the caller of this function 3. int size() : This operation returns the current size of the queue 4. <Type> front()/peek() : Returns the value of the front most value 5. bool isEmpty() : Returns True when the queue is empty 6. bool isFull() : Returns True when the queue is full","title":"Queue"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#applications_1","text":"Simulating anything that needs a queue such as a ticket counters, message queues and any Distributed computing algorithm that uses a FIFO message channels and any buffer.","title":"Applications"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Datastructures%20and%20Algorithms/Week9DSA.html#array-implementation_1","text":"The following is an implementation for Queue<int> : class Queue { int queue [ 100 ]; int front ; int rear ; Queue () { rear = - 1 ; front = 0 ; } void enqueue ( int item ) { if ( isFull ()) { return exception } rear = ( rear + 1 ) % queue . length queue [ rear ] = item } int dequeue () { if ( isEmpty ()) { return exception } temp = queue [ front ] front = ( front + 1 ) % queue . length return temp } int size () { size = rear - front + 1 return ( size >= 0 ) ? size : ( size + queue . length ) } int peek () { return queue [ front ] } bool isEmpty () { return ( size () == 0 ) } bool isFull () { return ( size () == queue . length ) } } Tags: !DatastructuresAndAlgorithmsIndex","title":"Array Implementation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/%21DistributedComputingIndex.html","text":"Distributed Computing Index # This is an Index page for all Distributed Computing Content Assignment # Assignment DCAssignment Question Papers # Mid Sem Paper DC End Sem Paper DC Course Content # Week 1 Pre Reading: PreRecordedModule12 Lecture Notes: Week1DC Week 2 Lecture Notes: Week2DC Week 3 Pre Reading: PreRecordedModule3 Lecture Notes: Week3DC Week 4 Lecture Notes: Week4DC Lecture Notes: Week4DC2 Week 5 Lecture Notes: Week5DC Week 6 Lecture Notes: Week6DC Week 7 Pre Reading: PreRecordedModule6 Lecture Notes: Week7DC Week 8 Pre Reading: PreRecordedModule7 Lecture Notes: Week8DC Week 9 Pre Reading: PreRecordedModule8 Lecture Notes: Week9DC Week 10 Lecture Notes: Week10DC Note Summary: EmbeddedNotesDC Tags: !Semester1Index","title":"Distributed Computing Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/%21DistributedComputingIndex.html#distributed-computing-index","text":"This is an Index page for all Distributed Computing Content","title":"Distributed Computing Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/%21DistributedComputingIndex.html#assignment","text":"Assignment DCAssignment","title":"Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/%21DistributedComputingIndex.html#question-papers","text":"Mid Sem Paper DC End Sem Paper DC","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/%21DistributedComputingIndex.html#course-content","text":"Week 1 Pre Reading: PreRecordedModule12 Lecture Notes: Week1DC Week 2 Lecture Notes: Week2DC Week 3 Pre Reading: PreRecordedModule3 Lecture Notes: Week3DC Week 4 Lecture Notes: Week4DC Lecture Notes: Week4DC2 Week 5 Lecture Notes: Week5DC Week 6 Lecture Notes: Week6DC Week 7 Pre Reading: PreRecordedModule6 Lecture Notes: Week7DC Week 8 Pre Reading: PreRecordedModule7 Lecture Notes: Week8DC Week 9 Pre Reading: PreRecordedModule8 Lecture Notes: Week9DC Week 10 Lecture Notes: Week10DC Note Summary: EmbeddedNotesDC Tags: !Semester1Index","title":"Course Content"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/DCAssignment.html","text":"DC Assignment # Task # Write a program to implement the Ricart-Agrawala algorithm for implementing distributed mutual exclusion. Assume the communication channels to be FIFO in nature Instructions # Put the source file(s) and a README file in one folder. Create a zipped folder containing the solution and submit the final zipped folder to the elearn portal. The README file should contain the relevant information required for compiling and executing code(s) for evaluation as well as any platform related dependency (like what operating system is required for execution). The README file should also contain the format of the input to be given to the program and any assumptions that you have made. It is mandatory to include the README file in your submission. My Submission # The code implementation along with the readme can be found here tags: !DistributedComputingIndex Assignments","title":"DC Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/DCAssignment.html#dc-assignment","text":"","title":"DC Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/DCAssignment.html#task","text":"Write a program to implement the Ricart-Agrawala algorithm for implementing distributed mutual exclusion. Assume the communication channels to be FIFO in nature","title":"Task"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/DCAssignment.html#instructions","text":"Put the source file(s) and a README file in one folder. Create a zipped folder containing the solution and submit the final zipped folder to the elearn portal. The README file should contain the relevant information required for compiling and executing code(s) for evaluation as well as any platform related dependency (like what operating system is required for execution). The README file should also contain the format of the input to be given to the program and any assumptions that you have made. It is mandatory to include the README file in your submission.","title":"Instructions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/DCAssignment.html#my-submission","text":"The code implementation along with the readme can be found here tags: !DistributedComputingIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/EmbeddedNotesDC.html","text":"Distributed Computing Notes # Prerecorded Lecture Notes # Live Lecture Notes #","title":"Distributed Computing Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/EmbeddedNotesDC.html#distributed-computing-notes","text":"","title":"Distributed Computing Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/EmbeddedNotesDC.html#prerecorded-lecture-notes","text":"","title":"Prerecorded Lecture Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/EmbeddedNotesDC.html#live-lecture-notes","text":"","title":"Live Lecture Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html","text":"End Sem DC Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # Question 8 # tags: !DistributedComputingIndex QuestionPapers","title":"End Sem DC Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#end-sem-dc-paper","text":"","title":"End Sem DC Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-7","text":"","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/End%20Sem%20Paper%20DC.html#question-8","text":"tags: !DistributedComputingIndex QuestionPapers","title":"Question 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html","text":"Mid Sem DC Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DistributedComputingIndex QuestionPapers","title":"Mid Sem DC Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#mid-sem-dc-paper","text":"","title":"Mid Sem DC Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Mid%20Sem%20Paper%20DC.html#question-7","text":"tags: !DistributedComputingIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html","text":"Module 1 # Session 1 # Relation between software components # The middleware and the distributed application abstracts the fact that the application is being run by multiple computers to the user. Some popular examples of Distributed computing: 1. Tor distributed network 2. Internet of Things: A network of several entities (ranging from small sensors to full fledged PCs) connected over a network so that an application can make sense of this data as a whole 3. Cloud computing is also a distributed computing paradigm. Session 2 # Multiprocessor Vs Multi-computer systems # Multiprocessor # Multiple CPUs connected with multiple memory banks through a high speed connection network (Uniform or non uniform memory access model). One cannot allow a more than one processor access the same memory bank unless some sort of synchronization is done to serialize the arbitrate the access request. In a Symmetric multi processor machine all CPUs have the autonomy of deciding what it needs to be done (No master slave) In an Asymmetric multi processor machine, there is a concept of master slave where the master processor decide what the other processors needs to compute. The multiple processors can connect to a particular memory bank when a particular switch is clicked on in the case of cross point switch , this method is not feasible since the number of cross connection can be limiting for very large configurations of CPU and memory banks In case of an omega switching network , each omega switch is arranged in stages and the layers allow for lesser switches to pair the CPUs to the memory banks In both of the above cases the s Multi-computer # Multiple computers are connected over a high speed connection network, where the latency involved in this communication depends on the type of distributed computing designed (Varies from cluster computing, grid computing, IOT, or a common consumer computers connected together across the interweb) Unlike multiprocessor systems, the computers themselves do not need switching networks as explained above but can be connected over networks (LAN, WAN, MAN) Pros and Cons of Distributed systems # Pros Communication and resource sharing possible Price to performance ratio is better More reliable and easily scalable Potential for incremental growth Cons Requires OSs to be able to work in a distributed mode(same goes for applications) High speed reliable network connectivity is essential (Like GARUDA Grid) Security and privacy (Similar to networking, the risks of getting hacked affects distributed computers as well) Design issues # Scaling # If possible use asynchronous communication, since more the synchronous communication the worse it scales (Only if possible) One can also move parts of computation(To the client) in order to reduce latency from the server side Here you see that the form checking is done on the client side which will make the form processing part of the server more scalable. DNS is a good example for distributed query processing where a whole URL Other design issues # Lack of global knowledge Naming (Name resolutions done by DNS is a challenging task) Compatibility (When multiple resources are used together all must be compatible) Process synchronization Resource management (Hard to arbiterate tasks) Security Session 3 # RPC (Remote Procedure Call) # Issues: Identifying and accessing the remote procedure Parameters required to run the procedure Return values that need to be given once the call is sompleted Examples: Sun RPC Microsoft DCOM OMG's CORBA Java RMI CML/RPC SOAP/.NET AJAX(Asynch. Javascript and XML) In an RPC, there is a client machine that calls the procedures to the client-side stub (Which abstracts the remote calls) The binding server handles the queries by the client-side stub to know the server address to which the remote procedure must be sent to. The client processes the data (Like conversion of big endian and little endian differences) by something called marshalling parameters The server side stub unmarshals the params given and a call is done to do the work and the return value is again marshalled and sent to the client side stub The client again unmarshalls and gives the result back A given procedure can be identified by the following: 1. Hostname (IP Address) 2. Program identifier (32 bit integers) 3. Procedure identifier (32 bit integers) 4. Program Version identifier Example of RPC programming Create a RPC file as follows: struct square_in { long arg1 }; struct square_out { long res1 }; program SQUARE_PROG { version SQUARE_VERS { square_out SQUAREPROC(square_in) = 1; } = 1; } = 0x13451111; Take that file and give it to rpcgen (Developed by sun microsystems) the rpc gen automatically produces the necessary files. Now we need to write the Client side logic to make the Remote call: Here we are including the \"square.h\" file where we can get access to the structs for the input, and even a client handle to pass the required params to send to the processing server Now we need to write the server side logic to process the remote call: Here we define the remote function that needs to be run based on the conventions put on by sun microsystems Module 2 # Session 1 # What is a distributed program? # A program that consists of a set of n asynchronous processes . These processes do not share a global memory and use only message passing APIs for coms. These process do not share a global clock that is accessible at the same time. The process These process executions and message transfers are asynchronous . Model of distributed executions # Given 3 processes \\(p1\\) , \\(p2\\) and \\(p3\\) , we represent them in a space-time diagram where Time is in x and space is in y axis. Here we see the relationships between the events for each process Event Process Type \\(e1\\) \\(P1\\) Local Event \\(e2\\) \\(P1\\) Message Send Event (to \\(e5\\) ) \\(e3\\) \\(P1\\) Message Receive Event (from \\(e4\\) ) \\(e4\\) \\(P2\\) Message Send Event (to \\(e3\\) ) \\(e5\\) \\(P2\\) Message Receive Event (from \\(e2\\) ) \\(e6\\) \\(P3\\) Local Event \\(e7\\) \\(P3\\) Local Event \\(e8\\) \\(P3\\) Local Event A local event is an event that is local to a given process A Message send event is an event in a process which asynchronously sent a message to another process A Message receive event is an event in a process which asynchronously receives a message from another process The diagram above shows us causal-effect relationships between events across different processes that gives us an idea on how to design a distributed system Models of communication networks # There are namely 3: 1. FIFO 2. Non FIFO 3. Causal Ordering Consider the following processes ( \\(P1\\) , \\(P2\\) and \\(P3\\) ) with communication channels( \\(c1\\) , \\(c2\\) , \\(c3\\) and \\(c4\\) ): graph LR P1--c1-->P2 P2--c2-->P3 P1--c3-->P3 P3--c4-->P1 If \\(P2\\) sends messages \\(m1\\) first and then \\(m2\\) to \\(P3\\) through the channel \\(c2\\) , then if \\(c2\\) was a FIFO channel then \\(P3\\) receives \\(m1\\) first and then \\(m2\\) , making FIFO channels very predictable. If \\(P3\\) sends four messages \\(m1\\) , \\(m2\\) , \\(m3\\) and \\(m4\\) through the channel \\(c4\\) , then if \\(c4\\) was a Non FIFO channel then \\(P1\\) would receive the messages in random order. This is useful if the algorithms in place can handle receiving messages in random order. If a channel follows causal order, then we do not need to check if a message was sent in a particular order or not, since for every message sent there is a receipt even in a process and the ordering is maintained by the receiving end of the process. Global state of a DS # A local state of a process is the current execution state at which a particular process is in. A Global state of a DS is a collection of such above mentioned local states of processes and channels The global state \\(GS\\) is defined as: \\[GS = \\{\\ \\bigcup_i\\ LS_i^{xi}\\ ,\\ \\bigcup_{j,k}\\ SC_{jk}^{yj,\\ zk}\\ \\}\\] \\(GS\\) is the union of all Local States of all machines and all the messages across all the message channels. - A \\(GS\\) is meaningful only when all the states of all the components of the DS are recorded at the same instant - The only way that is possible is when all the processes are synchronous in nature or if there were a global, instantaneously accessible clock (Both of which are impossible in a typical DS) - To calculate the \\(GS\\) there exists multiple algorithms to calculate the same Consistent global state # - In the above diagram if we were to calculate the \\(GS_1\\) at the red line drawn, we can say that the \\(GS_1\\) is the sum of \\(LS_1\\) , \\(LS_2\\) , \\(LS_3\\) and \\(LS_4\\) . Here the \\(GS\\) is strongly consistent since there are no send or receive messages crossing in that slice of time. - In the same diagram \\(GS_2\\) is the sum of \\(LS_5\\) , \\(LS_6\\) , \\(LS_7\\) and \\(LS_8\\) . Here the \\(GS\\) does have a record of the message sent by \\(e_1^4\\) but not the state of the message being received by \\(e_3^5\\) , hence the \\(GS\\) although consistent enough to use it for calculating typical things such as deadlocks etc but just not strongly consistent. - An \\(GS\\) is said to be inconsistent if the local states were calculated in a way where the message receive event is considered but the corresponding message sent is missed. This makes the \\(GS\\) is rendered useless. Session 2 # Logical Clocks # Lamport Logical Clocks (Scalar Time) - Causality among events in a DS is a way of analyzing processes and infer details regarding the computation. - The knowledge of causal precedence relation among several events of different processes helps solve a variety of problems in a DS. Scalar Time # In a scalar time clock, we condense the process, its local view of the global time into a single variable called \\(C_i\\) . With the above assumption there exists 2 rules to update the internal clock: Rule 1 : Before executing an event, process \\(p_i\\) executes the clock value to be as: \\[ C_i := C_i + d\\ \\ \\ (d > 0) \\] In general, every time \\(R_1\\) is executed, \\(d\\) can also have a different value. 2. Rule 2 : Each message piggybacks the clock value of its sender at sending time. When a process \\(p_i\\) receives a message with timestamp \\(C{msg}\\) , it executes the following actions: \\[ C_i := max(C_i, C_{msg}) \\] Execute \\(R_1\\) Deliver the message Example: Looking at the above scenario we can infer that if one event ( \\(e_i\\) ) sends a message to another event( \\(e_j\\) ) we can say that: \\[ for\\ two\\ events\\ e_i\\ and\\ e_j\\ ,\\ e_i\\ ->\\ e_j =>C(e_i) < C(e_j) \\] But the reverse is not true. - There are the following problems when it comes to this clock : - Non consistency : We can never say that the event \\(e_1\\) had happened before \\(e_5\\) merely because the clock value of one was lesser than the other. This is because there is no direct or indirect link that connects \\(e_1\\) to \\(e_5\\) . - There is a problem when the clock values are same for two events. In this case we can take precedence by considering the precedence of the process itself (Take the process index values into consideration). Considering the above example \\(e_1\\) and \\(e_3\\) have the same clock values so we can never say which one happened before what, hence we can use the indexes of the process itself, since index of process \\(P_2\\) is greater than process \\(P_1\\) we can assume that \\(e_1\\) has happened before \\(e_3\\) Tags: !DistributedComputingIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#module-1","text":"","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#session-1","text":"","title":"Session 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#relation-between-software-components","text":"The middleware and the distributed application abstracts the fact that the application is being run by multiple computers to the user. Some popular examples of Distributed computing: 1. Tor distributed network 2. Internet of Things: A network of several entities (ranging from small sensors to full fledged PCs) connected over a network so that an application can make sense of this data as a whole 3. Cloud computing is also a distributed computing paradigm.","title":"Relation between software components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#session-2","text":"","title":"Session 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#multiprocessor-vs-multi-computer-systems","text":"","title":"Multiprocessor Vs Multi-computer systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#multiprocessor","text":"Multiple CPUs connected with multiple memory banks through a high speed connection network (Uniform or non uniform memory access model). One cannot allow a more than one processor access the same memory bank unless some sort of synchronization is done to serialize the arbitrate the access request. In a Symmetric multi processor machine all CPUs have the autonomy of deciding what it needs to be done (No master slave) In an Asymmetric multi processor machine, there is a concept of master slave where the master processor decide what the other processors needs to compute. The multiple processors can connect to a particular memory bank when a particular switch is clicked on in the case of cross point switch , this method is not feasible since the number of cross connection can be limiting for very large configurations of CPU and memory banks In case of an omega switching network , each omega switch is arranged in stages and the layers allow for lesser switches to pair the CPUs to the memory banks In both of the above cases the s","title":"Multiprocessor"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#multi-computer","text":"Multiple computers are connected over a high speed connection network, where the latency involved in this communication depends on the type of distributed computing designed (Varies from cluster computing, grid computing, IOT, or a common consumer computers connected together across the interweb) Unlike multiprocessor systems, the computers themselves do not need switching networks as explained above but can be connected over networks (LAN, WAN, MAN)","title":"Multi-computer"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#pros-and-cons-of-distributed-systems","text":"Pros Communication and resource sharing possible Price to performance ratio is better More reliable and easily scalable Potential for incremental growth Cons Requires OSs to be able to work in a distributed mode(same goes for applications) High speed reliable network connectivity is essential (Like GARUDA Grid) Security and privacy (Similar to networking, the risks of getting hacked affects distributed computers as well)","title":"Pros and Cons of Distributed systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#design-issues","text":"","title":"Design issues"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#scaling","text":"If possible use asynchronous communication, since more the synchronous communication the worse it scales (Only if possible) One can also move parts of computation(To the client) in order to reduce latency from the server side Here you see that the form checking is done on the client side which will make the form processing part of the server more scalable. DNS is a good example for distributed query processing where a whole URL","title":"Scaling"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#other-design-issues","text":"Lack of global knowledge Naming (Name resolutions done by DNS is a challenging task) Compatibility (When multiple resources are used together all must be compatible) Process synchronization Resource management (Hard to arbiterate tasks) Security","title":"Other design issues"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#session-3","text":"","title":"Session 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#rpc-remote-procedure-call","text":"Issues: Identifying and accessing the remote procedure Parameters required to run the procedure Return values that need to be given once the call is sompleted Examples: Sun RPC Microsoft DCOM OMG's CORBA Java RMI CML/RPC SOAP/.NET AJAX(Asynch. Javascript and XML) In an RPC, there is a client machine that calls the procedures to the client-side stub (Which abstracts the remote calls) The binding server handles the queries by the client-side stub to know the server address to which the remote procedure must be sent to. The client processes the data (Like conversion of big endian and little endian differences) by something called marshalling parameters The server side stub unmarshals the params given and a call is done to do the work and the return value is again marshalled and sent to the client side stub The client again unmarshalls and gives the result back A given procedure can be identified by the following: 1. Hostname (IP Address) 2. Program identifier (32 bit integers) 3. Procedure identifier (32 bit integers) 4. Program Version identifier Example of RPC programming Create a RPC file as follows: struct square_in { long arg1 }; struct square_out { long res1 }; program SQUARE_PROG { version SQUARE_VERS { square_out SQUAREPROC(square_in) = 1; } = 1; } = 0x13451111; Take that file and give it to rpcgen (Developed by sun microsystems) the rpc gen automatically produces the necessary files. Now we need to write the Client side logic to make the Remote call: Here we are including the \"square.h\" file where we can get access to the structs for the input, and even a client handle to pass the required params to send to the processing server Now we need to write the server side logic to process the remote call: Here we define the remote function that needs to be run based on the conventions put on by sun microsystems","title":"RPC (Remote Procedure Call)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#module-2","text":"","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#session-1_1","text":"","title":"Session 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#what-is-a-distributed-program","text":"A program that consists of a set of n asynchronous processes . These processes do not share a global memory and use only message passing APIs for coms. These process do not share a global clock that is accessible at the same time. The process These process executions and message transfers are asynchronous .","title":"What is a distributed program?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#model-of-distributed-executions","text":"Given 3 processes \\(p1\\) , \\(p2\\) and \\(p3\\) , we represent them in a space-time diagram where Time is in x and space is in y axis. Here we see the relationships between the events for each process Event Process Type \\(e1\\) \\(P1\\) Local Event \\(e2\\) \\(P1\\) Message Send Event (to \\(e5\\) ) \\(e3\\) \\(P1\\) Message Receive Event (from \\(e4\\) ) \\(e4\\) \\(P2\\) Message Send Event (to \\(e3\\) ) \\(e5\\) \\(P2\\) Message Receive Event (from \\(e2\\) ) \\(e6\\) \\(P3\\) Local Event \\(e7\\) \\(P3\\) Local Event \\(e8\\) \\(P3\\) Local Event A local event is an event that is local to a given process A Message send event is an event in a process which asynchronously sent a message to another process A Message receive event is an event in a process which asynchronously receives a message from another process The diagram above shows us causal-effect relationships between events across different processes that gives us an idea on how to design a distributed system","title":"Model of distributed executions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#models-of-communication-networks","text":"There are namely 3: 1. FIFO 2. Non FIFO 3. Causal Ordering Consider the following processes ( \\(P1\\) , \\(P2\\) and \\(P3\\) ) with communication channels( \\(c1\\) , \\(c2\\) , \\(c3\\) and \\(c4\\) ): graph LR P1--c1-->P2 P2--c2-->P3 P1--c3-->P3 P3--c4-->P1 If \\(P2\\) sends messages \\(m1\\) first and then \\(m2\\) to \\(P3\\) through the channel \\(c2\\) , then if \\(c2\\) was a FIFO channel then \\(P3\\) receives \\(m1\\) first and then \\(m2\\) , making FIFO channels very predictable. If \\(P3\\) sends four messages \\(m1\\) , \\(m2\\) , \\(m3\\) and \\(m4\\) through the channel \\(c4\\) , then if \\(c4\\) was a Non FIFO channel then \\(P1\\) would receive the messages in random order. This is useful if the algorithms in place can handle receiving messages in random order. If a channel follows causal order, then we do not need to check if a message was sent in a particular order or not, since for every message sent there is a receipt even in a process and the ordering is maintained by the receiving end of the process.","title":"Models of communication networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#global-state-of-a-ds","text":"A local state of a process is the current execution state at which a particular process is in. A Global state of a DS is a collection of such above mentioned local states of processes and channels The global state \\(GS\\) is defined as: \\[GS = \\{\\ \\bigcup_i\\ LS_i^{xi}\\ ,\\ \\bigcup_{j,k}\\ SC_{jk}^{yj,\\ zk}\\ \\}\\] \\(GS\\) is the union of all Local States of all machines and all the messages across all the message channels. - A \\(GS\\) is meaningful only when all the states of all the components of the DS are recorded at the same instant - The only way that is possible is when all the processes are synchronous in nature or if there were a global, instantaneously accessible clock (Both of which are impossible in a typical DS) - To calculate the \\(GS\\) there exists multiple algorithms to calculate the same","title":"Global state of a DS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#consistent-global-state","text":"- In the above diagram if we were to calculate the \\(GS_1\\) at the red line drawn, we can say that the \\(GS_1\\) is the sum of \\(LS_1\\) , \\(LS_2\\) , \\(LS_3\\) and \\(LS_4\\) . Here the \\(GS\\) is strongly consistent since there are no send or receive messages crossing in that slice of time. - In the same diagram \\(GS_2\\) is the sum of \\(LS_5\\) , \\(LS_6\\) , \\(LS_7\\) and \\(LS_8\\) . Here the \\(GS\\) does have a record of the message sent by \\(e_1^4\\) but not the state of the message being received by \\(e_3^5\\) , hence the \\(GS\\) although consistent enough to use it for calculating typical things such as deadlocks etc but just not strongly consistent. - An \\(GS\\) is said to be inconsistent if the local states were calculated in a way where the message receive event is considered but the corresponding message sent is missed. This makes the \\(GS\\) is rendered useless.","title":"Consistent global state"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#session-2_1","text":"","title":"Session 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#logical-clocks","text":"Lamport Logical Clocks (Scalar Time) - Causality among events in a DS is a way of analyzing processes and infer details regarding the computation. - The knowledge of causal precedence relation among several events of different processes helps solve a variety of problems in a DS.","title":"Logical Clocks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule12.html#scalar-time","text":"In a scalar time clock, we condense the process, its local view of the global time into a single variable called \\(C_i\\) . With the above assumption there exists 2 rules to update the internal clock: Rule 1 : Before executing an event, process \\(p_i\\) executes the clock value to be as: \\[ C_i := C_i + d\\ \\ \\ (d > 0) \\] In general, every time \\(R_1\\) is executed, \\(d\\) can also have a different value. 2. Rule 2 : Each message piggybacks the clock value of its sender at sending time. When a process \\(p_i\\) receives a message with timestamp \\(C{msg}\\) , it executes the following actions: \\[ C_i := max(C_i, C_{msg}) \\] Execute \\(R_1\\) Deliver the message Example: Looking at the above scenario we can infer that if one event ( \\(e_i\\) ) sends a message to another event( \\(e_j\\) ) we can say that: \\[ for\\ two\\ events\\ e_i\\ and\\ e_j\\ ,\\ e_i\\ ->\\ e_j =>C(e_i) < C(e_j) \\] But the reverse is not true. - There are the following problems when it comes to this clock : - Non consistency : We can never say that the event \\(e_1\\) had happened before \\(e_5\\) merely because the clock value of one was lesser than the other. This is because there is no direct or indirect link that connects \\(e_1\\) to \\(e_5\\) . - There is a problem when the clock values are same for two events. In this case we can take precedence by considering the precedence of the process itself (Take the process index values into consideration). Considering the above example \\(e_1\\) and \\(e_3\\) have the same clock values so we can never say which one happened before what, hence we can use the indexes of the process itself, since index of process \\(P_2\\) is greater than process \\(P_1\\) we can assume that \\(e_1\\) has happened before \\(e_3\\) Tags: !DistributedComputingIndex","title":"Scalar Time"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html","text":"Module 3 # Global State Recording # Uses of Global State Recording # Recording a consistent state of the GS, checkpointing in fault tolerance (rollback/recovery) Detecting stable properties in a distributed system via snapshots. A property is stable if once it holds in a state, it holds in all subsequent states: Termination detection Deadlock detection Issues in recording GS # How to distinguish between the messages to be recorded in the snapshot from those not to be recorded Any message sent prior to recording must be recorded Any message that is sent by a process after recording its snapshot must not be considered How to determine the instant when a process takes its snapshot A process \\(p_i\\) must record its snapshot before processing a message \\(m_{ij}\\) that was sent by process \\(p_i\\) after recording its snapshot Note that since DS are non deterministic in nature so there need not be just one event happening at a given instant of time Chandy Lamport Algorithm (FIFO) # Uses a marker whose purpose is to differentiate messages to be included in the snapshot and the ones to not be included After a snapshot is recorded, the marker is sent to all outgoing channels before any other message is processed Snapshot must be recorded no later than when it receives a marker on any of its incoming channels When all the process have received a marker on all of its incoming channels it is terminated The algo can be initiated by a process by executing the marker sending rule by which it records its local state and sends a marker on each outgoing channel When a marker received a process can record its LS and then do the marker sending rule Examples # - Now consider the 50 sent by A reaches B and balance becomes 150 - B receives the Marker M - B records its state as 150 - A recorded its state as 50 - B sends marker to c_2 and c_3 - C receives marker from B - C records state as 200 - C sends marker through c_4 - A receives two markers from B and C - Since A has recorded its LS - A checks if messages are there in c_2 and C_4 - since they are empty, it marks the state of channel as empty The global state of this system if we sum all the LS of the processes it looks like it is consistent Lai-Yang Algorithm (non-FIFO) # Every process is initially white When a process decides to take a snapshot it becomes red Every process sent by a white/red process is colored white/red respectively Thus a white/red message is a message that was sent before/after the sender of that message recorded its local snapshot Every white process takes its snapshot at its convenience, but no later than the instant it receives a red message Every white process records a history of all white messages sent or received When a process turns red, it sends these histories along with its snapshot to the initiator process The initiator process evaluates transit ( \\(LS_i\\) , \\(LS_j\\) ) to compute the state of a channel \\(C_{ij}\\) as given below: \\(SC_{ij}\\) \\(=\\) white messages sent by \\(P_i\\) on \\(C_ij\\) - white messages received by \\(P_j\\) on \\(C_ij\\) Tags: !DistributedComputingIndex","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#module-3","text":"","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#global-state-recording","text":"","title":"Global State Recording"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#uses-of-global-state-recording","text":"Recording a consistent state of the GS, checkpointing in fault tolerance (rollback/recovery) Detecting stable properties in a distributed system via snapshots. A property is stable if once it holds in a state, it holds in all subsequent states: Termination detection Deadlock detection","title":"Uses of Global State Recording"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#issues-in-recording-gs","text":"How to distinguish between the messages to be recorded in the snapshot from those not to be recorded Any message sent prior to recording must be recorded Any message that is sent by a process after recording its snapshot must not be considered How to determine the instant when a process takes its snapshot A process \\(p_i\\) must record its snapshot before processing a message \\(m_{ij}\\) that was sent by process \\(p_i\\) after recording its snapshot Note that since DS are non deterministic in nature so there need not be just one event happening at a given instant of time","title":"Issues in recording GS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#chandy-lamport-algorithm-fifo","text":"Uses a marker whose purpose is to differentiate messages to be included in the snapshot and the ones to not be included After a snapshot is recorded, the marker is sent to all outgoing channels before any other message is processed Snapshot must be recorded no later than when it receives a marker on any of its incoming channels When all the process have received a marker on all of its incoming channels it is terminated The algo can be initiated by a process by executing the marker sending rule by which it records its local state and sends a marker on each outgoing channel When a marker received a process can record its LS and then do the marker sending rule","title":"Chandy Lamport Algorithm (FIFO)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#examples","text":"- Now consider the 50 sent by A reaches B and balance becomes 150 - B receives the Marker M - B records its state as 150 - A recorded its state as 50 - B sends marker to c_2 and c_3 - C receives marker from B - C records state as 200 - C sends marker through c_4 - A receives two markers from B and C - Since A has recorded its LS - A checks if messages are there in c_2 and C_4 - since they are empty, it marks the state of channel as empty The global state of this system if we sum all the LS of the processes it looks like it is consistent","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule3.html#lai-yang-algorithm-non-fifo","text":"Every process is initially white When a process decides to take a snapshot it becomes red Every process sent by a white/red process is colored white/red respectively Thus a white/red message is a message that was sent before/after the sender of that message recorded its local snapshot Every white process takes its snapshot at its convenience, but no later than the instant it receives a red message Every white process records a history of all white messages sent or received When a process turns red, it sends these histories along with its snapshot to the initiator process The initiator process evaluates transit ( \\(LS_i\\) , \\(LS_j\\) ) to compute the state of a channel \\(C_{ij}\\) as given below: \\(SC_{ij}\\) \\(=\\) white messages sent by \\(P_i\\) on \\(C_ij\\) - white messages received by \\(P_j\\) on \\(C_ij\\) Tags: !DistributedComputingIndex","title":"Lai-Yang Algorithm (non-FIFO)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html","text":"Module 4 # Birman Schiper Stephenson BSS Protocol # This algorithm works with broadcasting messages Before broadcasting \\(m\\) , process \\(P_i\\) increments vector time \\(VT_{pi}[i]\\) and timestamps \\(m\\) Other processes receives this \\(VT_m\\) from \\(P_i\\) The process delays the receive till the following two conditions are met: \\(VT_{Pj}[i] = VT_m[i] - 1\\) , this means \\(P_j\\) received all other messages from \\(P_i\\) \\(VT_{Pj}[k] >= VT_m[k]\\) for every \\(k \\in \\{1, 2, ... n\\} = \\{i\\}\\) , meaning \\(P_j\\) received all messages also received by \\(P_i\\) before sending message \\(m\\) . When \\(P_j\\) delivers \\(m\\) , \\(VT_{Pj}\\) updated by IR2 of Vector clock Example 1 # - \\(P_2\\) receives \\(m_2\\) first with a \\(VT = (2, 0)\\) - At \\(P_2\\) the \\(VT = (0, 0)\\) and \\((0, 0) \\ne (2, 0) - (1, 0)\\) - Since this rule is not satisfied, \\(m_2\\) is kept in queue - \\(P_2\\) receives \\(m_1\\) now with a \\(VT = (1, 0)\\) - At \\(P_2\\) the \\(VT\\) is still \\((0, 0)\\) , and the first condition check satisfies since \\((0, 0) = (1, 0) - (1, 0)\\) - Also second rule is satisfied as well. - Since the rule is satisfied, \\(m_1\\) is received, and \\(VT_2\\) is updated to \\((1, 0)\\) - \\(P_2\\) now checks condition for the message \\(m_2\\) in queue. Now since the \\(VT\\) at \\(P_2\\) is \\((1, 0)\\) , the two rules are satisfied, and the message \\(m_2\\) is now received. and the \\(VT\\) is updated to \\(max(VT_2, VT_1) = (2, 0)\\) Example 2 # - \\(P_1\\) sends message to \\(P_2\\) and \\(P_3\\) with \\(VT_1 = (1, 0, 0)\\) - \\(P_2\\) has \\(VT_2 = (0, 0, 0)\\) - \\(P_2\\) receives message from \\(P_1\\) - \\(VT_1 = (1, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - This message is received by \\(P_2\\) and \\(VT_2 = (1, 0, 0)\\) - \\(P_2\\) receives another message from \\(P_1\\) - \\(VT_1 = (2, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - This message is received by \\(P_2\\) and \\(VT_2 = (2, 0, 0)\\) - \\(P_3\\) has \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_1\\) - \\(VT_1 = (2, 0, 0)\\) does not satisfy the first condition - This message \\(m_{2P1}\\) is kept in queue - \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_2\\) - \\(VT_1 = (2, 1, 0)\\) satisfies the first condition - But it does not satisfy the second one because the 2 messages from \\(i\\) are not yet received - This message \\(m_{1P2}\\) is kept in queue - \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_1\\) - \\(VT_1 = (1, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - Since both are satisfied, \\(VT_3 = (1, 0, 0)\\) - Since \\(VT_3\\) is \\((1, 0, 0)\\) - The message \\(m_{2P1}\\) with \\(VT_1 = (2, 0, 0)\\) , satisfies the first condition and the second condition - Now this message is marked received and updates the \\(VT_3\\) to \\((2, 0, 0)\\) - Since \\(VT_3\\) is \\((2 0, 0)\\) - The message \\(m_{1P2}\\) with \\(VT_2 = (2, 1, 0)\\) , satisfies the first condition and the second condition - Now this message is marked received and updates the \\(VT_3\\) to \\((2, 1, 0)\\) - \\(P_1\\) has \\(VT_1 = (2, 0, 0)\\) - \\(P_1\\) receives message from \\(P_2\\) - This message has \\(VT_2 = (2, 1, 0)\\) - This VT satisfies the first condition - This VT also satisfies the second one as well - This message is marked as received and \\(VT_1\\) is updated to \\((2, 1, 0)\\) Schiper-Eggli-Sandoz SES protocol # There is no need for broadcast messages Each process maintains a vector \\(V_P\\) of size \\(N - 1\\) , \\(N\\) being the number of processes in the system \\(V_P\\) is a vector of tuple \\((P' , t)\\) : \\(P'\\) is the destination process id and \\(t\\) is the vector timestamp \\(T_m\\) : logical time of sending message \\(m\\) \\(T_{Pi}\\) : present logical time at \\(P_i\\) Initially \\(V_P\\) is empty Example 1 # - \\(P_1\\) sends \\(m_1\\) with \\((1, 0)\\{\\}\\) to \\(P_2\\) - \\(VP_1 = \\{(P_2, 10)\\}\\) - \\(P_1\\) sends \\(m_2\\) with \\((2, 0)\\{(P_2, 10)\\}\\) to \\(P_2\\) - \\(VP_1 = \\{(P_2, 20)\\}\\) - \\(P_2\\) receives message \\(m_2\\) from \\(P_1\\) containing \\((2, 0)\\{(P_2, 10)\\}\\) - Since \\(VP_1\\) has a tuple for \\(P_2\\) , and since the clock value is greater than the local clock, the message is buffered - \\(P_2\\) receives message \\(m_1\\) from \\(P_1\\) containing \\((1, 0)\\{\\}\\) - Since the \\(VP_1\\) does not have a tuple for \\(P_2\\) the message is received - And after receiving we apply IR2 to get \\(T_2\\) to be \\((1, 0)\\) and then apply IR1 to get \\((1, 1)\\) - The buffered message is now considered for receiving - Since the tuple has \\(t_m = (1, 0)\\) , the \\(T_2\\) becomes \\((2, 1)\\) after IR2 and then \\((2, 2)\\) after IR1 Example 2 # - \\(P_1\\) sends \\(m_{13}\\) with \\((1, 0, 0)\\{\\}\\) - \\(P_1\\) sends \\(m_{12}\\) with \\((2, 0, 0)\\{(P_3, 100)\\}\\) - \\(P_2\\) receives message from \\(P_1\\) , since the vector does not have an entry for P_2 the message is delivered and the local clock becomes \\(max(000, 200) + 010 = 200 + 010 = 210\\) and \\(VP_2\\) now has \\(\\{(P_3, 100)\\}\\) - \\(P_2\\) sends a message to \\(P_3\\) with \\((2, 2, 0)\\{(P_3, 100)\\}\\) data - \\(P_3\\) receives message from \\(P_2\\) - since the vector has entry for \\(P_3\\) and the clock is higher than local clock it is buffered - \\(P_3\\) receives message from \\(P_1\\) - Since vector is empty the message is received - The local clock of \\(P_3\\) becomes \\(max(000, 100) + 001 = 101\\) - \\(P_3\\) receives buffered message - Since 101 is greater than 100 the message is received - Now the local time becomes \\(max(101, 220) + 001 = 222\\) Tags: !DistributedComputingIndex","title":"Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#module-4","text":"","title":"Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#birman-schiper-stephenson-bss-protocol","text":"This algorithm works with broadcasting messages Before broadcasting \\(m\\) , process \\(P_i\\) increments vector time \\(VT_{pi}[i]\\) and timestamps \\(m\\) Other processes receives this \\(VT_m\\) from \\(P_i\\) The process delays the receive till the following two conditions are met: \\(VT_{Pj}[i] = VT_m[i] - 1\\) , this means \\(P_j\\) received all other messages from \\(P_i\\) \\(VT_{Pj}[k] >= VT_m[k]\\) for every \\(k \\in \\{1, 2, ... n\\} = \\{i\\}\\) , meaning \\(P_j\\) received all messages also received by \\(P_i\\) before sending message \\(m\\) . When \\(P_j\\) delivers \\(m\\) , \\(VT_{Pj}\\) updated by IR2 of Vector clock","title":"Birman Schiper Stephenson BSS Protocol"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#example-1","text":"- \\(P_2\\) receives \\(m_2\\) first with a \\(VT = (2, 0)\\) - At \\(P_2\\) the \\(VT = (0, 0)\\) and \\((0, 0) \\ne (2, 0) - (1, 0)\\) - Since this rule is not satisfied, \\(m_2\\) is kept in queue - \\(P_2\\) receives \\(m_1\\) now with a \\(VT = (1, 0)\\) - At \\(P_2\\) the \\(VT\\) is still \\((0, 0)\\) , and the first condition check satisfies since \\((0, 0) = (1, 0) - (1, 0)\\) - Also second rule is satisfied as well. - Since the rule is satisfied, \\(m_1\\) is received, and \\(VT_2\\) is updated to \\((1, 0)\\) - \\(P_2\\) now checks condition for the message \\(m_2\\) in queue. Now since the \\(VT\\) at \\(P_2\\) is \\((1, 0)\\) , the two rules are satisfied, and the message \\(m_2\\) is now received. and the \\(VT\\) is updated to \\(max(VT_2, VT_1) = (2, 0)\\)","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#example-2","text":"- \\(P_1\\) sends message to \\(P_2\\) and \\(P_3\\) with \\(VT_1 = (1, 0, 0)\\) - \\(P_2\\) has \\(VT_2 = (0, 0, 0)\\) - \\(P_2\\) receives message from \\(P_1\\) - \\(VT_1 = (1, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - This message is received by \\(P_2\\) and \\(VT_2 = (1, 0, 0)\\) - \\(P_2\\) receives another message from \\(P_1\\) - \\(VT_1 = (2, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - This message is received by \\(P_2\\) and \\(VT_2 = (2, 0, 0)\\) - \\(P_3\\) has \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_1\\) - \\(VT_1 = (2, 0, 0)\\) does not satisfy the first condition - This message \\(m_{2P1}\\) is kept in queue - \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_2\\) - \\(VT_1 = (2, 1, 0)\\) satisfies the first condition - But it does not satisfy the second one because the 2 messages from \\(i\\) are not yet received - This message \\(m_{1P2}\\) is kept in queue - \\(VT_3 = (0, 0, 0)\\) - \\(P_3\\) receives message from \\(P_1\\) - \\(VT_1 = (1, 0, 0)\\) satisfies the first condition - It also satisfies the second condition - Since both are satisfied, \\(VT_3 = (1, 0, 0)\\) - Since \\(VT_3\\) is \\((1, 0, 0)\\) - The message \\(m_{2P1}\\) with \\(VT_1 = (2, 0, 0)\\) , satisfies the first condition and the second condition - Now this message is marked received and updates the \\(VT_3\\) to \\((2, 0, 0)\\) - Since \\(VT_3\\) is \\((2 0, 0)\\) - The message \\(m_{1P2}\\) with \\(VT_2 = (2, 1, 0)\\) , satisfies the first condition and the second condition - Now this message is marked received and updates the \\(VT_3\\) to \\((2, 1, 0)\\) - \\(P_1\\) has \\(VT_1 = (2, 0, 0)\\) - \\(P_1\\) receives message from \\(P_2\\) - This message has \\(VT_2 = (2, 1, 0)\\) - This VT satisfies the first condition - This VT also satisfies the second one as well - This message is marked as received and \\(VT_1\\) is updated to \\((2, 1, 0)\\)","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#schiper-eggli-sandoz-ses-protocol","text":"There is no need for broadcast messages Each process maintains a vector \\(V_P\\) of size \\(N - 1\\) , \\(N\\) being the number of processes in the system \\(V_P\\) is a vector of tuple \\((P' , t)\\) : \\(P'\\) is the destination process id and \\(t\\) is the vector timestamp \\(T_m\\) : logical time of sending message \\(m\\) \\(T_{Pi}\\) : present logical time at \\(P_i\\) Initially \\(V_P\\) is empty","title":"Schiper-Eggli-Sandoz SES protocol"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#example-1_1","text":"- \\(P_1\\) sends \\(m_1\\) with \\((1, 0)\\{\\}\\) to \\(P_2\\) - \\(VP_1 = \\{(P_2, 10)\\}\\) - \\(P_1\\) sends \\(m_2\\) with \\((2, 0)\\{(P_2, 10)\\}\\) to \\(P_2\\) - \\(VP_1 = \\{(P_2, 20)\\}\\) - \\(P_2\\) receives message \\(m_2\\) from \\(P_1\\) containing \\((2, 0)\\{(P_2, 10)\\}\\) - Since \\(VP_1\\) has a tuple for \\(P_2\\) , and since the clock value is greater than the local clock, the message is buffered - \\(P_2\\) receives message \\(m_1\\) from \\(P_1\\) containing \\((1, 0)\\{\\}\\) - Since the \\(VP_1\\) does not have a tuple for \\(P_2\\) the message is received - And after receiving we apply IR2 to get \\(T_2\\) to be \\((1, 0)\\) and then apply IR1 to get \\((1, 1)\\) - The buffered message is now considered for receiving - Since the tuple has \\(t_m = (1, 0)\\) , the \\(T_2\\) becomes \\((2, 1)\\) after IR2 and then \\((2, 2)\\) after IR1","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule4.html#example-2_1","text":"- \\(P_1\\) sends \\(m_{13}\\) with \\((1, 0, 0)\\{\\}\\) - \\(P_1\\) sends \\(m_{12}\\) with \\((2, 0, 0)\\{(P_3, 100)\\}\\) - \\(P_2\\) receives message from \\(P_1\\) , since the vector does not have an entry for P_2 the message is delivered and the local clock becomes \\(max(000, 200) + 010 = 200 + 010 = 210\\) and \\(VP_2\\) now has \\(\\{(P_3, 100)\\}\\) - \\(P_2\\) sends a message to \\(P_3\\) with \\((2, 2, 0)\\{(P_3, 100)\\}\\) data - \\(P_3\\) receives message from \\(P_2\\) - since the vector has entry for \\(P_3\\) and the clock is higher than local clock it is buffered - \\(P_3\\) receives message from \\(P_1\\) - Since vector is empty the message is received - The local clock of \\(P_3\\) becomes \\(max(000, 100) + 001 = 101\\) - \\(P_3\\) receives buffered message - Since 101 is greater than 100 the message is received - Now the local time becomes \\(max(101, 220) + 001 = 222\\) Tags: !DistributedComputingIndex","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html","text":"Module 5 # Mutual Exclusion # Challenges while using shared resources # Simultaneous updates and read to a directory or a file Two processes sending their data to a printer can create chaos to a shared printer So there is a problem of giving exclusive access to the processes that are asking for these resources Requirements to be satisfied for such algos # - Performance is measured in terms of - No. of messages required for CS invocation - Synchronization delay - Response time - System throughput = \\(1/(sd + E)\\) A centralized algorithm # - A central controller \\(C\\) with a queue \\(Q\\) for deferring replies - Request, reply and release messages would have to be sent - Not reliable since one node is responsible and there are several performance bottlenecks Lamports Distributed Mutual Exclusion # Requesting the critical section # When a \\(S_i\\) wants to enter the \\(CS\\) , it sends a \\(REQUEST(T=tsi, i)\\) message to all the sites in its request set \\(R_i\\) and places the request on \\(RQ_i\\) When a site \\(S_j\\) receives a \\(REQUEST(tsi, i)\\) message from the site \\(S_i\\) , it returns a timestamped \\(REPLY\\) message to \\(S_i\\) and places site \\(S_i\\) request on \\(RQ_j\\) Executing the CS # Site \\(S_i\\) enters the CS when the two following conditions hold: 1. \\(S_i\\) has received a message with timestamp larger than \\((tsi, i)\\) from all other sites 2. \\(S_i\\) request is at the top of the \\(RQ_i\\) Releasing the CS # Site \\(S_i\\) , upon exiting the \\(CS\\) , removes its request from the top of its \\(RQ\\) and sends a timestamped \\(RELEASE\\) message to all the sites in its request set \\(R_i\\) When a site \\(S_j\\) receives this \\(RELEASE\\) message, it removes the \\(REQUEST\\) form \\(S_i\\) from its \\(RQ\\) After this, its own process can come to the top of the queue and enter into the \\(CS\\) Example # Correctness # There can never be two different sites \\(S_i\\) and \\(S_j\\) being in the \\(CS\\) since, the \\(RQ\\) makes sure the order of requests is maintained The total Number of messages \\(= 3 (N - 1)\\) \\(REQUEST = N - 1\\) \\(REPLY = N - 1\\) \\(RELEASE = N - 1\\) Ricart Agrawala # Requesting Site # A site P_i sends a message REQUEST(ts, i) to all sites Receiving Site # Upon receiving the request message, the site \\(P_j\\) will immediately send a timestamped \\(REPLY(ts, j)\\) message if and only if : \\(P_j\\) is not requesting or executing the \\(CS\\) OR \\(P_j\\) is requesting the CS buyt sent a request with a higher \\(ts\\) than the \\(ts\\) of \\(REQUEST\\) \\(P_i\\) Else, \\(P_j\\) will defer the \\(REPLY\\) to \\(P_i\\) Example # - \\(P_2\\) sends \\(REPLY\\) to \\(P_3\\) and \\(P_1\\) immediately \\(P_1\\) sends \\(REPLY\\) to \\(P_3\\) since the \\(ts\\) in the \\(REQUEST\\) is \\(1\\) and the \\(ts\\) for \\(P_1\\) \\(REQUEST\\) is \\(2\\) and \\(1 < 2\\) \\(P_3\\) defers \\(REPLY\\) to \\(P_1\\) since the \\(ts\\) in the \\(REQUEST\\) is \\(2\\) and the \\(ts\\) for \\(P_3\\) \\(REQUEST\\) is \\(1\\) and \\(2 > 1\\) \\(P_3\\) enters into \\(CS\\) since both the replies are received. Once \\(P_3\\) finishes executing it will see the deferred queue to see who to send the reply to. After sending reply \\(P_1\\) goes into \\(CS\\) Maekawas Algorithm # Some properties # Request Set rules # Also \\(N = K(K - 1) + 1\\) Example Request sets # \\(N = 3\\) \\(N = 7\\) \\(N = 13\\) Algorithm # Example # Normal Execution # Deadlock Case # Handling Deadlocks # Tags: !DistributedComputingIndex","title":"Module 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#module-5","text":"","title":"Module 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#mutual-exclusion","text":"","title":"Mutual Exclusion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#challenges-while-using-shared-resources","text":"Simultaneous updates and read to a directory or a file Two processes sending their data to a printer can create chaos to a shared printer So there is a problem of giving exclusive access to the processes that are asking for these resources","title":"Challenges while using shared resources"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#requirements-to-be-satisfied-for-such-algos","text":"- Performance is measured in terms of - No. of messages required for CS invocation - Synchronization delay - Response time - System throughput = \\(1/(sd + E)\\)","title":"Requirements to be satisfied for such algos"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#a-centralized-algorithm","text":"- A central controller \\(C\\) with a queue \\(Q\\) for deferring replies - Request, reply and release messages would have to be sent - Not reliable since one node is responsible and there are several performance bottlenecks","title":"A centralized algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#lamports-distributed-mutual-exclusion","text":"","title":"Lamports Distributed Mutual Exclusion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#requesting-the-critical-section","text":"When a \\(S_i\\) wants to enter the \\(CS\\) , it sends a \\(REQUEST(T=tsi, i)\\) message to all the sites in its request set \\(R_i\\) and places the request on \\(RQ_i\\) When a site \\(S_j\\) receives a \\(REQUEST(tsi, i)\\) message from the site \\(S_i\\) , it returns a timestamped \\(REPLY\\) message to \\(S_i\\) and places site \\(S_i\\) request on \\(RQ_j\\)","title":"Requesting the critical section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#executing-the-cs","text":"Site \\(S_i\\) enters the CS when the two following conditions hold: 1. \\(S_i\\) has received a message with timestamp larger than \\((tsi, i)\\) from all other sites 2. \\(S_i\\) request is at the top of the \\(RQ_i\\)","title":"Executing the CS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#releasing-the-cs","text":"Site \\(S_i\\) , upon exiting the \\(CS\\) , removes its request from the top of its \\(RQ\\) and sends a timestamped \\(RELEASE\\) message to all the sites in its request set \\(R_i\\) When a site \\(S_j\\) receives this \\(RELEASE\\) message, it removes the \\(REQUEST\\) form \\(S_i\\) from its \\(RQ\\) After this, its own process can come to the top of the queue and enter into the \\(CS\\)","title":"Releasing the CS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#example","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#correctness","text":"There can never be two different sites \\(S_i\\) and \\(S_j\\) being in the \\(CS\\) since, the \\(RQ\\) makes sure the order of requests is maintained The total Number of messages \\(= 3 (N - 1)\\) \\(REQUEST = N - 1\\) \\(REPLY = N - 1\\) \\(RELEASE = N - 1\\)","title":"Correctness"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#ricart-agrawala","text":"","title":"Ricart Agrawala"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#requesting-site","text":"A site P_i sends a message REQUEST(ts, i) to all sites","title":"Requesting Site"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#receiving-site","text":"Upon receiving the request message, the site \\(P_j\\) will immediately send a timestamped \\(REPLY(ts, j)\\) message if and only if : \\(P_j\\) is not requesting or executing the \\(CS\\) OR \\(P_j\\) is requesting the CS buyt sent a request with a higher \\(ts\\) than the \\(ts\\) of \\(REQUEST\\) \\(P_i\\) Else, \\(P_j\\) will defer the \\(REPLY\\) to \\(P_i\\)","title":"Receiving Site"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#example_1","text":"- \\(P_2\\) sends \\(REPLY\\) to \\(P_3\\) and \\(P_1\\) immediately \\(P_1\\) sends \\(REPLY\\) to \\(P_3\\) since the \\(ts\\) in the \\(REQUEST\\) is \\(1\\) and the \\(ts\\) for \\(P_1\\) \\(REQUEST\\) is \\(2\\) and \\(1 < 2\\) \\(P_3\\) defers \\(REPLY\\) to \\(P_1\\) since the \\(ts\\) in the \\(REQUEST\\) is \\(2\\) and the \\(ts\\) for \\(P_3\\) \\(REQUEST\\) is \\(1\\) and \\(2 > 1\\) \\(P_3\\) enters into \\(CS\\) since both the replies are received. Once \\(P_3\\) finishes executing it will see the deferred queue to see who to send the reply to. After sending reply \\(P_1\\) goes into \\(CS\\)","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#maekawas-algorithm","text":"","title":"Maekawas Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#some-properties","text":"","title":"Some properties"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#request-set-rules","text":"Also \\(N = K(K - 1) + 1\\)","title":"Request Set rules"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#example-request-sets","text":"\\(N = 3\\) \\(N = 7\\) \\(N = 13\\)","title":"Example Request sets"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#algorithm","text":"","title":"Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#example_2","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#normal-execution","text":"","title":"Normal Execution"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#deadlock-case","text":"","title":"Deadlock Case"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule5.html#handling-deadlocks","text":"Tags: !DistributedComputingIndex","title":"Handling Deadlocks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html","text":"Module 6 # What is a deadlock? # One or more processes waiting indefinitely for resources to be released by other waiting processes Can occur on h/w or s/w resources, and is mostly seen in DBs (Lock and Unlock events in DBs) Here you can see as time goes on, \\(T_1\\) locks on to resource \\(x\\) , once that is done, \\(T_2\\) cannot access \\(x\\) until it is unlocked, and similarly even \\(T_1\\) has to wait for \\(T_2\\) to unlock \\(y\\) , this is a deadlock since both have locked the resource each other needs to complete its tasks and are waiting indefinitely. Conditions for deadlock # Mutual Exclusion # If a resource is supposed to be used by more than one node in a DS, that resource cannot be shared across nodes, and the nodes must wait for the node using the resource to finish with its tasks, such a scenario is called mutual exclusion Hold and Wait # In the above diagram, the assignment edge denotes the resource \\(x\\) being held by the process \\(P_1\\) and \\(P_1\\) is waiting for the resource \\(y\\) after requesting for it. Such a scenario is required to say that a process could be in a potential deadlock No Preemption # For a deadlock to happen, one must not be able to take away a resource that is being used by a process. Only if a resource cannot be preempted from a process, the other proccess's request could potentially go into deadlock. Circular Wait # Here you can see that two processes P_1 and P_2 are waiting for resouirces that each other holds back to complete its tasks. Such a situation most likely leads to deadlocks Ways to handle deadlocks # Prevention : One can prevent a deadlock from happening by making sure that a resource is preemptive for example. One can also give all the necessary resources that a process requires in the beginning itself so that no two processes can request for each others locked resources. Avoidance : One can avoid deadlocks by making someone arbitrate a resource and make a judicious decision before giving a resource to a process Bankers algorithm: An algorithm that determines if it is safe or unsafe to give a resource to a process and try to find an optimal way of giving access to them to avoid a deadlock from happening. Avoidance usually incurs heavy overhead in a big DS and is even complex to build such systems Detection and Resolution : One can detect a cycle in a set of processes, and take a decision to kill a process inorder to resolve the deadlock. Only one process needs to be killed to resolve a deadlock, so decision can be taken based on some sort of priority The most practical solution for handling deadlocks in a DS Ignorance Cycle Vs Knot # The AND model of requests requires all resources currently being requested to be granted to unblobk a computation A cycle is sufficient condition to declare a deadlock with this model The OR modelof requests allows a computation making multiple resource requests to unbock as soon as any one is granted A cycle is necessary condition A knot is a sufficient condition. A knot is when a path is taken in an OR graph, there is no way out of that path leading to getting locked. Consider the AND graph shown below: We can say that the cycle is sufficient to say that the system is in deadlock Consider the following OR graph shown below: Here the cycle is not sufficient because P_1 can finish the process if it takes the other path out of this cycle Now consider another OR graph shown below: Here you can see that even the other path becomes a cycle to \\(P_1\\) through \\(R_4\\) , such a situation is called as a knot Detection requirements # Progress All deadlocks found Deadlocks found in finite time Safety No false deadlock detection Phantom deadlocks caused by network latenecies Chandy-Misra-Haas (CMH Edge-Chasing for AND Graphs) # Some processes wait for local resources Some processes wait for resources on other machines Algorithm invoked when a process has to wait for a resource for a longer period Uses local WFGs (Wait For Graphs) to detect local deadlocks and probes to determine the existence of global deadlocks \\(i\\) the processor that is blocked and initiating the algo \\(j\\) The process sending the probe message \\(k\\) The receiver of the probe message Example 1 # - \\(P_1\\) sends \\((1, 1, 2)\\) - \\(P_2\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 2, 3)\\) - \\(P_3\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 3, 4)\\) - \\(P_4\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 4, 5)\\) and \\((1, 4, 6)\\) - \\(P_5\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 5, 7)\\) - \\(P_6\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 6, 8)\\) - Since \\(P_8\\) is not dependent on anything it will finish using the resource that \\(P_6\\) wants and release it. It will also not forward the probe message since it is not dependent on other processes - \\(P_7\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 7, 9)\\) - \\(P_9\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 9, 1)\\) - Since \\(i == k\\) in the last probe message, we can conclude that the system is in a deadlock Example 2 # - When the probe messages travel the path \\(P_1 \\rightarrow P_2 \\rightarrow P_3 \\rightarrow P_4 \\rightarrow P_5\\) , there is no apparent cycle seen - Similarly when the probe messages travel the path \\(P_1 \\rightarrow P_4 \\rightarrow P_5\\) , there is no apparent cycle seen. - But when the probe message takes the path \\(P_1 \\rightarrow P_2 \\rightarrow P_6 \\rightarrow P_7 \\rightarrow P_1\\) , we can see that there is a cycle so we can conclude that there is a deadlock situation in the WFG. Advantages of and Disadvantages of CMH algorithm # Advantages # Popular, variants of this are used in locking schemes Easy to implement as each message is of fixed length and requires few computational steps No graph constructing and information collection False deadlocks are not detected Does not require a particular structure among processes Disadvantages # Two or more processes may independently detect the same deadlock and hence while resolving, several processes could potentially be aborted. Even though a process detects a deadlock, it does not know the full cycle. \\(M(n - 1)/2\\) messages required to detect deadlock, where \\(M\\) = no. of processes, \\(n\\) = no. of sites. OR WFGs # Consider the following OR WFG: In the above graph you can see that there is a cycle between \\(P_1\\) and \\(P_2\\) , but since this is an OR WFG we can say that there is no deadlock since \\(P_1\\) can take the resource \\(P_3\\) has in its hold and still cater to \\(P_2\\) 's needs hence a cycle is not sufficient to determine deadlocks Now consider the following OR WFG: You can see that \\(P_1\\) and \\(P_3\\) also have a cyclic dependency, so that would mean that in an OR graph such a system where no matter what path takes would lead to a cycle will determine the potential of a deadlock. Such a situation is called a knot. Chandy-Misra-Haas (CMH Diffusion Computation for OR Graphs) # Example 1 # If we apply the above algorithm, the messages being send from \\(P_1\\) is all marked in red and the state of the wait and the state of variable number is updated based on the messages sent. When the message comes to \\(P_1\\) finally, it will send a reply (Marked in Green) and the values of \\(n\\) is decremented whenever a reply is received. A reply is sent back only if \\(n == 0\\) . If for some case the \\(n\\) does not become \\(0\\) , then the reply is not sent, so since P_i is sending several non enage queries, these will end up getting replies from P_2 since its wait is true and since the number of sent messages is not equal to the number of received messages, there is no deadlock. The below WFG is the result of applying this algorithm: In the above example if we add an edge from \\(P_8 \\rightarrow P_9\\) , then \\(P_4\\) would also get a reply from \\(P_8\\) and this will make its \\(n == 0\\) and it will also send a reply back. This will go till \\(P_1\\) and since all the sent messages from \\(P_1\\) is replied to we can conclude that this system has a knot and it is in a deadlock state. Example 2 # In the above example the message sent from \\(P_1\\) will never receive a reply from \\(P_2\\) since there wont be a reply from \\(P_5\\) , even though the other path gives a reply to \\(P_2\\) . So this algorrithm makes sure that a cycle alone does not constitute a deadlock since there is another path that can be taken. Deadlock Persistence and Resolution # Deadlock Persistence: Average time a deadlock exists before it is resolved Deadlock Resolution: Aborting at least one process/request involved in the deadlock Efficient resolution of deadlock requires knowledge of all processes and resources If every process detects a deadlock and tries to resolve it independently it is highly inefficient Several processes might be aborted Tags: !DistributedComputingIndex","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#module-6","text":"","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#what-is-a-deadlock","text":"One or more processes waiting indefinitely for resources to be released by other waiting processes Can occur on h/w or s/w resources, and is mostly seen in DBs (Lock and Unlock events in DBs) Here you can see as time goes on, \\(T_1\\) locks on to resource \\(x\\) , once that is done, \\(T_2\\) cannot access \\(x\\) until it is unlocked, and similarly even \\(T_1\\) has to wait for \\(T_2\\) to unlock \\(y\\) , this is a deadlock since both have locked the resource each other needs to complete its tasks and are waiting indefinitely.","title":"What is a deadlock?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#conditions-for-deadlock","text":"","title":"Conditions for deadlock"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#mutual-exclusion","text":"If a resource is supposed to be used by more than one node in a DS, that resource cannot be shared across nodes, and the nodes must wait for the node using the resource to finish with its tasks, such a scenario is called mutual exclusion","title":"Mutual Exclusion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#hold-and-wait","text":"In the above diagram, the assignment edge denotes the resource \\(x\\) being held by the process \\(P_1\\) and \\(P_1\\) is waiting for the resource \\(y\\) after requesting for it. Such a scenario is required to say that a process could be in a potential deadlock","title":"Hold and Wait"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#no-preemption","text":"For a deadlock to happen, one must not be able to take away a resource that is being used by a process. Only if a resource cannot be preempted from a process, the other proccess's request could potentially go into deadlock.","title":"No Preemption"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#circular-wait","text":"Here you can see that two processes P_1 and P_2 are waiting for resouirces that each other holds back to complete its tasks. Such a situation most likely leads to deadlocks","title":"Circular Wait"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#ways-to-handle-deadlocks","text":"Prevention : One can prevent a deadlock from happening by making sure that a resource is preemptive for example. One can also give all the necessary resources that a process requires in the beginning itself so that no two processes can request for each others locked resources. Avoidance : One can avoid deadlocks by making someone arbitrate a resource and make a judicious decision before giving a resource to a process Bankers algorithm: An algorithm that determines if it is safe or unsafe to give a resource to a process and try to find an optimal way of giving access to them to avoid a deadlock from happening. Avoidance usually incurs heavy overhead in a big DS and is even complex to build such systems Detection and Resolution : One can detect a cycle in a set of processes, and take a decision to kill a process inorder to resolve the deadlock. Only one process needs to be killed to resolve a deadlock, so decision can be taken based on some sort of priority The most practical solution for handling deadlocks in a DS Ignorance","title":"Ways to handle deadlocks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#cycle-vs-knot","text":"The AND model of requests requires all resources currently being requested to be granted to unblobk a computation A cycle is sufficient condition to declare a deadlock with this model The OR modelof requests allows a computation making multiple resource requests to unbock as soon as any one is granted A cycle is necessary condition A knot is a sufficient condition. A knot is when a path is taken in an OR graph, there is no way out of that path leading to getting locked. Consider the AND graph shown below: We can say that the cycle is sufficient to say that the system is in deadlock Consider the following OR graph shown below: Here the cycle is not sufficient because P_1 can finish the process if it takes the other path out of this cycle Now consider another OR graph shown below: Here you can see that even the other path becomes a cycle to \\(P_1\\) through \\(R_4\\) , such a situation is called as a knot","title":"Cycle Vs Knot"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#detection-requirements","text":"Progress All deadlocks found Deadlocks found in finite time Safety No false deadlock detection Phantom deadlocks caused by network latenecies","title":"Detection requirements"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#chandy-misra-haas-cmh-edge-chasing-for-and-graphs","text":"Some processes wait for local resources Some processes wait for resources on other machines Algorithm invoked when a process has to wait for a resource for a longer period Uses local WFGs (Wait For Graphs) to detect local deadlocks and probes to determine the existence of global deadlocks \\(i\\) the processor that is blocked and initiating the algo \\(j\\) The process sending the probe message \\(k\\) The receiver of the probe message","title":"Chandy-Misra-Haas (CMH Edge-Chasing for AND Graphs)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#example-1","text":"- \\(P_1\\) sends \\((1, 1, 2)\\) - \\(P_2\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 2, 3)\\) - \\(P_3\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 3, 4)\\) - \\(P_4\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 4, 5)\\) and \\((1, 4, 6)\\) - \\(P_5\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 5, 7)\\) - \\(P_6\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 6, 8)\\) - Since \\(P_8\\) is not dependent on anything it will finish using the resource that \\(P_6\\) wants and release it. It will also not forward the probe message since it is not dependent on other processes - \\(P_7\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 7, 9)\\) - \\(P_9\\) sets \\(dependent\\) is set to \\(TRUE\\) and sends \\((1, 9, 1)\\) - Since \\(i == k\\) in the last probe message, we can conclude that the system is in a deadlock","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#example-2","text":"- When the probe messages travel the path \\(P_1 \\rightarrow P_2 \\rightarrow P_3 \\rightarrow P_4 \\rightarrow P_5\\) , there is no apparent cycle seen - Similarly when the probe messages travel the path \\(P_1 \\rightarrow P_4 \\rightarrow P_5\\) , there is no apparent cycle seen. - But when the probe message takes the path \\(P_1 \\rightarrow P_2 \\rightarrow P_6 \\rightarrow P_7 \\rightarrow P_1\\) , we can see that there is a cycle so we can conclude that there is a deadlock situation in the WFG.","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#advantages-of-and-disadvantages-of-cmh-algorithm","text":"","title":"Advantages of and Disadvantages of CMH algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#advantages","text":"Popular, variants of this are used in locking schemes Easy to implement as each message is of fixed length and requires few computational steps No graph constructing and information collection False deadlocks are not detected Does not require a particular structure among processes","title":"Advantages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#disadvantages","text":"Two or more processes may independently detect the same deadlock and hence while resolving, several processes could potentially be aborted. Even though a process detects a deadlock, it does not know the full cycle. \\(M(n - 1)/2\\) messages required to detect deadlock, where \\(M\\) = no. of processes, \\(n\\) = no. of sites.","title":"Disadvantages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#or-wfgs","text":"Consider the following OR WFG: In the above graph you can see that there is a cycle between \\(P_1\\) and \\(P_2\\) , but since this is an OR WFG we can say that there is no deadlock since \\(P_1\\) can take the resource \\(P_3\\) has in its hold and still cater to \\(P_2\\) 's needs hence a cycle is not sufficient to determine deadlocks Now consider the following OR WFG: You can see that \\(P_1\\) and \\(P_3\\) also have a cyclic dependency, so that would mean that in an OR graph such a system where no matter what path takes would lead to a cycle will determine the potential of a deadlock. Such a situation is called a knot.","title":"OR WFGs"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#chandy-misra-haas-cmh-diffusion-computation-for-or-graphs","text":"","title":"Chandy-Misra-Haas (CMH Diffusion Computation for OR Graphs)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#example-1_1","text":"If we apply the above algorithm, the messages being send from \\(P_1\\) is all marked in red and the state of the wait and the state of variable number is updated based on the messages sent. When the message comes to \\(P_1\\) finally, it will send a reply (Marked in Green) and the values of \\(n\\) is decremented whenever a reply is received. A reply is sent back only if \\(n == 0\\) . If for some case the \\(n\\) does not become \\(0\\) , then the reply is not sent, so since P_i is sending several non enage queries, these will end up getting replies from P_2 since its wait is true and since the number of sent messages is not equal to the number of received messages, there is no deadlock. The below WFG is the result of applying this algorithm: In the above example if we add an edge from \\(P_8 \\rightarrow P_9\\) , then \\(P_4\\) would also get a reply from \\(P_8\\) and this will make its \\(n == 0\\) and it will also send a reply back. This will go till \\(P_1\\) and since all the sent messages from \\(P_1\\) is replied to we can conclude that this system has a knot and it is in a deadlock state.","title":"Example 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#example-2_1","text":"In the above example the message sent from \\(P_1\\) will never receive a reply from \\(P_2\\) since there wont be a reply from \\(P_5\\) , even though the other path gives a reply to \\(P_2\\) . So this algorrithm makes sure that a cycle alone does not constitute a deadlock since there is another path that can be taken.","title":"Example 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule6.html#deadlock-persistence-and-resolution","text":"Deadlock Persistence: Average time a deadlock exists before it is resolved Deadlock Resolution: Aborting at least one process/request involved in the deadlock Efficient resolution of deadlock requires knowledge of all processes and resources If every process detects a deadlock and tries to resolve it independently it is highly inefficient Several processes might be aborted Tags: !DistributedComputingIndex","title":"Deadlock Persistence and Resolution"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html","text":"Module 7 # Agreement Protocols # The need for such algorithms is to solve the problem of getting into a consensus When a distributed system engage in cooperative efforts, process failure can become a critical factor Processors may fail in various ways, and their failure modes are central to the ability of healthy processors to detect and respond to such failures. The system model/Assumptions # There are n processors in the system and at most m of them can be faulty The processors can directly communicate with other processors via messages A receiver computation always knows the identity of a sending computation The communication system is reliable Communication Requirements # This model does not work in an asynchronous model Synchronous communication model is assumed in this section: Healthy processors receive and reply to messages in a lockstep manner This sequence is called a round In the synch-comm model, the processes know what messages they expect to receive in a round Processor Failures # Crash fault: Abrupt halt etc Omission fault: Processor omits to send required messages to some other processors Malicious fault: Processor behaves randomly and arbitrarily, these faults are also known as Byzantine faults. Message Types # Authenticated messages (signed): assure the receiver of correct identification of the sender Non authenticated messages (oral): are subject to intermediate manipulation Agreement Problems # Problem Who Initiates Value Final Agreement Byzantine Agreement One Processor Single Value Consensus All Processors Single Value Interactive Consistency All Processors A vector of values What happened at Byzantine # Byzantine Agreement # Impossibility Condition # Theorem : There is no algorithm to solve this problem with only oral messages, unless more than two thirds of the generals are loyal Impossible if, \\(n \\le 3f\\) for \\(n\\) processes, \\(f\\) of which are faulty. Oral messages are under control of the sender: Sender can alter messages that it received before forwarding it Case 1 # Case 2a # Case 2b # Case 3 # Oral Message Algorithm # Example with \\(m = 1\\) and \\(n = 4\\) # Another Example with \\(m = 1\\) and \\(n = 4\\) # Example with \\(m = 2\\) and \\(n = 7\\) # consider \\(7\\) nodes numbered \\(n1\\) to \\(n7\\) and assume \\(n6\\) and \\(n7\\) are faulty. Round 1: \\(n1\\) sends message to all the other nodes as part of the \\(OM(2)\\) operation. Round 2: \\(n2\\) to \\(n7\\) will run \\(OM(1)\\) Round 3: The other nodes that did not participate in \\(OM(1)\\) in the repective node will also send the values to other nodes Once this tree is constructed, we need to find the max of the values in the respective vectors and decide what the values are in each node. Complexity of OM # Looking at the above graph we see that the number of messages required grows with every \\(OM\\) stage and the final number at the \\(OM(0)\\) would look something like: This results in a complexity of \\(\\mathcal{O}(n^m)\\) Interactive Consistency Model # Signed message algorithm # Tags: !DistributedComputingIndex","title":"Module 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#module-7","text":"","title":"Module 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#agreement-protocols","text":"The need for such algorithms is to solve the problem of getting into a consensus When a distributed system engage in cooperative efforts, process failure can become a critical factor Processors may fail in various ways, and their failure modes are central to the ability of healthy processors to detect and respond to such failures.","title":"Agreement Protocols"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#the-system-modelassumptions","text":"There are n processors in the system and at most m of them can be faulty The processors can directly communicate with other processors via messages A receiver computation always knows the identity of a sending computation The communication system is reliable","title":"The system model/Assumptions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#communication-requirements","text":"This model does not work in an asynchronous model Synchronous communication model is assumed in this section: Healthy processors receive and reply to messages in a lockstep manner This sequence is called a round In the synch-comm model, the processes know what messages they expect to receive in a round","title":"Communication Requirements"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#processor-failures","text":"Crash fault: Abrupt halt etc Omission fault: Processor omits to send required messages to some other processors Malicious fault: Processor behaves randomly and arbitrarily, these faults are also known as Byzantine faults.","title":"Processor Failures"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#message-types","text":"Authenticated messages (signed): assure the receiver of correct identification of the sender Non authenticated messages (oral): are subject to intermediate manipulation","title":"Message Types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#agreement-problems","text":"Problem Who Initiates Value Final Agreement Byzantine Agreement One Processor Single Value Consensus All Processors Single Value Interactive Consistency All Processors A vector of values","title":"Agreement Problems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#what-happened-at-byzantine","text":"","title":"What happened at Byzantine"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#byzantine-agreement","text":"","title":"Byzantine Agreement"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#impossibility-condition","text":"Theorem : There is no algorithm to solve this problem with only oral messages, unless more than two thirds of the generals are loyal Impossible if, \\(n \\le 3f\\) for \\(n\\) processes, \\(f\\) of which are faulty. Oral messages are under control of the sender: Sender can alter messages that it received before forwarding it","title":"Impossibility Condition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#case-1","text":"","title":"Case 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#case-2a","text":"","title":"Case 2a"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#case-2b","text":"","title":"Case 2b"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#case-3","text":"","title":"Case 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#oral-message-algorithm","text":"","title":"Oral Message Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#example-with-m--1-and-n--4","text":"","title":"Example with \\(m = 1\\) and \\(n = 4\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#another-example-with-m--1-and-n--4","text":"","title":"Another Example with \\(m = 1\\) and \\(n = 4\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#example-with-m--2-and-n--7","text":"consider \\(7\\) nodes numbered \\(n1\\) to \\(n7\\) and assume \\(n6\\) and \\(n7\\) are faulty. Round 1: \\(n1\\) sends message to all the other nodes as part of the \\(OM(2)\\) operation. Round 2: \\(n2\\) to \\(n7\\) will run \\(OM(1)\\) Round 3: The other nodes that did not participate in \\(OM(1)\\) in the repective node will also send the values to other nodes Once this tree is constructed, we need to find the max of the values in the respective vectors and decide what the values are in each node.","title":"Example with \\(m = 2\\) and \\(n = 7\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#complexity-of-om","text":"Looking at the above graph we see that the number of messages required grows with every \\(OM\\) stage and the final number at the \\(OM(0)\\) would look something like: This results in a complexity of \\(\\mathcal{O}(n^m)\\)","title":"Complexity of OM"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#interactive-consistency-model","text":"","title":"Interactive Consistency Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule7.html#signed-message-algorithm","text":"Tags: !DistributedComputingIndex","title":"Signed message algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html","text":"Module 8 # P2P Architecture # Centralized P2P Architecture # Decentralized P2P Architecture # Examples of several P2P applications # Data Indexing and Overlays # The data in a P2P networks is identified by using indexing. Data indexing allows the physical data independence from the applications. Centralized indexing # Uses on or few central servers to store references to the data on many peers. The DNS lookup as well as the lookup of early p2p networks such as Napster used a central directory lookup. Local Indexing # This requires peer to index only the local data objects and remote objects need to be searched for. This form of indexing is typically used unstructured overlays in conjunction with flooding search or random walk search. Distributed Indexing # Classification of P2P overlay networks # Structured overlay network Unstructured overlay networks Unstructured P2P networks # BitTorrent # Gnutella query flooding # Tags: !DistributedComputingIndex","title":"Module 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#module-8","text":"","title":"Module 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#p2p-architecture","text":"","title":"P2P Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#centralized-p2p-architecture","text":"","title":"Centralized P2P Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#decentralized-p2p-architecture","text":"","title":"Decentralized P2P Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#examples-of-several-p2p-applications","text":"","title":"Examples of several P2P applications"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#data-indexing-and-overlays","text":"The data in a P2P networks is identified by using indexing. Data indexing allows the physical data independence from the applications.","title":"Data Indexing and Overlays"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#centralized-indexing","text":"Uses on or few central servers to store references to the data on many peers. The DNS lookup as well as the lookup of early p2p networks such as Napster used a central directory lookup.","title":"Centralized indexing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#local-indexing","text":"This requires peer to index only the local data objects and remote objects need to be searched for. This form of indexing is typically used unstructured overlays in conjunction with flooding search or random walk search.","title":"Local Indexing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#distributed-indexing","text":"","title":"Distributed Indexing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#classification-of-p2p-overlay-networks","text":"Structured overlay network Unstructured overlay networks","title":"Classification of P2P overlay networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#unstructured-p2p-networks","text":"","title":"Unstructured P2P networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#bittorrent","text":"","title":"BitTorrent"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/PreRecordedModule8.html#gnutella-query-flooding","text":"Tags: !DistributedComputingIndex","title":"Gnutella query flooding"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html","text":"Week 10 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 30/Oct/2021 Scalable Lookup # Each node \\(i\\) maintains a routing table called finger table \\(x^{th}\\) entry \\((1 \\le x \\le m)\\) is the node identifier of the node \\(succ(i + 2^{x - 1})\\) Size of the finger table is bounded by \\(m\\) entries Search is highly scalable For query on \\(key\\) at node \\(i\\) , if key lies between \\(i\\) and its successor, then \\(key\\) would reside at the successor and the successors address is returned Else the finger table is searched integer: successor <- initial value integer: predecessor <- initial value integer: finger [1...m] Clustering for Massive Parallelism # Computer cluster Consists of a collection of interconnected stand-alone/complete computers Cooperatively work together as a single, integrated computing resource Explores parallelism job level Benefits of clusters Scalable performance HA fault tolerance Modular growth Use of commodity components Design Objectives of Computer Clusters # Packaging # Cluster nodes can be packaged in a compact or a slack fashion Compact CLuster: Nodes are closely packaged in one or more racks sitting in a room Nodes are not attached to peripherals Slack Cluster Nodes are attached to their usual peripherals May be located in different rooms, Different buildings, or even remote regions Control # Cluster can be managed in a centralized or decentralized fashion Compact cluster normally has centralized control Slack cluster can be controlled either way Centralized Cluster: nodes are owned, managed and administered by a central operator Decentralized Cluster Nodes have individual owners Lacks a single point of control Homogeneity # Homogenous cluster Uses nodes from the same platform/architecture etc Heterogenous Cluster Uses nodes of different platforms Interoperability is an important issue Fundemental Cluster Design Issues # Cluster ob Management Achieve high system utilization Job management software is required to provide batching, load balancing, parallel processing, and other functionality Single System Image Cluster is a single system Appealing goal, very difficult to achieve SSI techniques are aimed at achieving this goal Avail;ability Support Redundancy in processors, memory, disks, IO devices, networks and operating system images Fault tolerance and recovery Eliminate all single point of failure Tolerate faulty conditions up to a certain extent through redundancy Critical jobs running on the failing nodes can be saved by failing over to the surviving node machines Rollback recovery schemes for periodic checkpointing Single System Image # Motivation - Allows a cluster toi be used, controlled and maintained as a familiar workstation Features: Single system single control symmetry location transparent single ob management system single user interface single process space HA (High Availability) # IoT for Ubiquitous Computing # Architecture of IoT # Tags: !DistributedComputingIndex","title":"Week 10"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#week-10","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 30/Oct/2021","title":"Week 10"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#scalable-lookup","text":"Each node \\(i\\) maintains a routing table called finger table \\(x^{th}\\) entry \\((1 \\le x \\le m)\\) is the node identifier of the node \\(succ(i + 2^{x - 1})\\) Size of the finger table is bounded by \\(m\\) entries Search is highly scalable For query on \\(key\\) at node \\(i\\) , if key lies between \\(i\\) and its successor, then \\(key\\) would reside at the successor and the successors address is returned Else the finger table is searched integer: successor <- initial value integer: predecessor <- initial value integer: finger [1...m]","title":"Scalable Lookup"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#clustering-for-massive-parallelism","text":"Computer cluster Consists of a collection of interconnected stand-alone/complete computers Cooperatively work together as a single, integrated computing resource Explores parallelism job level Benefits of clusters Scalable performance HA fault tolerance Modular growth Use of commodity components","title":"Clustering for Massive Parallelism"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#design-objectives-of-computer-clusters","text":"","title":"Design Objectives of Computer Clusters"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#packaging","text":"Cluster nodes can be packaged in a compact or a slack fashion Compact CLuster: Nodes are closely packaged in one or more racks sitting in a room Nodes are not attached to peripherals Slack Cluster Nodes are attached to their usual peripherals May be located in different rooms, Different buildings, or even remote regions","title":"Packaging"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#control","text":"Cluster can be managed in a centralized or decentralized fashion Compact cluster normally has centralized control Slack cluster can be controlled either way Centralized Cluster: nodes are owned, managed and administered by a central operator Decentralized Cluster Nodes have individual owners Lacks a single point of control","title":"Control"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#homogeneity","text":"Homogenous cluster Uses nodes from the same platform/architecture etc Heterogenous Cluster Uses nodes of different platforms Interoperability is an important issue","title":"Homogeneity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#fundemental-cluster-design-issues","text":"Cluster ob Management Achieve high system utilization Job management software is required to provide batching, load balancing, parallel processing, and other functionality Single System Image Cluster is a single system Appealing goal, very difficult to achieve SSI techniques are aimed at achieving this goal Avail;ability Support Redundancy in processors, memory, disks, IO devices, networks and operating system images Fault tolerance and recovery Eliminate all single point of failure Tolerate faulty conditions up to a certain extent through redundancy Critical jobs running on the failing nodes can be saved by failing over to the surviving node machines Rollback recovery schemes for periodic checkpointing","title":"Fundemental Cluster Design Issues"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#single-system-image","text":"Motivation - Allows a cluster toi be used, controlled and maintained as a familiar workstation Features: Single system single control symmetry location transparent single ob management system single user interface single process space","title":"Single System Image"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#ha-high-availability","text":"","title":"HA (High Availability)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#iot-for-ubiquitous-computing","text":"","title":"IoT for Ubiquitous Computing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week10DC.html#architecture-of-iot","text":"Tags: !DistributedComputingIndex","title":"Architecture of IoT"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html","text":"Week 1 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 24/Jul/2021 NOTE THAT THIS PAGE HAS DIAGRAMS THAT ARE BEST VISIBLE IN LIGHT MODE Topics Covered # What is a flipped mode course? What is a distributed system? Motivations for a distributed system Coupling Parallel Systems UMA (Uniform Memory Access) Model Omega Network (An example of UMA) What is a flipped mode course? # A course where there is content from a course ware and a live lecture the events Quizzes and Assignments will be done online in elearn portal 2 Quizzes (MCQ type) predetermined time slots 1 Assignment 1 Mid Sem (Half of the modules) (Theoretical) 1 Comprehensive (All modules) (Theoretical) What is a distributed system? # Collection of independent individual entities (Can function on it's own) to solve a given task collectively graph LR A[PM]---G B[PM]---G C[PM]---G G---D[PM] G---E[PM] G---F[PM] G((Communication Network WAN/LAN)) No common physical clock(system clock) No shared memory - employs message passing for communication Geographical separation - All the nodes taking part in the problem solving can be placed in different locations geographically Autonomy and heterogeneity Each node is a fully functioning independent system irrespective of being taking part in a distributed system. The processors of the nodes are loosely coupled Different processor speeds and operating systems are allowed And despite all these differences they cooperate with one another graph TB A[Application]---G B[Application]---G G---H H---E[Platform1] H---F[Platform2] G[Application Programming Interface API] H[Middleware Distributed Systems Services] Middleware drives the distributed system and the heterogeneity at a platform level is abstracted by APIs. Common Object Request Broker Architecture (CORBA) Remote Procedure Call (RPC) Distributed Component Object Model (DCOM) Remote Method Invocation (RMI) Motivations for a distributed system # Share resources Access to resources from different geographical locations (Like AWS Cloud servers at different locations or even a work from home situation can be covered) Increased performance/cost ratio, since there is a large resource pool and programs can be written in a way to efficiently use that pool to get tasks done more quickly Reliability, in the sense that since the system is distributed, even if one system breaks down, there is still a degree of availability of resources. Scaling, in the sense that it is easy to increase performance by merely adding more nodes to a distributed system. Modularity and Incremental Expandability. Coupling # High coupling: Homogeneous modules and hence have more restrictions imposed on these systems Low coupling: Heterogeneous modules and hence more flexibility is gained Parallel Systems # Multiprocessor systems: Direct access to shared memory area/address space Usually do not have a common system clock Eg, Omega, Butterfly Networks Multicomputer parallel systems There are multiple processors but no direct access to shared memory/address space There can be more than one nodes, but most likely are not geographically separated Eg, IBM Blue gene, CM* Connection Machine Array Processors Collocated Tightly Coupled Common system clock UMA (Uniform Memory Access) Model # Direct access to shared memory Access latency : Waiting time to complete an access to any memory location from any processor Access latency is same for all processors Processors remain in close proximity Connected by an interconnection network Processors are of the same type Omega Network (An example of UMA) # 2x2 switching elements data can be sent on any one for the input wires n-input and n-output network uses \\(log_2(n)\\) stages \\(log_2(n)\\) bits for addressing n processors, n memory banks \\(\\frac{n}{2}log_2(n)\\) switching elements of size 2/2 interconnection function defines how output \\(i\\) of one one stage is connected to input \\(j\\) of the next stage In this example the interconnection function is a left rotation operation on the binary representation of \\(i\\) to get \\(j\\) Tags: !DistributedComputingIndex","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#week-1","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 24/Jul/2021 NOTE THAT THIS PAGE HAS DIAGRAMS THAT ARE BEST VISIBLE IN LIGHT MODE","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#topics-covered","text":"What is a flipped mode course? What is a distributed system? Motivations for a distributed system Coupling Parallel Systems UMA (Uniform Memory Access) Model Omega Network (An example of UMA)","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#what-is-a-flipped-mode-course","text":"A course where there is content from a course ware and a live lecture the events Quizzes and Assignments will be done online in elearn portal 2 Quizzes (MCQ type) predetermined time slots 1 Assignment 1 Mid Sem (Half of the modules) (Theoretical) 1 Comprehensive (All modules) (Theoretical)","title":"What is a flipped mode course?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#what-is-a-distributed-system","text":"Collection of independent individual entities (Can function on it's own) to solve a given task collectively graph LR A[PM]---G B[PM]---G C[PM]---G G---D[PM] G---E[PM] G---F[PM] G((Communication Network WAN/LAN)) No common physical clock(system clock) No shared memory - employs message passing for communication Geographical separation - All the nodes taking part in the problem solving can be placed in different locations geographically Autonomy and heterogeneity Each node is a fully functioning independent system irrespective of being taking part in a distributed system. The processors of the nodes are loosely coupled Different processor speeds and operating systems are allowed And despite all these differences they cooperate with one another graph TB A[Application]---G B[Application]---G G---H H---E[Platform1] H---F[Platform2] G[Application Programming Interface API] H[Middleware Distributed Systems Services] Middleware drives the distributed system and the heterogeneity at a platform level is abstracted by APIs. Common Object Request Broker Architecture (CORBA) Remote Procedure Call (RPC) Distributed Component Object Model (DCOM) Remote Method Invocation (RMI)","title":"What is a distributed system?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#motivations-for-a-distributed-system","text":"Share resources Access to resources from different geographical locations (Like AWS Cloud servers at different locations or even a work from home situation can be covered) Increased performance/cost ratio, since there is a large resource pool and programs can be written in a way to efficiently use that pool to get tasks done more quickly Reliability, in the sense that since the system is distributed, even if one system breaks down, there is still a degree of availability of resources. Scaling, in the sense that it is easy to increase performance by merely adding more nodes to a distributed system. Modularity and Incremental Expandability.","title":"Motivations for a distributed system"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#coupling","text":"High coupling: Homogeneous modules and hence have more restrictions imposed on these systems Low coupling: Heterogeneous modules and hence more flexibility is gained","title":"Coupling"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#parallel-systems","text":"Multiprocessor systems: Direct access to shared memory area/address space Usually do not have a common system clock Eg, Omega, Butterfly Networks Multicomputer parallel systems There are multiple processors but no direct access to shared memory/address space There can be more than one nodes, but most likely are not geographically separated Eg, IBM Blue gene, CM* Connection Machine Array Processors Collocated Tightly Coupled Common system clock","title":"Parallel Systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#uma-uniform-memory-access-model","text":"Direct access to shared memory Access latency : Waiting time to complete an access to any memory location from any processor Access latency is same for all processors Processors remain in close proximity Connected by an interconnection network Processors are of the same type","title":"UMA (Uniform Memory Access) Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week1DC.html#omega-network-an-example-of-uma","text":"2x2 switching elements data can be sent on any one for the input wires n-input and n-output network uses \\(log_2(n)\\) stages \\(log_2(n)\\) bits for addressing n processors, n memory banks \\(\\frac{n}{2}log_2(n)\\) switching elements of size 2/2 interconnection function defines how output \\(i\\) of one one stage is connected to input \\(j\\) of the next stage In this example the interconnection function is a left rotation operation on the binary representation of \\(i\\) to get \\(j\\) Tags: !DistributedComputingIndex","title":"Omega Network (An example of UMA)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html","text":"Week 2 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 31/Jul/2021 NOTE THAT THIS PAGE HAS DIAGRAMS THAT ARE BEST VISIBLE IN LIGHT MODE Topics Covered # Module 1 Multicomputer parallel systems NUMA (Non Uniform Memory Access) Model Wraparound mesh interconnection in an MPS Hypercube interconnection in an MPS Distributed/Parallel Computing Jargon Parallelism/Speedup Parallel/Distributed Program Concurrency Distributed Communication Models RPC (Remote Procedure Call) Publish/Subscribe Module 2 Preliminary understandings Notations Space time diagram What are these letters? Causal precedence/dependency or happens before relation Concurrent events Module 1 # Multicomputer parallel systems # Processors do not have direct access to shared mem . Usually do not have a common clock . Processors are in close proximity . (THIS IS DIFFERENT FROM A DS) Very tightly coupled . (THIS IS DIFFERENT FROM A DS) Connected by an interconnection network. Communicate via Message passing APIs/common address space. (THIS IS DIFFERENT FROM A DS as DS can only use MP APIs). Typically this is used for high performance computing use cases. NUMA (Non Uniform Memory Access) Model # - Above you can see that the individual mem components are not clubbed together like the UMA Model (See here Week1DC#UMA Uniform Memory Access Model ). - The processor can still access the mem components but have a different access latency - non-uniform memory access - Different processor can have different access latency - Multicomputer system having a common address space . - Latency to access various shared memory location from the different processors varies. Wraparound mesh interconnection in an MPS # - The above diagram shows a 2d wraparound mesh - The above diagram corresponds to a \\(4x4\\) mesh containing 16 nodes. - Each node is connected to each and every node around it. The leaf nodes wrap around to the opposite leaf node of their respective row or column, hence the name Wrap Around Configuration . - If any of the nodes go down, the wrap around connection helps in maintaining the connection. - The connection can be bidirectional through a bus or any interconnection logic for that matter. Hypercube interconnection in an MPS # - A k dimensional hypercube has \\(2^k\\) processor and mem units (nodes) - Each node has at most \\(k\\) connections - Each dimension is associated with a bit position in the label - Labels of two adjacent nodes differ in the \\(k^{th}\\) bit position for the dimension k - From node \\(0000\\) we can see that each direction differs by a single bit. bottom is \\(0001\\) , right is \\(0010\\) and the adjacent one is \\(0100\\) - We can by applying the above rules get to know the label for the other adjacent nodes - The shortest path between any two processors is called Hamming distance - consider two bit labels \\(0110\\) and \\(1001\\) , the hamming distance would be 4 since all \\(4\\) bits differ. - consider two bit labels \\(0111\\) and \\(0110\\) , the hamming distance would be 1 since only \\(1\\) bit differs. - The above calculations can be used to connect the nodes according to the label - In the case of such an interconnection the \\(hamming\\ distance <= k\\) - Even if some of the nodes go down, there will be another path present to keep the connection alive. This provides fault tolerance and congestion control mechanism - Routing of messages happen hop by hop Distributed/Parallel Computing Jargon # Parallelism/Speedup # \\[Speedup = \\frac{T(1)}{T(n)}\\] \\(T(1)\\) = Time taken on a single processor \\(T(n)\\) = Time taken with n processors Measure of the relative speedup of a specific program on a given machine This depends on: Number of processors Mapping of the code to the processors Parallel/Distributed Program Concurrency # \\[Parallel\\ Program\\ Concurrency = \\frac{number\\ of\\ local\\ operations}{total\\ number\\ of\\ operations}\\] \\(number\\ of\\ local\\ operations\\) -> Non communication and non shared memory access operations \\(total\\ number\\ of\\ operations\\) -> All operations including communication or shared memory access Distributed Communication Models # RPC (Remote Procedure Call) # This topic is covered in a pre reading session here: PreRecordedModule12#RPC Remote Procedure Call - The client procedure calls a client stub to pass params - The client marshals the params (makes a common representation of the params), builds the message, and calls the local OS - The local OS sends the message to the remote OS - The server's remote OS gives the message to a server stub - The server stub de-marshals the params(Converts common form to server OS specific form) and calls the desired server routine - The server computes the result and hands it to the server stub - The server stub marshals the result into a message and calls the local OS - The server OS sends the message to the client OS - The client OS receives the message and sends it to the client stub - The client stub demarshals the result, and execution returns to the client Publish/Subscribe # - We have two entites mainly: - Publishers : Those who create information and publish them to a service (Through a Notify() call) - Subscriber : Those who consume the content published by publishers it had subscribed to. - The Subscriber gets data asynchronous though a notification, and they can continue their tasks until they receive notifications. - The subscriber has an option to unsubscribe as well Module 2 # Preliminary understanding # DS consists of a set of processors. There is an intercommunication network. Delay in communication is finite but is not predictable. Processors do not share a common global memory. Uses asynch message passing. No common physical global clock. Possibility of messages being delivered out of order. Messages may be lost, garbled or duplicated due to timeouts and retransmissions. There is always a possibility of communication and processor failure. Notations # \\(C_{ij}\\) : Channel from process \\(p_i\\) to process \\(p_j\\) \\(m_{ij}\\) : message sent by \\(p_i\\) to \\(p_j\\) Global state \\(GS\\) of a distributed computation consists of: States of the processes: Local memory state States of the communication channels: set of messages in transit Global state is explained in detail here PreRecordedModule12#Global state of a DS - Occurrences of events \\(e\\) - Causes changes in respective processes states - Causes changes in channels states - Causes transition in global system state Space time diagram # What are these letters? # \\(p_i\\) denotes the processes in the space axis \\(e_i^j\\) denotes the \\(j^{th}\\) event in process \\(p_i\\) Events that do not have arrows are The arrows denote the direction of the message sent. \\(e_1^2\\) is a message sending event \\(e_3^2\\) is a message receive event Causal precedence/dependency or happens before relation # - Relation ' \\(\\rightarrow\\) ' denotes flow of information in a distributed computation - \\(e_i \\rightarrow e_j\\) implies that all the information at \\(e_i\\) is accessible at \\(e_j\\) - In the above example we can see that \\(e_1^1\\) has a complete path to \\(e_3^3\\) hence we can be certain that \\(e_1^1 \\rightarrow e_3^3\\) Concurrent events # - For any two events \\(e_i\\) and \\(e_j\\) , if there are no paths that connect directly to each other then these two are concurrent and is denoted as \\(e_i\\ ||\\ e_j\\) - Concurrency is a non transitive relation meaning if \\(e_i\\ ||\\ e_j\\) and \\(e_j\\ ||\\ e_k\\) we cannot say that \\(e_i\\ ||\\ e_k\\) - In the above example \\(e_2^4\\) and \\(e_3^1\\) are not connected at all hence, we can say that \\(e_2^4\\ ||\\ e_3^1\\) Tags: !DistributedComputingIndex","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#week-2","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 31/Jul/2021 NOTE THAT THIS PAGE HAS DIAGRAMS THAT ARE BEST VISIBLE IN LIGHT MODE","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#topics-covered","text":"Module 1 Multicomputer parallel systems NUMA (Non Uniform Memory Access) Model Wraparound mesh interconnection in an MPS Hypercube interconnection in an MPS Distributed/Parallel Computing Jargon Parallelism/Speedup Parallel/Distributed Program Concurrency Distributed Communication Models RPC (Remote Procedure Call) Publish/Subscribe Module 2 Preliminary understandings Notations Space time diagram What are these letters? Causal precedence/dependency or happens before relation Concurrent events","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#module-1","text":"","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#multicomputer-parallel-systems","text":"Processors do not have direct access to shared mem . Usually do not have a common clock . Processors are in close proximity . (THIS IS DIFFERENT FROM A DS) Very tightly coupled . (THIS IS DIFFERENT FROM A DS) Connected by an interconnection network. Communicate via Message passing APIs/common address space. (THIS IS DIFFERENT FROM A DS as DS can only use MP APIs). Typically this is used for high performance computing use cases.","title":"Multicomputer parallel systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#numa-non-uniform-memory-access-model","text":"- Above you can see that the individual mem components are not clubbed together like the UMA Model (See here Week1DC#UMA Uniform Memory Access Model ). - The processor can still access the mem components but have a different access latency - non-uniform memory access - Different processor can have different access latency - Multicomputer system having a common address space . - Latency to access various shared memory location from the different processors varies.","title":"NUMA (Non Uniform Memory Access) Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#wraparound-mesh-interconnection-in-an-mps","text":"- The above diagram shows a 2d wraparound mesh - The above diagram corresponds to a \\(4x4\\) mesh containing 16 nodes. - Each node is connected to each and every node around it. The leaf nodes wrap around to the opposite leaf node of their respective row or column, hence the name Wrap Around Configuration . - If any of the nodes go down, the wrap around connection helps in maintaining the connection. - The connection can be bidirectional through a bus or any interconnection logic for that matter.","title":"Wraparound mesh interconnection in an MPS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#hypercube-interconnection-in-an-mps","text":"- A k dimensional hypercube has \\(2^k\\) processor and mem units (nodes) - Each node has at most \\(k\\) connections - Each dimension is associated with a bit position in the label - Labels of two adjacent nodes differ in the \\(k^{th}\\) bit position for the dimension k - From node \\(0000\\) we can see that each direction differs by a single bit. bottom is \\(0001\\) , right is \\(0010\\) and the adjacent one is \\(0100\\) - We can by applying the above rules get to know the label for the other adjacent nodes - The shortest path between any two processors is called Hamming distance - consider two bit labels \\(0110\\) and \\(1001\\) , the hamming distance would be 4 since all \\(4\\) bits differ. - consider two bit labels \\(0111\\) and \\(0110\\) , the hamming distance would be 1 since only \\(1\\) bit differs. - The above calculations can be used to connect the nodes according to the label - In the case of such an interconnection the \\(hamming\\ distance <= k\\) - Even if some of the nodes go down, there will be another path present to keep the connection alive. This provides fault tolerance and congestion control mechanism - Routing of messages happen hop by hop","title":"Hypercube interconnection in an MPS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#distributedparallel-computing-jargon","text":"","title":"Distributed/Parallel Computing Jargon"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#parallelismspeedup","text":"\\[Speedup = \\frac{T(1)}{T(n)}\\] \\(T(1)\\) = Time taken on a single processor \\(T(n)\\) = Time taken with n processors Measure of the relative speedup of a specific program on a given machine This depends on: Number of processors Mapping of the code to the processors","title":"Parallelism/Speedup"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#paralleldistributed-program-concurrency","text":"\\[Parallel\\ Program\\ Concurrency = \\frac{number\\ of\\ local\\ operations}{total\\ number\\ of\\ operations}\\] \\(number\\ of\\ local\\ operations\\) -> Non communication and non shared memory access operations \\(total\\ number\\ of\\ operations\\) -> All operations including communication or shared memory access","title":"Parallel/Distributed Program Concurrency"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#distributed-communication-models","text":"","title":"Distributed Communication Models"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#rpc-remote-procedure-call","text":"This topic is covered in a pre reading session here: PreRecordedModule12#RPC Remote Procedure Call - The client procedure calls a client stub to pass params - The client marshals the params (makes a common representation of the params), builds the message, and calls the local OS - The local OS sends the message to the remote OS - The server's remote OS gives the message to a server stub - The server stub de-marshals the params(Converts common form to server OS specific form) and calls the desired server routine - The server computes the result and hands it to the server stub - The server stub marshals the result into a message and calls the local OS - The server OS sends the message to the client OS - The client OS receives the message and sends it to the client stub - The client stub demarshals the result, and execution returns to the client","title":"RPC (Remote Procedure Call)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#publishsubscribe","text":"- We have two entites mainly: - Publishers : Those who create information and publish them to a service (Through a Notify() call) - Subscriber : Those who consume the content published by publishers it had subscribed to. - The Subscriber gets data asynchronous though a notification, and they can continue their tasks until they receive notifications. - The subscriber has an option to unsubscribe as well","title":"Publish/Subscribe"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#module-2","text":"","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#preliminary-understanding","text":"DS consists of a set of processors. There is an intercommunication network. Delay in communication is finite but is not predictable. Processors do not share a common global memory. Uses asynch message passing. No common physical global clock. Possibility of messages being delivered out of order. Messages may be lost, garbled or duplicated due to timeouts and retransmissions. There is always a possibility of communication and processor failure.","title":"Preliminary understanding"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#notations","text":"\\(C_{ij}\\) : Channel from process \\(p_i\\) to process \\(p_j\\) \\(m_{ij}\\) : message sent by \\(p_i\\) to \\(p_j\\) Global state \\(GS\\) of a distributed computation consists of: States of the processes: Local memory state States of the communication channels: set of messages in transit Global state is explained in detail here PreRecordedModule12#Global state of a DS - Occurrences of events \\(e\\) - Causes changes in respective processes states - Causes changes in channels states - Causes transition in global system state","title":"Notations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#space-time-diagram","text":"","title":"Space time diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#what-are-these-letters","text":"\\(p_i\\) denotes the processes in the space axis \\(e_i^j\\) denotes the \\(j^{th}\\) event in process \\(p_i\\) Events that do not have arrows are The arrows denote the direction of the message sent. \\(e_1^2\\) is a message sending event \\(e_3^2\\) is a message receive event","title":"What are these letters?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#causal-precedencedependency-or-happens-before-relation","text":"- Relation ' \\(\\rightarrow\\) ' denotes flow of information in a distributed computation - \\(e_i \\rightarrow e_j\\) implies that all the information at \\(e_i\\) is accessible at \\(e_j\\) - In the above example we can see that \\(e_1^1\\) has a complete path to \\(e_3^3\\) hence we can be certain that \\(e_1^1 \\rightarrow e_3^3\\)","title":"Causal precedence/dependency or happens before relation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week2DC.html#concurrent-events","text":"- For any two events \\(e_i\\) and \\(e_j\\) , if there are no paths that connect directly to each other then these two are concurrent and is denoted as \\(e_i\\ ||\\ e_j\\) - Concurrency is a non transitive relation meaning if \\(e_i\\ ||\\ e_j\\) and \\(e_j\\ ||\\ e_k\\) we cannot say that \\(e_i\\ ||\\ e_k\\) - In the above example \\(e_2^4\\) and \\(e_3^1\\) are not connected at all hence, we can say that \\(e_2^4\\ ||\\ e_3^1\\) Tags: !DistributedComputingIndex","title":"Concurrent events"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html","text":"Week 3 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 07/Aug/2021 Topics Covered # Models of Communication Networks FIFO Model Non FIFO Model Causal Ordering Model Global State of a DS Consistent Global State Cuts of a DC Consistent vs inconsistent cut Logical Time Scalar Time Notations Rules for updating scalar clock Height of an event Vector Time Notations Rules for updating vector clock Event Counting Models of Communication Networks # FIFO Model # Each channel acts as a FIFO queue Since the messages are sent in a queue, ordering is preserved by the channel If \\(P_1\\) sends \\(m_1\\) , \\(m_2\\) to \\(P_2\\) then \\(P_2\\) receives those two messages as a queue in the same order Non FIFO Model # The channel acts as a set of messages Since it is a set, the messages do not necessarily have the order maintained Sender process adds messages to channel Receiver process removes messages from it This helps in relieving the communication network from the overhead of maintaining the message ordering Causal Ordering Model # This is based on the happens before relation. Such a system must satisfy: If there are two messages \\(m_{ij}\\) and \\(m_{kj}\\) , then if \\(Send(m_{ij}) \\rightarrow Send(m_{kj})\\) , then it must also follow \\(Rec(m_{ij}) \\rightarrow Rec(m_{kj})\\) Note that the messages are coming to the same process. Causally ordered messages implies FIFO message delivery \\(CO \\subset FIFO \\subset Non FIFO\\) Global State of a DS # More details here Week2DC#Notations . - \\(LS_i^x\\) : State of process \\(p_i\\) after the occurrence of event \\(e_i^x\\) and before \\(e_i^{x+1}\\) - \\(SC_{ij}\\) : State of channel \\(C_{ij}\\) Consistent Global State # A state is meaningful, meaning that every message that is recorded as received is also recorded as sent For all \\(m_{ij}\\) , \\(send(m_{ij}) \\notin LS_i^x\\) \\(\\implies\\) \\(m_{ij} /notin SC_{ij} rec(m_{ij}) \\notin LS_j^y\\) The above space time diagram is consistent, because when we see all the \\(LS\\) that define the \\(GS\\) all the sends are recorded before the receives of that message Consider a \\(GS\\) of \\(LS_1^3, LS_2^3, LS_3^4, LS_4^2\\) , here the \\(GS\\) is inconsistent, because the receive of message \\(m_{21}\\) is recorded but not the send. Cuts of a DC # - Any zig-zag line drawn that cuts all the processes in a space time diagram - The events in a DS is partitioned into: - Past: Contains all events left of the cut - Future: Contains all events right of the cut - A cut is a representation of the \\(GS\\) at those points in the cut and processes Consistent vs inconsistent cut # - Consistent cut : - Every message received in the PAST of the cut was sent in the PAST of that cut - All messages that cross the cut from the PAST to the FUTURE are in transit - The above image \\(C2\\) is an example since all the sends and receives happen in the PAST and the message between \\(e_2^4\\) to \\(e_1^3\\) is in transit. - Inconsistent cut : - If the receive is recorded in the PAST and the send is in the FUTURE then it is an inconsistent cut - In the above image \\(C1\\) is inconsistent since \\(send(e_1^2)\\) is in the FUTURE and \\(rec(e_2^2)\\) is in the past. Logical Time # To show a sense of time in the sense of Logical time follows Monotonicity property: if event \\(\\alpha\\) has happened before event \\(\\beta\\) then the timestamp of event \\(\\alpha\\) is less that the timestamp of event \\(\\beta\\) Every process has a LC LC is advanced using a set of rules each event is assigned as timestamp There are two ways to represent logical time: Scalar Time Vector Time Scalar Time # Notations # This is covered in the pre reading content here: PreRecordedModule12#Scalar Time Time domain is the set of non negative integers \\(C_i\\) : Integer variable, denotes the logical clock of \\(p_i\\) Rules for updating scalar clock # R1 : Before executing an event, process \\(p_i\\) executes \\(C_i = C_i + d\\ (d \\gt 0)\\) d can be any value but usually is 1 R2 : When process \\(p_i\\) receives a message with a timestamp \\(C_{msg}\\) , it executes the following actions: \\(C_i = mac(C_i, C_{msg})\\) execute R1 deliver the message Whenever there is an internal event the R1 is executed The above rules can be seen in action in the steps below: Height of an event # Height is defined as the number of events that causally precedes it If \\(d = 1\\) , the height of a given event is 1 minus the timestamp at that event. Vector Time # Notations # Time domain is represented by a set of n-dimensional non-negative integer vectors. (Can be a row or a column vector) \\(vt_i[i]\\) , \\(vtj[j]\\) Rules for updating vector clock # R1 : First take the element wise max of the vector received and the local vector R2 : Add 1 to the local clock index of the vector decided above The above rules can be seen in action in the steps below: Event Counting # - if \\(d\\) is always \\(1\\) and \\(vt_i[i]\\) is the \\(i^{th}\\) component of vector clock at process \\(p_i\\) - Then \\(vt_i[i]\\) = no. of events that have occurred at \\(p_i\\) until that instant - \\(vh[j]\\) = number of events executed by process \\(p_j\\) that causally precede \\(e\\) - \\(\\sum vh[j] - 1\\) : Total no. of events that causally precede \\(e\\) in the DC Tags: !DistributedComputingIndex","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#week-3","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 07/Aug/2021","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#topics-covered","text":"Models of Communication Networks FIFO Model Non FIFO Model Causal Ordering Model Global State of a DS Consistent Global State Cuts of a DC Consistent vs inconsistent cut Logical Time Scalar Time Notations Rules for updating scalar clock Height of an event Vector Time Notations Rules for updating vector clock Event Counting","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#models-of-communication-networks","text":"","title":"Models of Communication Networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#fifo-model","text":"Each channel acts as a FIFO queue Since the messages are sent in a queue, ordering is preserved by the channel If \\(P_1\\) sends \\(m_1\\) , \\(m_2\\) to \\(P_2\\) then \\(P_2\\) receives those two messages as a queue in the same order","title":"FIFO Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#non-fifo-model","text":"The channel acts as a set of messages Since it is a set, the messages do not necessarily have the order maintained Sender process adds messages to channel Receiver process removes messages from it This helps in relieving the communication network from the overhead of maintaining the message ordering","title":"Non FIFO Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#causal-ordering-model","text":"This is based on the happens before relation. Such a system must satisfy: If there are two messages \\(m_{ij}\\) and \\(m_{kj}\\) , then if \\(Send(m_{ij}) \\rightarrow Send(m_{kj})\\) , then it must also follow \\(Rec(m_{ij}) \\rightarrow Rec(m_{kj})\\) Note that the messages are coming to the same process. Causally ordered messages implies FIFO message delivery \\(CO \\subset FIFO \\subset Non FIFO\\)","title":"Causal Ordering Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#global-state-of-a-ds","text":"More details here Week2DC#Notations . - \\(LS_i^x\\) : State of process \\(p_i\\) after the occurrence of event \\(e_i^x\\) and before \\(e_i^{x+1}\\) - \\(SC_{ij}\\) : State of channel \\(C_{ij}\\)","title":"Global State of a DS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#consistent-global-state","text":"A state is meaningful, meaning that every message that is recorded as received is also recorded as sent For all \\(m_{ij}\\) , \\(send(m_{ij}) \\notin LS_i^x\\) \\(\\implies\\) \\(m_{ij} /notin SC_{ij} rec(m_{ij}) \\notin LS_j^y\\) The above space time diagram is consistent, because when we see all the \\(LS\\) that define the \\(GS\\) all the sends are recorded before the receives of that message Consider a \\(GS\\) of \\(LS_1^3, LS_2^3, LS_3^4, LS_4^2\\) , here the \\(GS\\) is inconsistent, because the receive of message \\(m_{21}\\) is recorded but not the send.","title":"Consistent Global State"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#cuts-of-a-dc","text":"- Any zig-zag line drawn that cuts all the processes in a space time diagram - The events in a DS is partitioned into: - Past: Contains all events left of the cut - Future: Contains all events right of the cut - A cut is a representation of the \\(GS\\) at those points in the cut and processes","title":"Cuts of a DC"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#consistent-vs-inconsistent-cut","text":"- Consistent cut : - Every message received in the PAST of the cut was sent in the PAST of that cut - All messages that cross the cut from the PAST to the FUTURE are in transit - The above image \\(C2\\) is an example since all the sends and receives happen in the PAST and the message between \\(e_2^4\\) to \\(e_1^3\\) is in transit. - Inconsistent cut : - If the receive is recorded in the PAST and the send is in the FUTURE then it is an inconsistent cut - In the above image \\(C1\\) is inconsistent since \\(send(e_1^2)\\) is in the FUTURE and \\(rec(e_2^2)\\) is in the past.","title":"Consistent vs inconsistent cut"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#logical-time","text":"To show a sense of time in the sense of Logical time follows Monotonicity property: if event \\(\\alpha\\) has happened before event \\(\\beta\\) then the timestamp of event \\(\\alpha\\) is less that the timestamp of event \\(\\beta\\) Every process has a LC LC is advanced using a set of rules each event is assigned as timestamp There are two ways to represent logical time: Scalar Time Vector Time","title":"Logical Time"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#scalar-time","text":"","title":"Scalar Time"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#notations","text":"This is covered in the pre reading content here: PreRecordedModule12#Scalar Time Time domain is the set of non negative integers \\(C_i\\) : Integer variable, denotes the logical clock of \\(p_i\\)","title":"Notations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#rules-for-updating-scalar-clock","text":"R1 : Before executing an event, process \\(p_i\\) executes \\(C_i = C_i + d\\ (d \\gt 0)\\) d can be any value but usually is 1 R2 : When process \\(p_i\\) receives a message with a timestamp \\(C_{msg}\\) , it executes the following actions: \\(C_i = mac(C_i, C_{msg})\\) execute R1 deliver the message Whenever there is an internal event the R1 is executed The above rules can be seen in action in the steps below:","title":"Rules for updating scalar clock"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#height-of-an-event","text":"Height is defined as the number of events that causally precedes it If \\(d = 1\\) , the height of a given event is 1 minus the timestamp at that event.","title":"Height of an event"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#vector-time","text":"","title":"Vector Time"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#notations_1","text":"Time domain is represented by a set of n-dimensional non-negative integer vectors. (Can be a row or a column vector) \\(vt_i[i]\\) , \\(vtj[j]\\)","title":"Notations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#rules-for-updating-vector-clock","text":"R1 : First take the element wise max of the vector received and the local vector R2 : Add 1 to the local clock index of the vector decided above The above rules can be seen in action in the steps below:","title":"Rules for updating vector clock"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week3DC.html#event-counting","text":"- if \\(d\\) is always \\(1\\) and \\(vt_i[i]\\) is the \\(i^{th}\\) component of vector clock at process \\(p_i\\) - Then \\(vt_i[i]\\) = no. of events that have occurred at \\(p_i\\) until that instant - \\(vh[j]\\) = number of events executed by process \\(p_j\\) that causally precede \\(e\\) - \\(\\sum vh[j] - 1\\) : Total no. of events that causally precede \\(e\\) in the DC Tags: !DistributedComputingIndex","title":"Event Counting"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html","text":"Week 4 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 21/Aug/2021 Topics Covered # Singhal-Kshemakalyani's Differential Technique Fowler-Zwaenepoel's Direct-Dependency Technique Problems in recording global state Some Key Concepts of Consistency and Channel State Snapshot Recording Algorithms Chandy Lamport Algorithm (For FIFO) Lai and Yang algorithm (For NON-FIFO) Singhal-Kshemakalyani's Differential Technique # 1. The message contains a list of tuples that follows the given syntax: \\(\\{(index, val)\\}\\) , where \\(index =\\) Process that has the clock value of \\(= val\\) . 2. We see that a process sends only the details of the clocks that are changed on that given process only. 3. This method considerably saves the amount of data that is sent between processed in the initial timing Fowler-Zwaenepoel's Direct-Dependency Technique # 1. Here we send only the local clock value of the process to the receiver 2. Dependency to other process that is directly not connected by messages is lost. 3. Direct dependence relationship is preserved Problems in recording global state # Lack of a globally shared memory Lack of a global clock Message transmission is asynchronous message transfer delays are finite but unpredictable Some Key Concepts of Consistency and Channel State # Snapshot Recording Algorithms # Chandy Lamport Algorithm (For FIFO) # This algo uses marker messages to show other processes that it has recorded its state and notify others that they must record theirs too Marker Sending Rule Process \\(p_i\\) records its state For each outgoing channel \\(C\\) on which a marker has not been sent, \\(p_j\\) sends a marker along \\(C\\) before \\(p_i\\) sends further messages along \\(C\\) Marker Receiving Rule On receiving a marker along channel \\(C\\) : if \\(p_j\\) has not recorded its state then Record the state of \\(C\\) as empty set Execute the \"marker sending rule\" else Record the state of \\(C\\) as the set of messages received along \\(C\\) after \\(p_j\\) state was recorded and before \\(p_j\\) received the marker along \\(C\\) The vertical dashed lines are the amount of moneyt present between users A and B. The arrow shows a transaction between A and B Let \\(S1\\) sends a marker to \\(S2\\) through \\(C_{12}\\) , so \\(S1\\) => saves 450 as the local state \\(S2\\) => Since it has not saved its local state, it will record the value of \\(C_{12}\\) as empty and save the local state as 1030 After it has recorded \\(S2\\) , Since S1 has already set its local state, it will store the After \\(S1\\) has recorded its local state Lai and Yang algorithm (For NON-FIFO) # Since markers cannot be used in NON FIFO channels we use piggybacking, so that messages sent after and before the marker are distinguished. In Lai and Yang we use coloring scheme to do the piggy backing every process is initially white process turns red while taking a snapshot equivalent of the \"marker sending rule\" is executed when a process turns red Every message sent by a white process is colored white A white message is a message that was send before the sender of that message recorded its local snapshot Every message sent by a red process is colored white A white message is a message that was send before the sender of that message recorded its local snapshot \\(LS = 750\\) All subsequent messages are red messages \\(P2\\) turns red when 10 is received, \\(P2\\) will record the \\(LS\\) as 190 \\[C_{12} = white_message(20) + white_message(30) - white_message(20) = 30\\] \\[C_{21} = white_message(30) - 0 = 30\\] Tags: !DistributedComputingIndex","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#week-4","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 21/Aug/2021","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#topics-covered","text":"Singhal-Kshemakalyani's Differential Technique Fowler-Zwaenepoel's Direct-Dependency Technique Problems in recording global state Some Key Concepts of Consistency and Channel State Snapshot Recording Algorithms Chandy Lamport Algorithm (For FIFO) Lai and Yang algorithm (For NON-FIFO)","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#singhal-kshemakalyanis-differential-technique","text":"1. The message contains a list of tuples that follows the given syntax: \\(\\{(index, val)\\}\\) , where \\(index =\\) Process that has the clock value of \\(= val\\) . 2. We see that a process sends only the details of the clocks that are changed on that given process only. 3. This method considerably saves the amount of data that is sent between processed in the initial timing","title":"Singhal-Kshemakalyani's Differential Technique"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#fowler-zwaenepoels-direct-dependency-technique","text":"1. Here we send only the local clock value of the process to the receiver 2. Dependency to other process that is directly not connected by messages is lost. 3. Direct dependence relationship is preserved","title":"Fowler-Zwaenepoel's Direct-Dependency Technique"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#problems-in-recording-global-state","text":"Lack of a globally shared memory Lack of a global clock Message transmission is asynchronous message transfer delays are finite but unpredictable","title":"Problems in recording global state"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#some-key-concepts-of-consistency-and-channel-state","text":"","title":"Some Key Concepts of Consistency and Channel State"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#snapshot-recording-algorithms","text":"","title":"Snapshot Recording Algorithms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#chandy-lamport-algorithm-for-fifo","text":"This algo uses marker messages to show other processes that it has recorded its state and notify others that they must record theirs too Marker Sending Rule Process \\(p_i\\) records its state For each outgoing channel \\(C\\) on which a marker has not been sent, \\(p_j\\) sends a marker along \\(C\\) before \\(p_i\\) sends further messages along \\(C\\) Marker Receiving Rule On receiving a marker along channel \\(C\\) : if \\(p_j\\) has not recorded its state then Record the state of \\(C\\) as empty set Execute the \"marker sending rule\" else Record the state of \\(C\\) as the set of messages received along \\(C\\) after \\(p_j\\) state was recorded and before \\(p_j\\) received the marker along \\(C\\) The vertical dashed lines are the amount of moneyt present between users A and B. The arrow shows a transaction between A and B Let \\(S1\\) sends a marker to \\(S2\\) through \\(C_{12}\\) , so \\(S1\\) => saves 450 as the local state \\(S2\\) => Since it has not saved its local state, it will record the value of \\(C_{12}\\) as empty and save the local state as 1030 After it has recorded \\(S2\\) , Since S1 has already set its local state, it will store the After \\(S1\\) has recorded its local state","title":"Chandy Lamport Algorithm (For FIFO)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC.html#lai-and-yang-algorithm-for-non-fifo","text":"Since markers cannot be used in NON FIFO channels we use piggybacking, so that messages sent after and before the marker are distinguished. In Lai and Yang we use coloring scheme to do the piggy backing every process is initially white process turns red while taking a snapshot equivalent of the \"marker sending rule\" is executed when a process turns red Every message sent by a white process is colored white A white message is a message that was send before the sender of that message recorded its local snapshot Every message sent by a red process is colored white A white message is a message that was send before the sender of that message recorded its local snapshot \\(LS = 750\\) All subsequent messages are red messages \\(P2\\) turns red when 10 is received, \\(P2\\) will record the \\(LS\\) as 190 \\[C_{12} = white_message(20) + white_message(30) - white_message(20) = 30\\] \\[C_{21} = white_message(30) - 0 = 30\\] Tags: !DistributedComputingIndex","title":"Lai and Yang algorithm (For NON-FIFO)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html","text":"Week 4 (cont.) # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 22/Aug/2021 Topics Covered # Terminology and Basic Algorithms Notations and Definitions Synchronous Single-Initiator Spanning Tree Algorithm using Flooding Algorithm Design Struct for each process \\(P_i\\) Algorithm pseudo code Algorithm in action Broadcast and Convergecast Algorithm on a Tree Broadcast Algorithm Convergecast Algorithm Complexity Message Ordering Terminology and Basic Algorithms # Notations and Definitions # Undirected unweighted graph \\(G = (M, L)\\) , represents topology of an example graph such as the one shown below: Vertices are nodes Edges are channels \\(n = |N|\\) , Cardinality of set of all nodes \\(N = [A, B, C, D, E, F]\\) \\(l = |L|\\) , Cardinality of set of all edges \\(L = [AB, BC, CF, DE, EF, AD, AE, CE]\\) diameter of a graph : minimum number of edges that need to be traversed to go from any node to any other node \\(Diameter = max_{i, j \\in N}\\) , Length of the shortest path between i and j where j belongs to the set N. We basically find all the minimum distances from i to all j and the diameter is the maximum value In the above example let \\(i = A\\) and \\(j = [A, B, C, D, E, F]\\) then we can say that the max value of the lengths is 2 (In case of the path length for A to F), so the \\(diameter = 2\\) A spanning tree is a tree where the graph does not have any edges that will cause cycles in it. An example for a spanning tree for the above example is shown below: As a rule of thumb, a spanning tree for a graph with \\(n\\) nodes, will have \\(n - 1\\) edges So in the example graph has 6 nodes and the number of edges in the spanning tree is 5. Synchronous Single-Initiator Spanning Tree Algorithm using Flooding # Algorithm executes steps synchronously (When a message is sent, the sender waits till all the other nodes receives the messages) Root of the graph initiates the algorithm (An arbitrary node can be selected as the root) QUERY messages are flooded . This means that first the root node will send a message to all its nearby nodes, and inturn those nodes will send it to its neighbors and so on. The final spanning tree will have the root node as the initial arbitrary node selected in the graph. Each process \\(P_i (P_i \\ne root)\\) should output its own parent for the spanning tree. In distributed computing we interchangeably use nodes and processes Algorithm Design # Struct for each process \\(P_i\\) # Variables maintained at each \\(P_i\\) Initial variable values at each \\(P_i\\) int visited 0 int depth 0 int parent NULL set of int Neighbors set of neighbors Algorithm pseudo code # Algorithm for Pi When Round r = 1 if Pi = root then visited = 1 depth = 0 send QUERY to Neighbors if Pi receives a QUERY message then visited = 1 depth = r parent = root plan to send QUERY to Neighbors at the next round When Round r > 1 and r < = diameter if Pi planned to send in previous round then Pi sends QUERY to Neighbors if Pi receives QUERY messages then visited = 1 depth = r parent = any randomly selected nodes from which Query was receives plan to send QUERY to Neighbors but not to any nodes from which send was received Algorithm in action # Round 1 : Root \\(A\\) sends \\(QUERY\\) to neighbors \\([B, F]\\) \\(B\\) and \\(F\\) set \\(A\\) as parent and plans to send \\(QUERY\\) to its neighbors Round 2 : \\(B\\) sends \\(QUERY\\) to neighbors \\([C, E]\\) and \\(F\\) sends \\(QUERY\\) to neighbors \\([E]\\) \\(E\\) randomly chooses \\(F\\) as parent and \\(C\\) chooses \\(B\\) as parent and both \\(E\\) and \\(F\\) plans to send \\(QUERY\\) to its neighbors Round 3 : \\(E\\) sends \\(QUERY\\) to neighbors \\([C, D]\\) and \\(C\\) sends \\(QUERY\\) to neighbors \\([E, D]\\) Since \\(C\\) and \\(E\\) are already visited, the \\(QUERY\\) is ignored in those cases and \\(D\\) randomly chooses \\(C\\) as parent over \\(E\\) The spanning tree generated is as follows : The above spanning tree has 6 nodes and 5 edges Broadcast and Convergecast Algorithm on a Tree # A spanning tree is useful for distributing (via Broadcast ) and collecting information (via Convergecast ) to and from all the nodes Broadcast Algorithm # - BC1: - The root sends the information to be sent to all its children - BC2: - When a non root node receives information from its parent, it copies it and forwards it to its children Convergecast Algorithm # - CVC1: - Leaf node sends what it needs to report to its parent - CVC2: - At a non leaf node that is not root, a report is received from all the child nodes, the collective report is sent to its parent - CVC3: - When a root node receives information from its child nodes, the global function is evaluated using the reports Complexity # Each broadcast and each convergecast requires \\(n - 1\\) messages. Each broadcast and each convergecast requires time equal to the maximum height \\(h\\) of the tree which is \\(\\mathcal{O}(n)\\) Message Ordering # Group Communication # Broadcast - Sending a message to all members in the distributed system Multicasting - A message is sent to a certain subset, identified as a group, of the processes in the system. Unicasting - Point-to-point message communication Causal Order # Causal Order is explained here: Week3DC#Causal Ordering Model 2 criteria must be satisfied by causal ordering protocol Safety: A message M arriving at a process may need to be buffered until all system wide messages sent in the causal past of the send(M) event to the same destination have already arrived Distinction is made between: Arrival of messages at a process Event at which the message is given to the application process Liveness: A message that arrives at a process must be eventually be delivered to the process Raynal-Schiper-Toueg Algorithm # Each message M should carry a log of All other messages Send causally before M's send event, and sent to the same destination dest(M) Log can be examined to ensure when it is safe to deliver a message Channels are assumed to be FIFO Local Variables # array of int \\(SENT[1 .... n, 1 .... n]\\) (n x n array) Where \\(SENT_i[j, k]\\) = no. of messages sent by \\(P_j\\) to \\(P_k\\) as known to \\(P_i\\) array of int \\(DELIV[1 .... n]\\) Where \\(DELIV_i[j]\\) = no. of messages from \\(P_j\\) that have been delivered to \\(P_i\\) The Algorithm # Message Send Event , where \\(P_i\\) wants to send message \\(M\\) to \\(P_j\\) : \\(send(M, SENT)\\) to \\(P_j\\) (Here \\(SENT\\) array is the log which will be used to determine if the message needs to be buffered or not) \\(SENT[i, j] = SENT[i, j] + 1\\) Message Arrival Event , when \\((M, SENT_j)\\) arrives at \\(P_i\\) from \\(P_j\\) : deliver \\(M\\) to \\(P_i\\) when for each process \\(x\\) , \\(DELIV_i[x] \\ge SENT_j[x, i]\\) \\(\\forall x, y, SENT_i[x, y] = max(SENT_i[x, y], SENT_j[x, y])\\) \\(DELIV_i[j] = DELIV_i[j] + 1\\) Algorithm Complexity # Space complexity at each process: \\(\\mathcal{O}(n^2)\\) integers Space overhead per message: \\(n^2\\) integers Time complexity at each process for each send and deliver event: \\(\\mathcal{O}(n^2)\\) Algorithm in action # Assume following steps have occurred till now - \\(P_1\\) sent 3 messages to \\(P_2\\) - \\(P_1\\) sent 4 messages to \\(P_3\\) - \\(P_2\\) sent 5 messages to \\(P_1\\) - \\(P_2\\) sent 2 messages to \\(P_3\\) - \\(P_3\\) sent 4 messages to \\(P_2\\) Variable state Assuming \\(SENT\\) of all \\(P\\) is aware of all the other values of \\(SENT\\) , \\(SENT\\) of \\(P_1\\) : | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Assume the following values for \\(DELIV\\) arrays \\(DELIV_1 = [0, 4, 0]\\) \\(DELIV_2 = [3, 0, 4]\\) \\(DELIV_3 = [3, 2, 0]\\) MESSAGE EVENTS: Now if, \\(P_1\\) sends \\(m_1\\) to \\(P_2\\) 1. \\(SEND\\) event from \\(P_1\\) 1. \\(SENT_1\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_1, SENT_1)\\) 3. Updated \\(SENT_1\\) | 0 | 3+1 | 4 | | 0 | 4 | 4 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4 | 0 | | 0 | 4 | 0 | 2. \\(RECEIVE\\) \\(m_1\\) from \\(P_1\\) at \\(P_2\\) 1. \\(DELIV_2[1] \\ge SENT_1[1, 2]\\) 2. \\(DELIV_2[3] \\ge SENT_1[3, 2]\\) 3. Deliver \\(m_1\\) to \\(P_2\\) 4. \\(DELIV_2[1] = DELIV_2[1] + 1 = 4\\) 5. \\(DELIV_2 = [4, 0, 4]\\) 6. \\(SENT_2\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Now if, \\(P_3\\) sends \\(m_2\\) to \\(P_2\\) 1. \\(SEND\\) event from \\(P_3\\) 1. \\(SENT_3\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_2, SENT_3)\\) 3. Updated \\(SENT_3\\) | 0 | 3 | 4 | | 0 | 3 | 4 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4+1 | 0 | | 0 | 5 | 0 | 2. \\(RECEIVE\\) \\(m_2\\) from \\(P_3\\) at \\(P_2\\) 1. \\(DELIV_2[1] \\ge SENT_3[1, 2]\\) 2. \\(DELIV_2[3] \\ge SENT_3[3, 2]\\) 3. Deliver \\(m_2\\) to \\(P_2\\) 4. \\(DELIV_2[3] = DELIV_2[3] + 1 = 5\\) 5. \\(DELIV_2 = [4, 0, 5]\\) 6. \\(SENT_2\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Now if, \\(P_1\\) sends \\(m_3\\) to \\(P_3\\) 1. \\(SEND\\) event from \\(P_1\\) 1. \\(SENT_1\\) | 0 | 4 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_2, SENT_3)\\) 3. Updated \\(SENT_3\\) | 0 | 4 | 4+1 | | 0 | 4 | 5 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4 | 0 | | 0 | 5 | 0 | 2. \\(RECEIVE\\) \\(m_3\\) from \\(P_1\\) at \\(P_3\\) 1. \\(DELIV_3[1] \\ge SENT_1[1, 3]\\) (FALSE) 2. \\(DELIV_3[2] \\ge SENT_1[2, 3]\\) (TRUE) 3. Put \\(m_3\\) in buffer since the first condition failed, only process it after the other \\(P_1\\) messages are delivered to \\(P_3\\) Tags: !DistributedComputingIndex","title":"Week 4 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#week-4-cont","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 22/Aug/2021","title":"Week 4 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#topics-covered","text":"Terminology and Basic Algorithms Notations and Definitions Synchronous Single-Initiator Spanning Tree Algorithm using Flooding Algorithm Design Struct for each process \\(P_i\\) Algorithm pseudo code Algorithm in action Broadcast and Convergecast Algorithm on a Tree Broadcast Algorithm Convergecast Algorithm Complexity Message Ordering","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#terminology-and-basic-algorithms","text":"","title":"Terminology and Basic Algorithms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#notations-and-definitions","text":"Undirected unweighted graph \\(G = (M, L)\\) , represents topology of an example graph such as the one shown below: Vertices are nodes Edges are channels \\(n = |N|\\) , Cardinality of set of all nodes \\(N = [A, B, C, D, E, F]\\) \\(l = |L|\\) , Cardinality of set of all edges \\(L = [AB, BC, CF, DE, EF, AD, AE, CE]\\) diameter of a graph : minimum number of edges that need to be traversed to go from any node to any other node \\(Diameter = max_{i, j \\in N}\\) , Length of the shortest path between i and j where j belongs to the set N. We basically find all the minimum distances from i to all j and the diameter is the maximum value In the above example let \\(i = A\\) and \\(j = [A, B, C, D, E, F]\\) then we can say that the max value of the lengths is 2 (In case of the path length for A to F), so the \\(diameter = 2\\) A spanning tree is a tree where the graph does not have any edges that will cause cycles in it. An example for a spanning tree for the above example is shown below: As a rule of thumb, a spanning tree for a graph with \\(n\\) nodes, will have \\(n - 1\\) edges So in the example graph has 6 nodes and the number of edges in the spanning tree is 5.","title":"Notations and Definitions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#synchronous-single-initiator-spanning-tree-algorithm-using-flooding","text":"Algorithm executes steps synchronously (When a message is sent, the sender waits till all the other nodes receives the messages) Root of the graph initiates the algorithm (An arbitrary node can be selected as the root) QUERY messages are flooded . This means that first the root node will send a message to all its nearby nodes, and inturn those nodes will send it to its neighbors and so on. The final spanning tree will have the root node as the initial arbitrary node selected in the graph. Each process \\(P_i (P_i \\ne root)\\) should output its own parent for the spanning tree. In distributed computing we interchangeably use nodes and processes","title":"Synchronous Single-Initiator Spanning Tree Algorithm using Flooding"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#algorithm-design","text":"","title":"Algorithm Design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#struct-for-each-process-p_i","text":"Variables maintained at each \\(P_i\\) Initial variable values at each \\(P_i\\) int visited 0 int depth 0 int parent NULL set of int Neighbors set of neighbors","title":"Struct for each process \\(P_i\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#algorithm-pseudo-code","text":"Algorithm for Pi When Round r = 1 if Pi = root then visited = 1 depth = 0 send QUERY to Neighbors if Pi receives a QUERY message then visited = 1 depth = r parent = root plan to send QUERY to Neighbors at the next round When Round r > 1 and r < = diameter if Pi planned to send in previous round then Pi sends QUERY to Neighbors if Pi receives QUERY messages then visited = 1 depth = r parent = any randomly selected nodes from which Query was receives plan to send QUERY to Neighbors but not to any nodes from which send was received","title":"Algorithm pseudo code"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#algorithm-in-action","text":"Round 1 : Root \\(A\\) sends \\(QUERY\\) to neighbors \\([B, F]\\) \\(B\\) and \\(F\\) set \\(A\\) as parent and plans to send \\(QUERY\\) to its neighbors Round 2 : \\(B\\) sends \\(QUERY\\) to neighbors \\([C, E]\\) and \\(F\\) sends \\(QUERY\\) to neighbors \\([E]\\) \\(E\\) randomly chooses \\(F\\) as parent and \\(C\\) chooses \\(B\\) as parent and both \\(E\\) and \\(F\\) plans to send \\(QUERY\\) to its neighbors Round 3 : \\(E\\) sends \\(QUERY\\) to neighbors \\([C, D]\\) and \\(C\\) sends \\(QUERY\\) to neighbors \\([E, D]\\) Since \\(C\\) and \\(E\\) are already visited, the \\(QUERY\\) is ignored in those cases and \\(D\\) randomly chooses \\(C\\) as parent over \\(E\\) The spanning tree generated is as follows : The above spanning tree has 6 nodes and 5 edges","title":"Algorithm in action"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#broadcast-and-convergecast-algorithm-on-a-tree","text":"A spanning tree is useful for distributing (via Broadcast ) and collecting information (via Convergecast ) to and from all the nodes","title":"Broadcast and Convergecast Algorithm on a Tree"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#broadcast-algorithm","text":"- BC1: - The root sends the information to be sent to all its children - BC2: - When a non root node receives information from its parent, it copies it and forwards it to its children","title":"Broadcast Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#convergecast-algorithm","text":"- CVC1: - Leaf node sends what it needs to report to its parent - CVC2: - At a non leaf node that is not root, a report is received from all the child nodes, the collective report is sent to its parent - CVC3: - When a root node receives information from its child nodes, the global function is evaluated using the reports","title":"Convergecast Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#complexity","text":"Each broadcast and each convergecast requires \\(n - 1\\) messages. Each broadcast and each convergecast requires time equal to the maximum height \\(h\\) of the tree which is \\(\\mathcal{O}(n)\\)","title":"Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#message-ordering","text":"","title":"Message Ordering"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#group-communication","text":"Broadcast - Sending a message to all members in the distributed system Multicasting - A message is sent to a certain subset, identified as a group, of the processes in the system. Unicasting - Point-to-point message communication","title":"Group Communication"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#causal-order","text":"Causal Order is explained here: Week3DC#Causal Ordering Model 2 criteria must be satisfied by causal ordering protocol Safety: A message M arriving at a process may need to be buffered until all system wide messages sent in the causal past of the send(M) event to the same destination have already arrived Distinction is made between: Arrival of messages at a process Event at which the message is given to the application process Liveness: A message that arrives at a process must be eventually be delivered to the process","title":"Causal Order"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#raynal-schiper-toueg-algorithm","text":"Each message M should carry a log of All other messages Send causally before M's send event, and sent to the same destination dest(M) Log can be examined to ensure when it is safe to deliver a message Channels are assumed to be FIFO","title":"Raynal-Schiper-Toueg Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#local-variables","text":"array of int \\(SENT[1 .... n, 1 .... n]\\) (n x n array) Where \\(SENT_i[j, k]\\) = no. of messages sent by \\(P_j\\) to \\(P_k\\) as known to \\(P_i\\) array of int \\(DELIV[1 .... n]\\) Where \\(DELIV_i[j]\\) = no. of messages from \\(P_j\\) that have been delivered to \\(P_i\\)","title":"Local Variables"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#the-algorithm","text":"Message Send Event , where \\(P_i\\) wants to send message \\(M\\) to \\(P_j\\) : \\(send(M, SENT)\\) to \\(P_j\\) (Here \\(SENT\\) array is the log which will be used to determine if the message needs to be buffered or not) \\(SENT[i, j] = SENT[i, j] + 1\\) Message Arrival Event , when \\((M, SENT_j)\\) arrives at \\(P_i\\) from \\(P_j\\) : deliver \\(M\\) to \\(P_i\\) when for each process \\(x\\) , \\(DELIV_i[x] \\ge SENT_j[x, i]\\) \\(\\forall x, y, SENT_i[x, y] = max(SENT_i[x, y], SENT_j[x, y])\\) \\(DELIV_i[j] = DELIV_i[j] + 1\\)","title":"The Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#algorithm-complexity","text":"Space complexity at each process: \\(\\mathcal{O}(n^2)\\) integers Space overhead per message: \\(n^2\\) integers Time complexity at each process for each send and deliver event: \\(\\mathcal{O}(n^2)\\)","title":"Algorithm Complexity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week4DC2.html#algorithm-in-action_1","text":"Assume following steps have occurred till now - \\(P_1\\) sent 3 messages to \\(P_2\\) - \\(P_1\\) sent 4 messages to \\(P_3\\) - \\(P_2\\) sent 5 messages to \\(P_1\\) - \\(P_2\\) sent 2 messages to \\(P_3\\) - \\(P_3\\) sent 4 messages to \\(P_2\\) Variable state Assuming \\(SENT\\) of all \\(P\\) is aware of all the other values of \\(SENT\\) , \\(SENT\\) of \\(P_1\\) : | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Assume the following values for \\(DELIV\\) arrays \\(DELIV_1 = [0, 4, 0]\\) \\(DELIV_2 = [3, 0, 4]\\) \\(DELIV_3 = [3, 2, 0]\\) MESSAGE EVENTS: Now if, \\(P_1\\) sends \\(m_1\\) to \\(P_2\\) 1. \\(SEND\\) event from \\(P_1\\) 1. \\(SENT_1\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_1, SENT_1)\\) 3. Updated \\(SENT_1\\) | 0 | 3+1 | 4 | | 0 | 4 | 4 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4 | 0 | | 0 | 4 | 0 | 2. \\(RECEIVE\\) \\(m_1\\) from \\(P_1\\) at \\(P_2\\) 1. \\(DELIV_2[1] \\ge SENT_1[1, 2]\\) 2. \\(DELIV_2[3] \\ge SENT_1[3, 2]\\) 3. Deliver \\(m_1\\) to \\(P_2\\) 4. \\(DELIV_2[1] = DELIV_2[1] + 1 = 4\\) 5. \\(DELIV_2 = [4, 0, 4]\\) 6. \\(SENT_2\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Now if, \\(P_3\\) sends \\(m_2\\) to \\(P_2\\) 1. \\(SEND\\) event from \\(P_3\\) 1. \\(SENT_3\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_2, SENT_3)\\) 3. Updated \\(SENT_3\\) | 0 | 3 | 4 | | 0 | 3 | 4 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4+1 | 0 | | 0 | 5 | 0 | 2. \\(RECEIVE\\) \\(m_2\\) from \\(P_3\\) at \\(P_2\\) 1. \\(DELIV_2[1] \\ge SENT_3[1, 2]\\) 2. \\(DELIV_2[3] \\ge SENT_3[3, 2]\\) 3. Deliver \\(m_2\\) to \\(P_2\\) 4. \\(DELIV_2[3] = DELIV_2[3] + 1 = 5\\) 5. \\(DELIV_2 = [4, 0, 5]\\) 6. \\(SENT_2\\) | 0 | 3 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | Now if, \\(P_1\\) sends \\(m_3\\) to \\(P_3\\) 1. \\(SEND\\) event from \\(P_1\\) 1. \\(SENT_1\\) | 0 | 4 | 4 | | 5 | 0 | 2 | | 0 | 4 | 0 | 2. Send \\((m_2, SENT_3)\\) 3. Updated \\(SENT_3\\) | 0 | 4 | 4+1 | | 0 | 4 | 5 | | 5 | 0 | 2 | => | 5 | 0 | 2 | | 0 | 4 | 0 | | 0 | 5 | 0 | 2. \\(RECEIVE\\) \\(m_3\\) from \\(P_1\\) at \\(P_3\\) 1. \\(DELIV_3[1] \\ge SENT_1[1, 3]\\) (FALSE) 2. \\(DELIV_3[2] \\ge SENT_1[2, 3]\\) (TRUE) 3. Put \\(m_3\\) in buffer since the first condition failed, only process it after the other \\(P_1\\) messages are delivered to \\(P_3\\) Tags: !DistributedComputingIndex","title":"Algorithm in action"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html","text":"Week 5 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 28/Aug/2021 Topics Covered # Message Ordering (cont.) Birman-Schiper-Stephenson Protocol \\(P_i\\) sends a message \\(m\\) to \\(P_j\\) \\(P_j\\) receives a message \\(m\\) to \\(P_i\\) Distributed Mutual Exclusion Types of Approaches Quorum Based Requirements of Mutual Exclusion Algorithms Performance Metrics Lamport's Algorithm Requesting the Critical Section Executing the Critical Section Releasing the Critical Section Example Problem Message Ordering (cont.) # Birman-Schiper-Stephenson Protocol # \\(C_i\\) = Vector clock of \\(P_i\\) \\(C_i[j]\\) = \\(j^{th}\\) element of \\(C_i\\) \\(tm\\) = Vector timestamp for message \\(m\\) , stamped after local clock is incremented. NOTE , we also assume that all messages taking part in this algorithm is a broadcast \\(P_i\\) sends a message \\(m\\) to \\(P_j\\) # \\(P_i\\) increments \\(C_i[i]\\) \\(P_i\\) sets the timestamp \\(tm = C_i\\) for message \\(m\\) \\(P_j\\) receives a message \\(m\\) to \\(P_i\\) # When \\(P_j (j \\ne i)\\) reveives \\(m\\) with timestamp \\(tm\\) , it delays message delivery until: \\(C_j[i] = tm[i] - 1\\) , \\(P_j\\) has received all preceding messages sent by \\(P_i\\) \\(\\forall k \\le n\\) and \\(k \\ne i\\) , \\(C_j[k] \\ge tm[k]\\) , P_j has received all the messages that were received at \\(P_i\\) from other processes before \\(P_i\\) sent \\(m\\) Here we need to check if \\(P_j\\) also received \\(P_k\\) message before message from \\(P_i\\) is processed When \\(m\\) is delivered to \\(P_j\\) , update \\(P_j\\) 's vector clock, \\(\\forall i, C_j[i] = max(C_j[i], tm[i])\\) Check buffered messages to see if any can be delivered Example Problem # \\(P_3\\) executes a broadcast - \\(C_3 = [0, 0, 1]\\) - \\(tm_1 = [0, 0, 1]\\) \\(P_2\\) receives message \\(m_1\\) - \\(C_2 = [0, 0, 0]\\) - Condition checks: 1. \\(C_2[3] = tm_1[3] - 1\\) is TRUE 2. \\(C_2[i] \\ge tm_1[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 3\\) is TRUE - \\(C_2[i] = max(C_2[i], tm_1[i]), \\forall i\\) - \\(C_2 = [0, 0, 1]\\) \\(P_2\\) sends message \\(m_2\\) - \\(C_2 = [0, 1, 1]\\) - \\(tm_2 = [0, 1, 1]\\) \\(P_3\\) receives message \\(m_2\\) - \\(C_3 = [0, 0, 1]\\) - Condition checks: 1. \\(C_3[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_3[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is TRUE - \\(C_3[i] = max(C_3[i], tm_2[i]), \\forall i\\) - \\(C_3 = [0, 1, 1]\\) \\(P_1\\) receives message \\(m_2\\) - \\(C_1 = [0, 0, 0]\\) - Condition checks: 1. \\(C_1[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is FALSE - \\(m_2\\) is added to BUFFER \\(P_1\\) receives message \\(m_1\\) - \\(C_1 = [0, 0, 0]\\) - Condition checks: 1. \\(C_1[3] = tm_1[3] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_1[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 3\\) is TRUE - \\(C_1[i] = max(C_1[i], tm_1[i]), \\forall i\\) - \\(C_1 = [0, 0, 1]\\) Deliver \\(m_2\\) from BUFFER to \\(P_1\\) - \\(C_1 = [0, 0, 1]\\) - Condition checks: 1. \\(C_1[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is TRUE - \\(C_1[i] = max(C_1[i], tm_1[i]), \\forall i\\) - \\(C_1 = [0, 1, 1]\\) NOTE , in the exam make sure that all the above steps are also written in the paper, annotating the diagram alone will lead to getting partial marks Distributed Mutual Exclusion # Types of Approaches # Token based Assertion based From Recorded Lecture Quorum based Quorum Based # Each site requests permission to execute the CS from a subset of sites called quorum Quorums are formed in such a way that when two sites concurrently request access to the CS At least one site receives both the requests This site is responsible to make sure that only one request executes the CS at any time Requirements of Mutual Exclusion Algorithms # Safety Property: At any instant, only one process can execute the CS Liveness Property: A site must not wait indefinitely to execute the CS while other sites are repeatedly executing the CS Fairness: Each process gets a fair chance to execute the CS CS execution requests are executed in order of their arrival time Time is determined by a logical clock Performance Metrics # From Recorded lectures Lamport's Algorithm # Every site \\(S_i\\) keeps a queue, \\(request\\_queue_i\\) \\(request\\_queue_i\\) contains mutual exclusion requests ordered by their timestamps FIFO \\(CS\\) requests are executed in increasing order of timestamps Requesting the Critical Section # When a site \\(S_i\\) wants to enter the CS, it broadcasts a \\(REQUEST(ts_i, i)\\) message to all other sites and places the request on \\(request\\_queue_i\\) . When site \\(S_j\\) receives the \\(REQUEST(ts_i, i)\\) message from site \\(S_i\\) , it places site \\(S_j\\) 's request on \\(request\\_queue_j\\) and returns a timestamped REPLY message to \\(S_i\\) Executing the Critical Section # Site \\(S_i\\) enters the CS when the following conditions hold: - L1: \\(S_i\\) has received a message with timestamp larger than \\((ts_i, i)\\) - L2: \\(S_i\\) 's request is at the top of \\(request\\_queue_i\\) Releasing the Critical Section # Site \\(S_i\\) , upon exiting the CS, removes its request from the top of its request queue and broadcasts a timestamped RELEASE message to all other sites When a site \\(S_j\\) receives a RELEASE message from site \\(S_i\\) , it removes \\(S_i\\) 's request from its request queue Example Problem # \\(S_1\\) and \\(S_2\\) are requesting for the CS at \\(S_2\\) \\(request\\_queue_2\\) \\(= [(1, 2)]\\) at \\(S_1\\) \\(request\\_queue_1\\) \\(= [(1, 1)]\\) \\(S_1\\) Receive request for CS from \\(S_2\\) : \\(request\\_queue_1\\) \\(= [(1, 1),(1, 2)]\\) (This priority is calculated based on the index value \\(i\\) ) \\(S_1\\) sends a REPLY to \\(S_2\\) \\(S_3\\) Receive request for CS from \\(S_2\\) : \\(request\\_queue_3\\) \\(= [(1, 2)]\\) \\(S_3\\) sends a REPLY to \\(S_2\\) \\(S_3\\) Receive request for CS from \\(S_1\\) : \\(request\\_queue_3\\) \\(= [(1, 1), (1, 2)]\\) \\(S_3\\) sends a REPLY to \\(S_1\\) \\(S_1\\) enters the CS \\(S_1\\) sends RELEASE message \\(S_2\\) removes \\((1,1)\\) from \\(request\\_queue_2\\) \\(S_2\\) enters the CS \\(S_3\\) removes \\((1,1)\\) from \\(request\\_queue_3\\) \\(S_2\\) sends RELEASE message \\(S_1\\) removes \\((1,2)\\) from \\(request\\_queue_1\\) \\(S_3\\) removes \\((1,2)\\) from \\(request\\_queue_3\\) Tags: !DistributedComputingIndex","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#week-5","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 28/Aug/2021","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#topics-covered","text":"Message Ordering (cont.) Birman-Schiper-Stephenson Protocol \\(P_i\\) sends a message \\(m\\) to \\(P_j\\) \\(P_j\\) receives a message \\(m\\) to \\(P_i\\) Distributed Mutual Exclusion Types of Approaches Quorum Based Requirements of Mutual Exclusion Algorithms Performance Metrics Lamport's Algorithm Requesting the Critical Section Executing the Critical Section Releasing the Critical Section Example Problem","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#message-ordering-cont","text":"","title":"Message Ordering (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#birman-schiper-stephenson-protocol","text":"\\(C_i\\) = Vector clock of \\(P_i\\) \\(C_i[j]\\) = \\(j^{th}\\) element of \\(C_i\\) \\(tm\\) = Vector timestamp for message \\(m\\) , stamped after local clock is incremented. NOTE , we also assume that all messages taking part in this algorithm is a broadcast","title":"Birman-Schiper-Stephenson Protocol"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#p_i-sends-a-message-m-to-p_j","text":"\\(P_i\\) increments \\(C_i[i]\\) \\(P_i\\) sets the timestamp \\(tm = C_i\\) for message \\(m\\)","title":"\\(P_i\\) sends a message \\(m\\) to \\(P_j\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#p_j-receives-a-message-m-to-p_i","text":"When \\(P_j (j \\ne i)\\) reveives \\(m\\) with timestamp \\(tm\\) , it delays message delivery until: \\(C_j[i] = tm[i] - 1\\) , \\(P_j\\) has received all preceding messages sent by \\(P_i\\) \\(\\forall k \\le n\\) and \\(k \\ne i\\) , \\(C_j[k] \\ge tm[k]\\) , P_j has received all the messages that were received at \\(P_i\\) from other processes before \\(P_i\\) sent \\(m\\) Here we need to check if \\(P_j\\) also received \\(P_k\\) message before message from \\(P_i\\) is processed When \\(m\\) is delivered to \\(P_j\\) , update \\(P_j\\) 's vector clock, \\(\\forall i, C_j[i] = max(C_j[i], tm[i])\\) Check buffered messages to see if any can be delivered","title":"\\(P_j\\) receives a message \\(m\\) to \\(P_i\\)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#example-problem","text":"\\(P_3\\) executes a broadcast - \\(C_3 = [0, 0, 1]\\) - \\(tm_1 = [0, 0, 1]\\) \\(P_2\\) receives message \\(m_1\\) - \\(C_2 = [0, 0, 0]\\) - Condition checks: 1. \\(C_2[3] = tm_1[3] - 1\\) is TRUE 2. \\(C_2[i] \\ge tm_1[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 3\\) is TRUE - \\(C_2[i] = max(C_2[i], tm_1[i]), \\forall i\\) - \\(C_2 = [0, 0, 1]\\) \\(P_2\\) sends message \\(m_2\\) - \\(C_2 = [0, 1, 1]\\) - \\(tm_2 = [0, 1, 1]\\) \\(P_3\\) receives message \\(m_2\\) - \\(C_3 = [0, 0, 1]\\) - Condition checks: 1. \\(C_3[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_3[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is TRUE - \\(C_3[i] = max(C_3[i], tm_2[i]), \\forall i\\) - \\(C_3 = [0, 1, 1]\\) \\(P_1\\) receives message \\(m_2\\) - \\(C_1 = [0, 0, 0]\\) - Condition checks: 1. \\(C_1[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is FALSE - \\(m_2\\) is added to BUFFER \\(P_1\\) receives message \\(m_1\\) - \\(C_1 = [0, 0, 0]\\) - Condition checks: 1. \\(C_1[3] = tm_1[3] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_1[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 3\\) is TRUE - \\(C_1[i] = max(C_1[i], tm_1[i]), \\forall i\\) - \\(C_1 = [0, 0, 1]\\) Deliver \\(m_2\\) from BUFFER to \\(P_1\\) - \\(C_1 = [0, 0, 1]\\) - Condition checks: 1. \\(C_1[2] = tm_2[2] - 1\\) is TRUE 2. \\(C_1[i] \\ge tm_2[i]\\) \\(\\forall i \\le n\\) and \\(i \\ne 2\\) is TRUE - \\(C_1[i] = max(C_1[i], tm_1[i]), \\forall i\\) - \\(C_1 = [0, 1, 1]\\) NOTE , in the exam make sure that all the above steps are also written in the paper, annotating the diagram alone will lead to getting partial marks","title":"Example Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#distributed-mutual-exclusion","text":"","title":"Distributed Mutual Exclusion"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#types-of-approaches","text":"Token based Assertion based From Recorded Lecture Quorum based","title":"Types of Approaches"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#quorum-based","text":"Each site requests permission to execute the CS from a subset of sites called quorum Quorums are formed in such a way that when two sites concurrently request access to the CS At least one site receives both the requests This site is responsible to make sure that only one request executes the CS at any time","title":"Quorum Based"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#requirements-of-mutual-exclusion-algorithms","text":"Safety Property: At any instant, only one process can execute the CS Liveness Property: A site must not wait indefinitely to execute the CS while other sites are repeatedly executing the CS Fairness: Each process gets a fair chance to execute the CS CS execution requests are executed in order of their arrival time Time is determined by a logical clock","title":"Requirements of Mutual Exclusion Algorithms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#performance-metrics","text":"From Recorded lectures","title":"Performance Metrics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#lamports-algorithm","text":"Every site \\(S_i\\) keeps a queue, \\(request\\_queue_i\\) \\(request\\_queue_i\\) contains mutual exclusion requests ordered by their timestamps FIFO \\(CS\\) requests are executed in increasing order of timestamps","title":"Lamport's Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#requesting-the-critical-section","text":"When a site \\(S_i\\) wants to enter the CS, it broadcasts a \\(REQUEST(ts_i, i)\\) message to all other sites and places the request on \\(request\\_queue_i\\) . When site \\(S_j\\) receives the \\(REQUEST(ts_i, i)\\) message from site \\(S_i\\) , it places site \\(S_j\\) 's request on \\(request\\_queue_j\\) and returns a timestamped REPLY message to \\(S_i\\)","title":"Requesting the Critical Section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#executing-the-critical-section","text":"Site \\(S_i\\) enters the CS when the following conditions hold: - L1: \\(S_i\\) has received a message with timestamp larger than \\((ts_i, i)\\) - L2: \\(S_i\\) 's request is at the top of \\(request\\_queue_i\\)","title":"Executing the Critical Section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#releasing-the-critical-section","text":"Site \\(S_i\\) , upon exiting the CS, removes its request from the top of its request queue and broadcasts a timestamped RELEASE message to all other sites When a site \\(S_j\\) receives a RELEASE message from site \\(S_i\\) , it removes \\(S_i\\) 's request from its request queue","title":"Releasing the Critical Section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week5DC.html#example-problem_1","text":"\\(S_1\\) and \\(S_2\\) are requesting for the CS at \\(S_2\\) \\(request\\_queue_2\\) \\(= [(1, 2)]\\) at \\(S_1\\) \\(request\\_queue_1\\) \\(= [(1, 1)]\\) \\(S_1\\) Receive request for CS from \\(S_2\\) : \\(request\\_queue_1\\) \\(= [(1, 1),(1, 2)]\\) (This priority is calculated based on the index value \\(i\\) ) \\(S_1\\) sends a REPLY to \\(S_2\\) \\(S_3\\) Receive request for CS from \\(S_2\\) : \\(request\\_queue_3\\) \\(= [(1, 2)]\\) \\(S_3\\) sends a REPLY to \\(S_2\\) \\(S_3\\) Receive request for CS from \\(S_1\\) : \\(request\\_queue_3\\) \\(= [(1, 1), (1, 2)]\\) \\(S_3\\) sends a REPLY to \\(S_1\\) \\(S_1\\) enters the CS \\(S_1\\) sends RELEASE message \\(S_2\\) removes \\((1,1)\\) from \\(request\\_queue_2\\) \\(S_2\\) enters the CS \\(S_3\\) removes \\((1,1)\\) from \\(request\\_queue_3\\) \\(S_2\\) sends RELEASE message \\(S_1\\) removes \\((1,2)\\) from \\(request\\_queue_1\\) \\(S_3\\) removes \\((1,2)\\) from \\(request\\_queue_3\\) Tags: !DistributedComputingIndex","title":"Example Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html","text":"Week 6 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 4/Sep/2021 Topics Covered # Distributed Mutual Exclusion (Cont.) Ricart Agrawala Algorithm Requesting the critical section Executing the critical section Releasing the critical section Example Problem Maekawa's Algorithm Problem Example Algorithm Steps Deadlocks Raymond's Tree Based Algorithm Example Data Structures Steps of algorithm Suzuki-Kasami's Broadcast Algorithm Distributed Mutual Exclusion (Cont.) # Ricart Agrawala Algorithm # Communication channels are not required to be FIFO \\(REQUEST\\) and \\(REPLY\\) messages Each process \\(p_i\\) maintains the request deferred array \\(RD_i\\) Size of \\(RD_i\\) = no. of processes in the system Initially, \\(\\forall i\\ \\forall j: RD_i[j] = 0\\) Whenever \\(p_i\\) defers the request sent by \\(p_j\\) , it sets \\(RD_i[j] = 1\\) After it has sent a \\(REPLY\\) message to \\(p_j\\) , it sets \\(RD_i[j] = 0\\) Requesting the critical section # When a site \\(S_i\\) wants to enter the CS, it broadcasts a timestamped \\(REQUEST\\) message to all other sites When site \\(S_j\\) receives a \\(REQUEST\\) message from site \\(S_i\\) , it sends a \\(REPLY\\) message to site \\(S_i\\) if site \\(S_j\\) is neither requesting nor executing the CS, or if the site \\(S_j\\) is requesting and \\(S_i\\) 's request's timestamp is smaller than site \\(S_j\\) 's own requests's timestamp. Otherwise, the reply is deferred and \\(S_j\\) sets \\(RD_j[i] = 1\\) Executing the critical section # Site \\(S_i\\) enters the CS after it has received a \\(REPLY\\) message from every site it sent a \\(REQUEST\\) message to Releasing the critical section # When site \\(S_i\\) exits the CS, it sends all the deferred REPLY messages: \\(\\forall\\ j\\) if \\(RD_i[j] = 1\\) , then \\(S_i\\) sends a REPLY message to \\(S_j\\) and sets \\(RD_i[j] = 0\\) Example Problem # - \\(S_1\\) has timestamp \\((1, 1)\\) - \\(S_2\\) has timestamp \\((1, 2)\\) - \\(S_3\\) receives \\(S_2\\) request so sends \\(REPLY\\) (dotted line) to \\(S_2\\) - \\(S_1\\) defers \\(S_2\\) request since it needs to get CS - \\(S_2\\) sends \\(S_1\\) \\(REPLY\\) since \\(S_1\\) has higher priority - \\(S_3\\) sends \\(S_1\\) \\(REPLY\\) - \\(S_1\\) enters CS since it received \\(REPLY\\) from all other sites - At this point \\(RD_1 = [0, 1, 0]\\) - \\(S_1\\) finishes executing the CS - \\(S_1\\) sends \\(REPLY\\) to \\(S_2\\) deferred \\(REQUEST\\) - At this point \\(RD_1 = [0, 0, 0]\\) - \\(S_2\\) enters the CS since \\(S_2\\) has received \\(REPLY\\) from all the sites Performance - requires 2(N - 1) messages per CS execution Maekawa's Algorithm # It is a quorum based mutual exclusion algorithm (Explained here Week5DC#Quorum Based ) Request sets for sites are constructed to satisfy the following conditionsL M1: \\(\\forall i \\forall j, i \\ne j, q \\le i, j \\le N :: R_i \\cap R_j \\ne \\phi\\) M2: \\(\\forall i : 1 \\le i \\le N :: S_i \\in R_i\\) M3: \\(\\forall i : 1 \\le i \\le N :: |R_i| = K\\) for some \\(K\\) M4: Any site S_j is contained in K number of R_i's, \\(1 \\le i\\) , \\(j \\le N\\) Maekawa showed that \\(N = K(K - 1) + 1\\) This relation gives \\(|R_j| = K = \\sqrt(N)\\) Uses \\(REQUEST\\) , \\(REPLY\\) and \\(RELEASE\\) messages Performance -> \\(3\\sqrt(N)\\) messages per CS invocation Problem Example # If \\(N = 7\\) \\(K = 3\\) since there are 7 Sites, there will be 7 Request Sets ( \\(R_1\\) to \\(R_7\\) ) each of size \\(K = 3\\) Let us take \\(R_1 = {S_1, S_3, S_4}\\) , then: - The Request Set \\(R_2 = {S_2, S_5, S_6}\\) is wrong since M1 is not satisfied - The Request Set \\(R3 = {S_3, S_4}\\) is wring since M3 is not satisfied If the following Request sets are taken: - \\(R_1 = {S_1, S_3, S_4}\\) - \\(R_2 = {S_1, S_2, S_5}\\) - \\(R_3 = {S_1, S_2, S_3}\\) - \\(R_4 = {S_1, S_4, S_6}\\) Then, it is wrong since S_1 is in 4 places and according to M4 only K (3) number of occurrences are allowed Algorithm Steps # From Recorded Lecture Deadlocks # From Recorded Lecture Raymond's Tree Based Algorithm # Uses a spanning tree of the network Each node maintains a \\(HOLDER\\) variable that provides information about the placement of the privilege in relation to the node itself A node stores in its \\(HOLDER\\) variable the identity of a node that it thinks has the privilege or leads to the node having the privilege For 2 nodes \\(X\\) and \\(Y\\) , if \\(HOLDER_X = Y\\) the undirected edge between \\(X\\) and \\(Y\\) can be redrawn as a directed edge from \\(X\\) to \\(Y\\) Node containing the privilege is also known as the root node Example # - Messages between nodes traverse along undirected edges of the tree - Node needs to hold information about and communicate only to its immediate neighboring nodes - Here \\(N3\\) holds privilege to itself Consider \\(N6\\) sends a \\(REQUEST\\) , then it travels along the directed edge When \\(N3\\) receives \\(REQUEST\\) to give privilege, and it is not using it, then \\(N3\\) will send message back to whoever sent the \\(REQUEST\\) and that will further send it down till the node that sent the original \\(REQUEST\\) . When that happens the \\(HOLDER\\) variable for each node changes it when it forwards the \\(REQUEST\\) back to the original node Data Structure # \\(HOLDER\\) \\(USING\\) Possible values true or false Indicates if the current node is executing the critical section \\(ASKED\\) Possible values true or false Indicates if node has sent a request for the privilege Prevents the sending of duplicate requests for privilege \\(REQUEST_Q\\) FIFO queue that can contain \"self\" or the identities of immediate neighbors as elements \\(REQUEST_Q\\) of a node consists of the identities of those immediate neighbors that have requested for privilege but have not yet been sent the privilege Maximum size of \\(REQUEST_Q\\) of a node is the number of \\(immediate neighbors + 1\\) (for \"self\") Steps of algorithm # From Recorded Lectures Suzuki-Kasami's Broadcast Algorithm # Broadcasts a \\(REQUEST\\) message for the token to all other sites Site that possesses the token sends it to the requesting site upon the receipt of its \\(REQUEST\\) message If a site receives a \\(REQUEST\\) message when it is executing the CS, it sends the token only after it has completed the CS execution Distinguish outdated and current REQUEST messages \\(REQUEST(j, sn)\\) where \\(sn\\) is a sequence number that indicates that \\(S_j\\) is requesting its \\(sn^{th}\\) CS execution \\(S_i\\) keeps an array of integers \\(RN_i[1, .... , n]\\) where \\(RN_i[i]\\) is the largest sequence number received in a \\(REQUEST\\) message so far from \\(S_j\\) When \\(S_i\\) receives a \\(REQUEST(j, sn)\\) message, it sets \\(RN_i[j] = max(RN_i[j], sn)\\) When \\(S_i\\) receives a \\(REQUEST(j, sn)\\) message, the request is outdated if \\(RN_i[j] > sn\\) Tags: !DistributedComputingIndex","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#week-6","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 4/Sep/2021","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#topics-covered","text":"Distributed Mutual Exclusion (Cont.) Ricart Agrawala Algorithm Requesting the critical section Executing the critical section Releasing the critical section Example Problem Maekawa's Algorithm Problem Example Algorithm Steps Deadlocks Raymond's Tree Based Algorithm Example Data Structures Steps of algorithm Suzuki-Kasami's Broadcast Algorithm","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#distributed-mutual-exclusion-cont","text":"","title":"Distributed Mutual Exclusion (Cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#ricart-agrawala-algorithm","text":"Communication channels are not required to be FIFO \\(REQUEST\\) and \\(REPLY\\) messages Each process \\(p_i\\) maintains the request deferred array \\(RD_i\\) Size of \\(RD_i\\) = no. of processes in the system Initially, \\(\\forall i\\ \\forall j: RD_i[j] = 0\\) Whenever \\(p_i\\) defers the request sent by \\(p_j\\) , it sets \\(RD_i[j] = 1\\) After it has sent a \\(REPLY\\) message to \\(p_j\\) , it sets \\(RD_i[j] = 0\\)","title":"Ricart Agrawala Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#requesting-the-critical-section","text":"When a site \\(S_i\\) wants to enter the CS, it broadcasts a timestamped \\(REQUEST\\) message to all other sites When site \\(S_j\\) receives a \\(REQUEST\\) message from site \\(S_i\\) , it sends a \\(REPLY\\) message to site \\(S_i\\) if site \\(S_j\\) is neither requesting nor executing the CS, or if the site \\(S_j\\) is requesting and \\(S_i\\) 's request's timestamp is smaller than site \\(S_j\\) 's own requests's timestamp. Otherwise, the reply is deferred and \\(S_j\\) sets \\(RD_j[i] = 1\\)","title":"Requesting the critical section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#executing-the-critical-section","text":"Site \\(S_i\\) enters the CS after it has received a \\(REPLY\\) message from every site it sent a \\(REQUEST\\) message to","title":"Executing the critical section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#releasing-the-critical-section","text":"When site \\(S_i\\) exits the CS, it sends all the deferred REPLY messages: \\(\\forall\\ j\\) if \\(RD_i[j] = 1\\) , then \\(S_i\\) sends a REPLY message to \\(S_j\\) and sets \\(RD_i[j] = 0\\)","title":"Releasing the critical section"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#example-problem","text":"- \\(S_1\\) has timestamp \\((1, 1)\\) - \\(S_2\\) has timestamp \\((1, 2)\\) - \\(S_3\\) receives \\(S_2\\) request so sends \\(REPLY\\) (dotted line) to \\(S_2\\) - \\(S_1\\) defers \\(S_2\\) request since it needs to get CS - \\(S_2\\) sends \\(S_1\\) \\(REPLY\\) since \\(S_1\\) has higher priority - \\(S_3\\) sends \\(S_1\\) \\(REPLY\\) - \\(S_1\\) enters CS since it received \\(REPLY\\) from all other sites - At this point \\(RD_1 = [0, 1, 0]\\) - \\(S_1\\) finishes executing the CS - \\(S_1\\) sends \\(REPLY\\) to \\(S_2\\) deferred \\(REQUEST\\) - At this point \\(RD_1 = [0, 0, 0]\\) - \\(S_2\\) enters the CS since \\(S_2\\) has received \\(REPLY\\) from all the sites Performance - requires 2(N - 1) messages per CS execution","title":"Example Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#maekawas-algorithm","text":"It is a quorum based mutual exclusion algorithm (Explained here Week5DC#Quorum Based ) Request sets for sites are constructed to satisfy the following conditionsL M1: \\(\\forall i \\forall j, i \\ne j, q \\le i, j \\le N :: R_i \\cap R_j \\ne \\phi\\) M2: \\(\\forall i : 1 \\le i \\le N :: S_i \\in R_i\\) M3: \\(\\forall i : 1 \\le i \\le N :: |R_i| = K\\) for some \\(K\\) M4: Any site S_j is contained in K number of R_i's, \\(1 \\le i\\) , \\(j \\le N\\) Maekawa showed that \\(N = K(K - 1) + 1\\) This relation gives \\(|R_j| = K = \\sqrt(N)\\) Uses \\(REQUEST\\) , \\(REPLY\\) and \\(RELEASE\\) messages Performance -> \\(3\\sqrt(N)\\) messages per CS invocation","title":"Maekawa's Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#problem-example","text":"If \\(N = 7\\) \\(K = 3\\) since there are 7 Sites, there will be 7 Request Sets ( \\(R_1\\) to \\(R_7\\) ) each of size \\(K = 3\\) Let us take \\(R_1 = {S_1, S_3, S_4}\\) , then: - The Request Set \\(R_2 = {S_2, S_5, S_6}\\) is wrong since M1 is not satisfied - The Request Set \\(R3 = {S_3, S_4}\\) is wring since M3 is not satisfied If the following Request sets are taken: - \\(R_1 = {S_1, S_3, S_4}\\) - \\(R_2 = {S_1, S_2, S_5}\\) - \\(R_3 = {S_1, S_2, S_3}\\) - \\(R_4 = {S_1, S_4, S_6}\\) Then, it is wrong since S_1 is in 4 places and according to M4 only K (3) number of occurrences are allowed","title":"Problem Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#algorithm-steps","text":"From Recorded Lecture","title":"Algorithm Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#deadlocks","text":"From Recorded Lecture","title":"Deadlocks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#raymonds-tree-based-algorithm","text":"Uses a spanning tree of the network Each node maintains a \\(HOLDER\\) variable that provides information about the placement of the privilege in relation to the node itself A node stores in its \\(HOLDER\\) variable the identity of a node that it thinks has the privilege or leads to the node having the privilege For 2 nodes \\(X\\) and \\(Y\\) , if \\(HOLDER_X = Y\\) the undirected edge between \\(X\\) and \\(Y\\) can be redrawn as a directed edge from \\(X\\) to \\(Y\\) Node containing the privilege is also known as the root node","title":"Raymond's Tree Based Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#example","text":"- Messages between nodes traverse along undirected edges of the tree - Node needs to hold information about and communicate only to its immediate neighboring nodes - Here \\(N3\\) holds privilege to itself Consider \\(N6\\) sends a \\(REQUEST\\) , then it travels along the directed edge When \\(N3\\) receives \\(REQUEST\\) to give privilege, and it is not using it, then \\(N3\\) will send message back to whoever sent the \\(REQUEST\\) and that will further send it down till the node that sent the original \\(REQUEST\\) . When that happens the \\(HOLDER\\) variable for each node changes it when it forwards the \\(REQUEST\\) back to the original node","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#data-structure","text":"\\(HOLDER\\) \\(USING\\) Possible values true or false Indicates if the current node is executing the critical section \\(ASKED\\) Possible values true or false Indicates if node has sent a request for the privilege Prevents the sending of duplicate requests for privilege \\(REQUEST_Q\\) FIFO queue that can contain \"self\" or the identities of immediate neighbors as elements \\(REQUEST_Q\\) of a node consists of the identities of those immediate neighbors that have requested for privilege but have not yet been sent the privilege Maximum size of \\(REQUEST_Q\\) of a node is the number of \\(immediate neighbors + 1\\) (for \"self\")","title":"Data Structure"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#steps-of-algorithm","text":"From Recorded Lectures","title":"Steps of algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week6DC.html#suzuki-kasamis-broadcast-algorithm","text":"Broadcasts a \\(REQUEST\\) message for the token to all other sites Site that possesses the token sends it to the requesting site upon the receipt of its \\(REQUEST\\) message If a site receives a \\(REQUEST\\) message when it is executing the CS, it sends the token only after it has completed the CS execution Distinguish outdated and current REQUEST messages \\(REQUEST(j, sn)\\) where \\(sn\\) is a sequence number that indicates that \\(S_j\\) is requesting its \\(sn^{th}\\) CS execution \\(S_i\\) keeps an array of integers \\(RN_i[1, .... , n]\\) where \\(RN_i[i]\\) is the largest sequence number received in a \\(REQUEST\\) message so far from \\(S_j\\) When \\(S_i\\) receives a \\(REQUEST(j, sn)\\) message, it sets \\(RN_i[j] = max(RN_i[j], sn)\\) When \\(S_i\\) receives a \\(REQUEST(j, sn)\\) message, the request is outdated if \\(RN_i[j] > sn\\) Tags: !DistributedComputingIndex","title":"Suzuki-Kasami's Broadcast Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html","text":"Week 7 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 11/Sep/2021 Topics Covered # Deadlock Introduction System Model Wait-For-Graph (WFG) Knot AND Model OR Model Chandy Misra Haas Algorithm for the AND model Data Structures Steps Example Deadlock Introduction # Deadlocks are explained in details here PreRecordedModule6#What is a deadlock A process may request resources in any order Request order may not be known apriori A process can request a resource while holding others If the allocation sequence of process resources is not controlled in such environments Deadlock condition where a set of processes request resources that are held by other processes in the set System Model # N processors, N processes, each process runs on a prrocessor Systems have only reusable resources Processes are allowed to make only exclusive access to resources Only on copy/instance of each resource is present Process can be in two states Running Blocked Running state/active state: Process has all the needed resources Either is executing or is ready for execution Blocked state: Process is waiting to acquire some resource Wait-For-Graph (WFG) # - State of a distributed system can be modeled as a directed graph - Nodes are processes - A directed edge from node \\(P_1\\) to node \\(P_2\\) if - \\(P_1\\) is blocked - \\(P_1\\) is waiting for \\(P_2\\) to release some resource - A system is deadlocked if and only if there exists a directed cycle or knot in the WFG Knot # More info here PreRecordedModule6#Cycle Vs Knot AND Model # More details inn PreRecordedModule6#Cycle Vs Knot In an AND graph a cycle is a sufficient condition to determine a deadlock OR Model # More details in PreRecordedModule6#OR WFGs and PreRecordedModule6#Cycle Vs Knot In an OR graph, a cycle is not a sufficient condition to determine a deadlock, we also need a knot In the above diagram you can see that P_11 can satisfy its need by taking the resource used by P_33 and can change from Waiting to Active state. Chandy Misra Haas Algorithm for the AND model # Explained in detail here PreRecordedModule6#Chandy-Misra-Haas CMH Edge-Chasing for AND Graphs - Uses a special message called probe - Probe is a triplet \\((i, j, k)\\) - Denotes that - It belongs to a deadlock detection initiated for \\(P_i\\) (1 st element) - It is sent by the site of \\(P_j\\) - It sent to the site \\(P_k\\) - Probe message travels along the edges of the global WFG graph - Deadlock is detected when a probe message returns to the process that initialed it Data Structures # Each process \\(P_i\\) maintains a bl=boolean array, \\(dependent_i\\) \\(dependent_i(j)\\) is \\(TRUE\\) if \\(P_i\\) knows that \\(P_j\\) is dependent on it Initially \\(dependent_i(j)\\) is false for all \\(i\\) and \\(j\\) Steps # Example # Tags: !DistributedComputingIndex","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#week-7","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 11/Sep/2021","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#topics-covered","text":"Deadlock Introduction System Model Wait-For-Graph (WFG) Knot AND Model OR Model Chandy Misra Haas Algorithm for the AND model Data Structures Steps Example","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#deadlock-introduction","text":"Deadlocks are explained in details here PreRecordedModule6#What is a deadlock A process may request resources in any order Request order may not be known apriori A process can request a resource while holding others If the allocation sequence of process resources is not controlled in such environments Deadlock condition where a set of processes request resources that are held by other processes in the set","title":"Deadlock Introduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#system-model","text":"N processors, N processes, each process runs on a prrocessor Systems have only reusable resources Processes are allowed to make only exclusive access to resources Only on copy/instance of each resource is present Process can be in two states Running Blocked Running state/active state: Process has all the needed resources Either is executing or is ready for execution Blocked state: Process is waiting to acquire some resource","title":"System Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#wait-for-graph-wfg","text":"- State of a distributed system can be modeled as a directed graph - Nodes are processes - A directed edge from node \\(P_1\\) to node \\(P_2\\) if - \\(P_1\\) is blocked - \\(P_1\\) is waiting for \\(P_2\\) to release some resource - A system is deadlocked if and only if there exists a directed cycle or knot in the WFG","title":"Wait-For-Graph (WFG)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#knot","text":"More info here PreRecordedModule6#Cycle Vs Knot","title":"Knot"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#and-model","text":"More details inn PreRecordedModule6#Cycle Vs Knot In an AND graph a cycle is a sufficient condition to determine a deadlock","title":"AND Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#or-model","text":"More details in PreRecordedModule6#OR WFGs and PreRecordedModule6#Cycle Vs Knot In an OR graph, a cycle is not a sufficient condition to determine a deadlock, we also need a knot In the above diagram you can see that P_11 can satisfy its need by taking the resource used by P_33 and can change from Waiting to Active state.","title":"OR Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#chandy-misra-haas-algorithm-for-the-and-model","text":"Explained in detail here PreRecordedModule6#Chandy-Misra-Haas CMH Edge-Chasing for AND Graphs - Uses a special message called probe - Probe is a triplet \\((i, j, k)\\) - Denotes that - It belongs to a deadlock detection initiated for \\(P_i\\) (1 st element) - It is sent by the site of \\(P_j\\) - It sent to the site \\(P_k\\) - Probe message travels along the edges of the global WFG graph - Deadlock is detected when a probe message returns to the process that initialed it","title":"Chandy Misra Haas Algorithm for the AND model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#data-structures","text":"Each process \\(P_i\\) maintains a bl=boolean array, \\(dependent_i\\) \\(dependent_i(j)\\) is \\(TRUE\\) if \\(P_i\\) knows that \\(P_j\\) is dependent on it Initially \\(dependent_i(j)\\) is false for all \\(i\\) and \\(j\\)","title":"Data Structures"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#steps","text":"","title":"Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week7DC.html#example","text":"Tags: !DistributedComputingIndex","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html","text":"Week 8 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 16/Oct/2021 Topics Covered # Chandy Misra Haas Algorithm for the OR model Steps Examples Performance Analysis Deadlock Resolution Chandy Misra Haas Algorithm for the OR model # 2 types of messages are used: \\(query(i, j, k)\\) \\(reply(i, j, k)\\) Denote that they belong to a deadlock detection initiated by \\(P_i\\) and are being sent from \\(P_j\\) to \\(P_k\\) A blocked process initiates deadlock detection by sending query messages to all processes in its dependent set \\(DEPEN\\) Local variable \\(num_k(i)\\) = number of query messages sent \\(P_k\\) maintains a boolean variable \\(wait_k(i)\\) \\(wait_k(i)\\) denotes that \\(P_k\\) has been continuously blocked since it received the last engaging query \\(P_i\\) Steps # Examples # \\(B\\) sends first message \\(C\\) sends query message after getting query from \\(B\\) \\(F\\) sends query message after getting query from \\(B\\) \\(E\\) sends query message after getting query from \\(C\\) The message sent by \\(E\\) is non engaging for \\(F\\) so \\(F\\) sends a \\(reply\\) to \\(E\\) \\(D\\) sends a query to \\(E\\) after getting query from \\(C\\) The message sent by \\(D\\) is non engaging for \\(E\\) so \\(E\\) sends a \\(reply\\) to \\(D\\) \\(A\\) sends a query to \\(B\\) after getting query from \\(F\\) Since the query message from \\(A\\) is received by \\(B\\) who initiated the algorithm, hence \\(B\\) sends out a reply The reply path looks like this: Since \\(B\\) received all replies and \\(num_B = 0\\) the node B declares deadlock Performance Analysis # For every deadlock detection, the algorithm exchanges \\(e\\) query messages and \\(e\\) reply messages, where \\(e = n(n - 1)\\) \\(e\\) is the number of edges, \\(n\\) is the number of processes Deadlock Resolution # Deadlock reslutions involves breaking exisiting wait-for dependencies between processes Rolling back one or more deadlocked processes Assigning resources of rolled back processes to blocked processes Blocked processes resume execution When a wait-for dependency is broken Corresponding information should be immediately cleaned from the system Otherwise may result in detection of phantom deadlocks Agreement Problem # Agreement among processes in a distributed system Processes need to exchange information to negotiate with one another and eventually reach a common understanding or agreement, before taking actions Failure Models : - Among the \\(n\\) processes in the system, at most \\(f\\) processes can be faulty - Faulty process can behave in any manner allowed by the failure model assumed - Various processor failure models - From recorded lecture Synchronous/Asynchronous communication : - If a failure prone processes chooses to send a message to process \\(P_i\\) but fails, then \\(P_i\\) cannot detect the non arrival of the messages in an asynchronous system - Scenario is indistinguishable from the scenario in which the message takes a very long time to travel - Impossible to reach a consensus in an asynchronous system. - In a synchronous system, a message that has not been sent can be recognized by the intended recipient, at the end of the round - The intended recipient can deal with the non arrival of the expected message by assuming the arrival of a message containing some default data, and then proceeding with the next round of the algorithm Network Connectivity : - System has full connectivity, i.e., each process can communicate with any other by direct message passing Sender Identification : - A process receiving a message always knows identity of sender process - Also true for malicious senders Channel Reliability : - Channels are reliable - Only the processes may fail (Under one of the various failure models) Agreement Variable : - May be boolean or multi valued - Need not be an integer Non authenticated messages : - With such messages, when a faulty process relays a message to other processes - It can forge the message and claim that it was received from another process - It can also tamper with the contents of a received messages before relaying it - Receiver cannot verify its authenticity - Unauthenticated message is also called oral message or unsigned message Byzantine Agreement Problem # - Requires a designated process, called the source process , with an initial value, to reach agreement with the other processes about its initial value, subject to the following conditions: - Agreement: All non faulty process must agree on the same value - Validity: If the source process is non-faulty, then the agreed upon value by all the non-faulty processes must be the same as the initial value of the source - Termination: Each non faulty process must eventually decide on a value. - If the source process is faulty, then the correct processes can agree upon any default value - Irrelevant what the faulty processes agree upon - or whether they terminate and agree upon anything at all Consensus Problem # It is initiated by all processes Agreement: All non faulty processes must agree on the same (single) value Validity: If all the non-faulty processes have the same initial value, then the agreed upon value by all the non-faulty processes must be that same value If non-faulty processes broadcast different initial values, then these processes should decide upon a common value Termination: Each non-faulty process must eventually decide on a value Interactive Consistency Problem # Initiated by all processes Agreement: All non faulty processes must agree on the same array of values Validity: If process \\(P_i\\) is non faulty and its initial value is \\(V_i\\) then all non faulty processes agree on \\(V_i\\) as the \\(i^{th}\\) element of the array \\(A\\) If \\(P_j\\) is faulty, then the non faulty processes can agree on any value for \\(A[j]\\) Termination: each non faulty process must agree on some value for \\(A[]\\) Upper Bound on Byzantine Processes # In a system of n processes, this problem can be solved in a synchronous system only fo the number of Byzantine processes f is such that \\(f \\le floor((n - 1)/3)\\) Tags: !DistributedComputingIndex","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#week-8","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 16/Oct/2021","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#topics-covered","text":"Chandy Misra Haas Algorithm for the OR model Steps Examples Performance Analysis Deadlock Resolution","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#chandy-misra-haas-algorithm-for-the-or-model","text":"2 types of messages are used: \\(query(i, j, k)\\) \\(reply(i, j, k)\\) Denote that they belong to a deadlock detection initiated by \\(P_i\\) and are being sent from \\(P_j\\) to \\(P_k\\) A blocked process initiates deadlock detection by sending query messages to all processes in its dependent set \\(DEPEN\\) Local variable \\(num_k(i)\\) = number of query messages sent \\(P_k\\) maintains a boolean variable \\(wait_k(i)\\) \\(wait_k(i)\\) denotes that \\(P_k\\) has been continuously blocked since it received the last engaging query \\(P_i\\)","title":"Chandy Misra Haas Algorithm for the OR model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#steps","text":"","title":"Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#examples","text":"\\(B\\) sends first message \\(C\\) sends query message after getting query from \\(B\\) \\(F\\) sends query message after getting query from \\(B\\) \\(E\\) sends query message after getting query from \\(C\\) The message sent by \\(E\\) is non engaging for \\(F\\) so \\(F\\) sends a \\(reply\\) to \\(E\\) \\(D\\) sends a query to \\(E\\) after getting query from \\(C\\) The message sent by \\(D\\) is non engaging for \\(E\\) so \\(E\\) sends a \\(reply\\) to \\(D\\) \\(A\\) sends a query to \\(B\\) after getting query from \\(F\\) Since the query message from \\(A\\) is received by \\(B\\) who initiated the algorithm, hence \\(B\\) sends out a reply The reply path looks like this: Since \\(B\\) received all replies and \\(num_B = 0\\) the node B declares deadlock","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#performance-analysis","text":"For every deadlock detection, the algorithm exchanges \\(e\\) query messages and \\(e\\) reply messages, where \\(e = n(n - 1)\\) \\(e\\) is the number of edges, \\(n\\) is the number of processes","title":"Performance Analysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#deadlock-resolution","text":"Deadlock reslutions involves breaking exisiting wait-for dependencies between processes Rolling back one or more deadlocked processes Assigning resources of rolled back processes to blocked processes Blocked processes resume execution When a wait-for dependency is broken Corresponding information should be immediately cleaned from the system Otherwise may result in detection of phantom deadlocks","title":"Deadlock Resolution"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#agreement-problem","text":"Agreement among processes in a distributed system Processes need to exchange information to negotiate with one another and eventually reach a common understanding or agreement, before taking actions Failure Models : - Among the \\(n\\) processes in the system, at most \\(f\\) processes can be faulty - Faulty process can behave in any manner allowed by the failure model assumed - Various processor failure models - From recorded lecture Synchronous/Asynchronous communication : - If a failure prone processes chooses to send a message to process \\(P_i\\) but fails, then \\(P_i\\) cannot detect the non arrival of the messages in an asynchronous system - Scenario is indistinguishable from the scenario in which the message takes a very long time to travel - Impossible to reach a consensus in an asynchronous system. - In a synchronous system, a message that has not been sent can be recognized by the intended recipient, at the end of the round - The intended recipient can deal with the non arrival of the expected message by assuming the arrival of a message containing some default data, and then proceeding with the next round of the algorithm Network Connectivity : - System has full connectivity, i.e., each process can communicate with any other by direct message passing Sender Identification : - A process receiving a message always knows identity of sender process - Also true for malicious senders Channel Reliability : - Channels are reliable - Only the processes may fail (Under one of the various failure models) Agreement Variable : - May be boolean or multi valued - Need not be an integer Non authenticated messages : - With such messages, when a faulty process relays a message to other processes - It can forge the message and claim that it was received from another process - It can also tamper with the contents of a received messages before relaying it - Receiver cannot verify its authenticity - Unauthenticated message is also called oral message or unsigned message","title":"Agreement Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#byzantine-agreement-problem","text":"- Requires a designated process, called the source process , with an initial value, to reach agreement with the other processes about its initial value, subject to the following conditions: - Agreement: All non faulty process must agree on the same value - Validity: If the source process is non-faulty, then the agreed upon value by all the non-faulty processes must be the same as the initial value of the source - Termination: Each non faulty process must eventually decide on a value. - If the source process is faulty, then the correct processes can agree upon any default value - Irrelevant what the faulty processes agree upon - or whether they terminate and agree upon anything at all","title":"Byzantine Agreement Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#consensus-problem","text":"It is initiated by all processes Agreement: All non faulty processes must agree on the same (single) value Validity: If all the non-faulty processes have the same initial value, then the agreed upon value by all the non-faulty processes must be that same value If non-faulty processes broadcast different initial values, then these processes should decide upon a common value Termination: Each non-faulty process must eventually decide on a value","title":"Consensus Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#interactive-consistency-problem","text":"Initiated by all processes Agreement: All non faulty processes must agree on the same array of values Validity: If process \\(P_i\\) is non faulty and its initial value is \\(V_i\\) then all non faulty processes agree on \\(V_i\\) as the \\(i^{th}\\) element of the array \\(A\\) If \\(P_j\\) is faulty, then the non faulty processes can agree on any value for \\(A[j]\\) Termination: each non faulty process must agree on some value for \\(A[]\\)","title":"Interactive Consistency Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week8DC.html#upper-bound-on-byzantine-processes","text":"In a system of n processes, this problem can be solved in a synchronous system only fo the number of Byzantine processes f is such that \\(f \\le floor((n - 1)/3)\\) Tags: !DistributedComputingIndex","title":"Upper Bound on Byzantine Processes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html","text":"Week 9 # Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 24/Oct/2021 Byzantine Agreement Problem # Assumptions made # \\(n = 4\\) \\(f = 1\\) Byzantine Agreement Tree Algorithm: Recursive Formulae # Peer to Peer Network # Well known P2P networks # Napster Gnutella Freenet Pastry Chord CAN Introduction to P2P networks # P2p networks allow the location of arbitrary data objects impose a low cost for scalability, and for entry into an exit from the network Ongoing entry and exit of various nodes and dynamic insertion and deletion of objects is termed as churn Impact of churn should be as transparent as possible Data Indexing and Overlays # Centralized Indexing: # Use of one or a few central servers to store references to the data on many peers Napster uses centralized indexing Local Indexing # Requires each peer to index only the local data objects Remote objects need to be searched for Used in unstructured overlays Gnutella uses local indexing Disctributed Indexing - Involves the indexes to the objects at various peers being scattered across other peers throughoput the P2P network - Disctributed indexing is the most challenging of the indexing schemes - Many novel mechanisms have been proposed, most notably the disctributed hash table (DHT) Structured Overlays # Unstructured Overlays # Chord Distributed Hash Table: Overview # Simple Lookup # Tags: !DistributedComputingIndex","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#week-9","text":"Lecturer : Barsha Mitra , BITS Pilani, Hyderabad Campus Date : 24/Oct/2021","title":"Week 9"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#byzantine-agreement-problem","text":"","title":"Byzantine Agreement Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#assumptions-made","text":"\\(n = 4\\) \\(f = 1\\)","title":"Assumptions made"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#byzantine-agreement-tree-algorithm-recursive-formulae","text":"","title":"Byzantine Agreement Tree Algorithm: Recursive Formulae"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#peer-to-peer-network","text":"","title":"Peer to Peer Network"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#well-known-p2p-networks","text":"Napster Gnutella Freenet Pastry Chord CAN","title":"Well known P2P networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#introduction-to-p2p-networks","text":"P2p networks allow the location of arbitrary data objects impose a low cost for scalability, and for entry into an exit from the network Ongoing entry and exit of various nodes and dynamic insertion and deletion of objects is termed as churn Impact of churn should be as transparent as possible","title":"Introduction to P2P networks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#data-indexing-and-overlays","text":"","title":"Data Indexing and Overlays"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#centralized-indexing","text":"Use of one or a few central servers to store references to the data on many peers Napster uses centralized indexing","title":"Centralized Indexing:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#local-indexing","text":"Requires each peer to index only the local data objects Remote objects need to be searched for Used in unstructured overlays Gnutella uses local indexing Disctributed Indexing - Involves the indexes to the objects at various peers being scattered across other peers throughoput the P2P network - Disctributed indexing is the most challenging of the indexing schemes - Many novel mechanisms have been proposed, most notably the disctributed hash table (DHT)","title":"Local Indexing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#structured-overlays","text":"","title":"Structured Overlays"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#unstructured-overlays","text":"","title":"Unstructured Overlays"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#chord-distributed-hash-table-overview","text":"","title":"Chord Distributed Hash Table: Overview"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Distributed%20Computing/Week9DC.html#simple-lookup","text":"Tags: !DistributedComputingIndex","title":"Simple Lookup"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html","text":"Mathematical Foundations for Data Science Index # This is the page for all Mathematical Foundations for Data Science Content. Note: Since the live lectures for this subject was interactive and less theoretical, the notes takes as a result is quite minimal Assignments # Assignment 1 MFDSAssignment1 Assignment 2 MFDSAssignment2 Question Papers # Mid Sem Paper MFDS End Sem Paper MFDS Course Content # Part 1 (Matrices / Vectors / Linear Programming) # Octave Cheat Sheet: OctaveCheatSheet Week 1 Lecture Notes: Week1MFDS Week 2 Lecture Notes: Week2MFDS Gaussian Elimination Code and Analysis in Python: GaussianEliminationPython Week 3 Lecture Notes: Week3MFDS Week 4 Lecture Notes: Week4MFDS Week 5 Lecture Notes: Week5MFDS Week 6 Lecture Notes: Week6MFDS Lecture Notes: Week6MFDS2 Week 7 Lecture Notes: Week7MFDS Week 8 Lecture Notes: Week8MFDS Lecture Notes: Week8MFDS2 Part 2 (Calculus / Induction / Counting) # Part 2 of this subject was problem heavy, so notes are captured in the following pdf . Problems are not completely solved since this is only the notes captured during the live lecture itself. Note Summary: EmbeddedNotesMFDS Tags: !Semester1Index","title":"Mathematical Foundations for Data Science Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#mathematical-foundations-for-data-science-index","text":"This is the page for all Mathematical Foundations for Data Science Content. Note: Since the live lectures for this subject was interactive and less theoretical, the notes takes as a result is quite minimal","title":"Mathematical Foundations for Data Science Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#assignments","text":"Assignment 1 MFDSAssignment1 Assignment 2 MFDSAssignment2","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#question-papers","text":"Mid Sem Paper MFDS End Sem Paper MFDS","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#course-content","text":"","title":"Course Content"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#part-1-matrices--vectors--linear-programming","text":"Octave Cheat Sheet: OctaveCheatSheet Week 1 Lecture Notes: Week1MFDS Week 2 Lecture Notes: Week2MFDS Gaussian Elimination Code and Analysis in Python: GaussianEliminationPython Week 3 Lecture Notes: Week3MFDS Week 4 Lecture Notes: Week4MFDS Week 5 Lecture Notes: Week5MFDS Week 6 Lecture Notes: Week6MFDS Lecture Notes: Week6MFDS2 Week 7 Lecture Notes: Week7MFDS Week 8 Lecture Notes: Week8MFDS Lecture Notes: Week8MFDS2","title":"Part 1 (Matrices / Vectors / Linear Programming)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/%21MathematicalFoundationsIndex.html#part-2-calculus--induction--counting","text":"Part 2 of this subject was problem heavy, so notes are captured in the following pdf . Problems are not completely solved since this is only the notes captured during the live lecture itself. Note Summary: EmbeddedNotesMFDS Tags: !Semester1Index","title":"Part 2 (Calculus / Induction / Counting)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/EmbeddedNotesMFDS.html","text":"Mathematical Foundations For Datascience Notes # Part 1 # Part 2 # Part 2 of this subject was problem heavy, so notes are captured in the following pdf . Problems are not completely solved since this is only the notes captured during the live lecture itself.","title":"Mathematical Foundations For Datascience Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/EmbeddedNotesMFDS.html#mathematical-foundations-for-datascience-notes","text":"","title":"Mathematical Foundations For Datascience Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/EmbeddedNotesMFDS.html#part-1","text":"","title":"Part 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/EmbeddedNotesMFDS.html#part-2","text":"Part 2 of this subject was problem heavy, so notes are captured in the following pdf . Problems are not completely solved since this is only the notes captured during the live lecture itself.","title":"Part 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html","text":"End Sem MFDS Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # tags: !MathematicalFoundationsIndex QuestionPapers","title":"End Sem MFDS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#end-sem-mfds-paper","text":"","title":"End Sem MFDS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/End%20Sem%20Paper%20MFDS.html#question-6","text":"tags: !MathematicalFoundationsIndex QuestionPapers","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/GaussianEliminationPython.html","text":"Gaussian Elimination Code # Code in Python # import copy # This class is to hold matrix data class Matrix : def __init__ ( self , row : int , col : int , listOfList : list ): self . n : int = row self . m : int = col self . data : list = listOfList # This class has functions for different operations on a matrix class MatrixOperations : def print ( self , message : str , matrix : Matrix ): '''Print Matrix Input: message (A string to print before the matrix), matrix (A Matrix Object) Output: A multi line string of the Matrix data ''' print () print ( message ) for row in matrix . data : line : str = \"\" for element in row : line += str ( element ) + \" \\t \" print ( line ) print () def REF ( self , matrix : Matrix ): '''Calculate the Row Echelon Form for a Matrix Input: matrix (A Matrix Object) Output: REF (A Matrix Object holding the REF form for matrix) ''' REF : Matrix = Matrix ( matrix . n , matrix . m , matrix . data ) for i in range ( REF . n ): # Pivot the row when the pivot element is 0 if ( REF . data [ i ][ i ] == 0 ): for j in range ( i , REF . n ): # Find the row with greatest value below pivot element and swap with that row if abs ( REF . data [ j ][ i ]) > abs ( REF . data [ i ][ i ]): REF . data [ j ], REF . data [ i ] = REF . data [ i ], REF . data [ j ] for j in range ( i + 1 , REF . n ): # Doing this once so that I minimize the number of divisions ratio : float = REF . data [ j ][ i ] / REF . data [ i ][ i ] # Assign 0 hence reducing multiplication and addition by one for elements under the pivot REF . data [ j ][ i ] = 0 for k in range ( i + 1 , REF . m ): REF . data [ j ][ k ] = REF . data [ j ][ k ] - ratio * REF . data [ i ][ k ] return REF def augment ( self , matrixA : Matrix , matrixB : Matrix ): '''Augments two matrices Input: matrixA (A Matrix Object), matrixB (B Matrix Object) Output: matrix (A Matrix Object holding the REF form for matrix) ''' listOfList : list = copy . deepcopy ( matrixA . data ) for i in range ( matrixA . n ): listOfList [ i ] . extend ( matrixB . data [ i ]) matrix : Matrix = Matrix ( matrixA . n , matrixA . m + matrixB . m , listOfList ) return matrix def rank ( self , matrix : Matrix , isREF : bool = False ): '''Calculate rank of matrix Input: matrix (Matrix Object), isREF (A boolean flag that denotes if the given matrix is in REF form) Output: rank (Rank of matrix) ''' if not ref : matrix = self . REF ( matrix ) rank : int = matrix . n while ( rank - 1 >= 0 ): flag : bool = False for element in matrix . data [ rank - 1 ]: print ( element ) if element == 0 : continue flag = True if flag : break rank -= 1 return rank def backSubstitute ( self , matrix : Matrix , isREF : bool = False ): '''Solve set of linear equations given as an augmented matrix Input: matrix (Augmented Matrix Object), isREF (A boolean flag that denotes if the given matrix is in REF form) Output: x (Matrix object of the result x) ''' if not isREF : matrix = self . REF ( matrix ) n = matrix . n data = matrix . data x : list = [ 0 ] * n x [ n - 1 ] = data [ n - 1 ][ n ] / data [ n - 1 ][ n - 1 ] for i in range ( n - 2 , - 1 , - 1 ): x [ i ] = matrix . data [ i ][ n ] for j in range ( i + 1 , n ): x [ i ] = x [ i ] - data [ i ][ j ] * x [ j ] x [ i ] = x [ i ] / data [ i ][ i ] return Matrix ( 1 , n , [ x ]) def main (): print ( '=======================================================================' ) print ( 'This commandline utility accepts two matrices is Ax = b format' ) print ( 'Enter details for A: ' ) print ( 'Enter number of rows: ' ) n : int = int ( input ()) print ( 'Enter number of columns: ' ) m : int = int ( input ()) listOfList : list = [] i : int = 0 while ( i < n ): print ( 'Enter the ' + str ( i ) + ' row elements' ) row : list = list ( map ( float , input () . split ())) if len ( row ) > m : print ( 'number of elements in row greater than column count hence truncating' ) del row [ m :] print ( row ) listOfList . append ( row ) i += 1 matrixA : Matrix = Matrix ( n , m , listOfList ) print ( '=======================================================================' ) print ( 'Enter details for b: ' ) listOfList : list = [] i : int = 0 while ( i < n ): print ( 'Enter the ' + str ( i ) + ' row element' ) row : list = list ( map ( float , input () . split ())) if len ( row ) > 1 : print ( 'number of elements in row greater than 1 truncating' ) del row [ 1 :] print ( row ) listOfList . append ( row ) i += 1 matrixB : Matrix = Matrix ( n , 1 , listOfList ) print ( '=======================================================================' ) operation : MatrixOperations = MatrixOperations () operation . print ( 'Input Matrix A is:' , matrixA ) operation . print ( 'Input Matrix B is:' , matrixB ) matrix : Matrix = operation . augment ( matrixA , matrixB ) operation . print ( 'Augmented Matrix is:' , matrix ) matrix : Matrix = operation . REF ( matrix ) operation . print ( 'REF for Matrix is:' , matrix ) x : Matrix = operation . backSubstitute ( matrix , True ) operation . print ( 'Solution Matrix x is:' , x ) print ( '=======================================================================' ) if __name__ == '__main__' : main () Operations Count # Look at the function for calculating the REF for a matrix: def REF ( self , matrix : Matrix ): '''Calculate the Row Echelon Form for a Matrix Input: matrix (A Matrix Object) Output: REF (A Matrix Object holding the REF form for matrix) ''' REF : Matrix = Matrix ( matrix . n , matrix . m , matrix . data ) for i in range ( REF . n ): # Pivot the row when the pivot element is 0 if ( REF . data [ i ][ i ] == 0 ): for j in range ( i , REF . n ): # Find the row with greatest value below pivot element and swap with that row if abs ( REF . data [ j ][ i ]) > abs ( REF . data [ i ][ i ]): REF . data [ j ], REF . data [ i ] = REF . data [ i ], REF . data [ j ] # Row operations starts here for j in range ( i + 1 , REF . n ): # Doing this once so that I minimize the number of divisions ratio : float = REF . data [ j ][ i ] / REF . data [ i ][ i ] # Assign 0 hence reducing multiplication and addition by one for elements under the pivot REF . data [ j ][ i ] = 0 for k in range ( i + 1 , REF . m ): REF . data [ j ][ k ] = REF . data [ j ][ k ] - ratio * REF . data [ i ][ k ] return REF For the first Pivot element: \\[Total\\ number\\ of\\ Multiplications= (n-1)(n-1+1)\\] \\[Total\\ number\\ of\\ Additions= (n-1)(n-1+1)\\] Final counts as seen in REF function: \\[Total\\ number\\ of\\ Multiplications = \\sum_{k=1}^{n-1} (n-k)(n-k+1) = \\mathcal{O}(n^3)\\] \\[Total\\ number\\ of\\ Additions = \\sum_{k=1}^{n-1} (n-k)(n-k+1) = \\mathcal{O}(n^3)\\] \\[Total\\ number\\ of\\ Divisions = (n-1) + (n-2) + \\dots + 1 = \\sum_{k=1}^{n-1}(n-k) = \\mathcal{O}(n^2)\\] Tags: !MathematicalFoundationsIndex Week2MFDS","title":"Gaussian Elimination Code"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/GaussianEliminationPython.html#gaussian-elimination-code","text":"","title":"Gaussian Elimination Code"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/GaussianEliminationPython.html#code-in-python","text":"import copy # This class is to hold matrix data class Matrix : def __init__ ( self , row : int , col : int , listOfList : list ): self . n : int = row self . m : int = col self . data : list = listOfList # This class has functions for different operations on a matrix class MatrixOperations : def print ( self , message : str , matrix : Matrix ): '''Print Matrix Input: message (A string to print before the matrix), matrix (A Matrix Object) Output: A multi line string of the Matrix data ''' print () print ( message ) for row in matrix . data : line : str = \"\" for element in row : line += str ( element ) + \" \\t \" print ( line ) print () def REF ( self , matrix : Matrix ): '''Calculate the Row Echelon Form for a Matrix Input: matrix (A Matrix Object) Output: REF (A Matrix Object holding the REF form for matrix) ''' REF : Matrix = Matrix ( matrix . n , matrix . m , matrix . data ) for i in range ( REF . n ): # Pivot the row when the pivot element is 0 if ( REF . data [ i ][ i ] == 0 ): for j in range ( i , REF . n ): # Find the row with greatest value below pivot element and swap with that row if abs ( REF . data [ j ][ i ]) > abs ( REF . data [ i ][ i ]): REF . data [ j ], REF . data [ i ] = REF . data [ i ], REF . data [ j ] for j in range ( i + 1 , REF . n ): # Doing this once so that I minimize the number of divisions ratio : float = REF . data [ j ][ i ] / REF . data [ i ][ i ] # Assign 0 hence reducing multiplication and addition by one for elements under the pivot REF . data [ j ][ i ] = 0 for k in range ( i + 1 , REF . m ): REF . data [ j ][ k ] = REF . data [ j ][ k ] - ratio * REF . data [ i ][ k ] return REF def augment ( self , matrixA : Matrix , matrixB : Matrix ): '''Augments two matrices Input: matrixA (A Matrix Object), matrixB (B Matrix Object) Output: matrix (A Matrix Object holding the REF form for matrix) ''' listOfList : list = copy . deepcopy ( matrixA . data ) for i in range ( matrixA . n ): listOfList [ i ] . extend ( matrixB . data [ i ]) matrix : Matrix = Matrix ( matrixA . n , matrixA . m + matrixB . m , listOfList ) return matrix def rank ( self , matrix : Matrix , isREF : bool = False ): '''Calculate rank of matrix Input: matrix (Matrix Object), isREF (A boolean flag that denotes if the given matrix is in REF form) Output: rank (Rank of matrix) ''' if not ref : matrix = self . REF ( matrix ) rank : int = matrix . n while ( rank - 1 >= 0 ): flag : bool = False for element in matrix . data [ rank - 1 ]: print ( element ) if element == 0 : continue flag = True if flag : break rank -= 1 return rank def backSubstitute ( self , matrix : Matrix , isREF : bool = False ): '''Solve set of linear equations given as an augmented matrix Input: matrix (Augmented Matrix Object), isREF (A boolean flag that denotes if the given matrix is in REF form) Output: x (Matrix object of the result x) ''' if not isREF : matrix = self . REF ( matrix ) n = matrix . n data = matrix . data x : list = [ 0 ] * n x [ n - 1 ] = data [ n - 1 ][ n ] / data [ n - 1 ][ n - 1 ] for i in range ( n - 2 , - 1 , - 1 ): x [ i ] = matrix . data [ i ][ n ] for j in range ( i + 1 , n ): x [ i ] = x [ i ] - data [ i ][ j ] * x [ j ] x [ i ] = x [ i ] / data [ i ][ i ] return Matrix ( 1 , n , [ x ]) def main (): print ( '=======================================================================' ) print ( 'This commandline utility accepts two matrices is Ax = b format' ) print ( 'Enter details for A: ' ) print ( 'Enter number of rows: ' ) n : int = int ( input ()) print ( 'Enter number of columns: ' ) m : int = int ( input ()) listOfList : list = [] i : int = 0 while ( i < n ): print ( 'Enter the ' + str ( i ) + ' row elements' ) row : list = list ( map ( float , input () . split ())) if len ( row ) > m : print ( 'number of elements in row greater than column count hence truncating' ) del row [ m :] print ( row ) listOfList . append ( row ) i += 1 matrixA : Matrix = Matrix ( n , m , listOfList ) print ( '=======================================================================' ) print ( 'Enter details for b: ' ) listOfList : list = [] i : int = 0 while ( i < n ): print ( 'Enter the ' + str ( i ) + ' row element' ) row : list = list ( map ( float , input () . split ())) if len ( row ) > 1 : print ( 'number of elements in row greater than 1 truncating' ) del row [ 1 :] print ( row ) listOfList . append ( row ) i += 1 matrixB : Matrix = Matrix ( n , 1 , listOfList ) print ( '=======================================================================' ) operation : MatrixOperations = MatrixOperations () operation . print ( 'Input Matrix A is:' , matrixA ) operation . print ( 'Input Matrix B is:' , matrixB ) matrix : Matrix = operation . augment ( matrixA , matrixB ) operation . print ( 'Augmented Matrix is:' , matrix ) matrix : Matrix = operation . REF ( matrix ) operation . print ( 'REF for Matrix is:' , matrix ) x : Matrix = operation . backSubstitute ( matrix , True ) operation . print ( 'Solution Matrix x is:' , x ) print ( '=======================================================================' ) if __name__ == '__main__' : main ()","title":"Code in Python"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/GaussianEliminationPython.html#operations-count","text":"Look at the function for calculating the REF for a matrix: def REF ( self , matrix : Matrix ): '''Calculate the Row Echelon Form for a Matrix Input: matrix (A Matrix Object) Output: REF (A Matrix Object holding the REF form for matrix) ''' REF : Matrix = Matrix ( matrix . n , matrix . m , matrix . data ) for i in range ( REF . n ): # Pivot the row when the pivot element is 0 if ( REF . data [ i ][ i ] == 0 ): for j in range ( i , REF . n ): # Find the row with greatest value below pivot element and swap with that row if abs ( REF . data [ j ][ i ]) > abs ( REF . data [ i ][ i ]): REF . data [ j ], REF . data [ i ] = REF . data [ i ], REF . data [ j ] # Row operations starts here for j in range ( i + 1 , REF . n ): # Doing this once so that I minimize the number of divisions ratio : float = REF . data [ j ][ i ] / REF . data [ i ][ i ] # Assign 0 hence reducing multiplication and addition by one for elements under the pivot REF . data [ j ][ i ] = 0 for k in range ( i + 1 , REF . m ): REF . data [ j ][ k ] = REF . data [ j ][ k ] - ratio * REF . data [ i ][ k ] return REF For the first Pivot element: \\[Total\\ number\\ of\\ Multiplications= (n-1)(n-1+1)\\] \\[Total\\ number\\ of\\ Additions= (n-1)(n-1+1)\\] Final counts as seen in REF function: \\[Total\\ number\\ of\\ Multiplications = \\sum_{k=1}^{n-1} (n-k)(n-k+1) = \\mathcal{O}(n^3)\\] \\[Total\\ number\\ of\\ Additions = \\sum_{k=1}^{n-1} (n-k)(n-k+1) = \\mathcal{O}(n^3)\\] \\[Total\\ number\\ of\\ Divisions = (n-1) + (n-2) + \\dots + 1 = \\sum_{k=1}^{n-1}(n-k) = \\mathcal{O}(n^2)\\] Tags: !MathematicalFoundationsIndex Week2MFDS","title":"Operations Count"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment1.html","text":"MFDS Assignment 1 # Tasks # Q1) Write code to implement the Gaussian elimination with partial pivoting for the system \\(An\u00d7nx = b\\) . Include a statement in the code to indicate the swapping of rows. Using the code draw the \\(log\u2212log\\) plot of \\(n\\) versus the time taken for forward elimination and backward substitution (as separate graphs) by taking values of in-between \\(1000\\) and \\(10000\\) in steps of \\(1000\\) . Determine the time taken for a single computation in your machine (by averaging over \\(1000\\) runs) and compare the time taken with the actual time derived in the class. This should give the time taken for the partial pivoting. solve the system \\(A5\u00d75x = b\\) , with random entries and display your results. Q2) Gauss Jordan method To find the inverse of a non-singular matrix \\(A\\) by Gauss Jordan method, one starts with the augmented matrix \\([A|I]\\) where \\(I\\) is the identity matrix of the same size and performs elementary row operations on \\(A\\) so that \\(A\\) is reduced to \\(I\\) . Performing the same elementary operations on \\(I\\) would give \\(A\u22121\\) . Assuming that \\(Am\u00d7m\\) is an invertible matrix, write a code to find the inverse of \\(A\\) using the Gauss Jordan method. Using the code, find the inverse of a \\(6 x 6\\) random matrix which is non-singular. My Submission # The actual Assignment can be found here The code along with the metrics collected can be found here tags: !MathematicalFoundationsIndex Assignments","title":"MFDS Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment1.html#mfds-assignment-1","text":"","title":"MFDS Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment1.html#tasks","text":"Q1) Write code to implement the Gaussian elimination with partial pivoting for the system \\(An\u00d7nx = b\\) . Include a statement in the code to indicate the swapping of rows. Using the code draw the \\(log\u2212log\\) plot of \\(n\\) versus the time taken for forward elimination and backward substitution (as separate graphs) by taking values of in-between \\(1000\\) and \\(10000\\) in steps of \\(1000\\) . Determine the time taken for a single computation in your machine (by averaging over \\(1000\\) runs) and compare the time taken with the actual time derived in the class. This should give the time taken for the partial pivoting. solve the system \\(A5\u00d75x = b\\) , with random entries and display your results. Q2) Gauss Jordan method To find the inverse of a non-singular matrix \\(A\\) by Gauss Jordan method, one starts with the augmented matrix \\([A|I]\\) where \\(I\\) is the identity matrix of the same size and performs elementary row operations on \\(A\\) so that \\(A\\) is reduced to \\(I\\) . Performing the same elementary operations on \\(I\\) would give \\(A\u22121\\) . Assuming that \\(Am\u00d7m\\) is an invertible matrix, write a code to find the inverse of \\(A\\) using the Gauss Jordan method. Using the code, find the inverse of a \\(6 x 6\\) random matrix which is non-singular.","title":"Tasks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment1.html#my-submission","text":"The actual Assignment can be found here The code along with the metrics collected can be found here tags: !MathematicalFoundationsIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment2.html","text":"MFDS Assignment 2 # Tasks # Q1) Write a code to minimize \\(f(x, y) = (x \u2212 y)^2 + 1 \u2212 x\\) using the Gradient descent method. Take a suitable initial value and do the iterations till the tolerance is \\(0.01\\) . Please attach the code along with the final output. Q2) Find the least natural number that divides \\(11^{n+1} + 12^{2n\u22121}\\) by examining the values of this expression for small values of \\(n \\in N\\) , and prove the formula you conjectured. My Submission # The actual Assignment can be found here The code along with the metrics collected can be found here tags: !MathematicalFoundationsIndex Assignments","title":"MFDS Assignment 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment2.html#mfds-assignment-2","text":"","title":"MFDS Assignment 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment2.html#tasks","text":"Q1) Write a code to minimize \\(f(x, y) = (x \u2212 y)^2 + 1 \u2212 x\\) using the Gradient descent method. Take a suitable initial value and do the iterations till the tolerance is \\(0.01\\) . Please attach the code along with the final output. Q2) Find the least natural number that divides \\(11^{n+1} + 12^{2n\u22121}\\) by examining the values of this expression for small values of \\(n \\in N\\) , and prove the formula you conjectured.","title":"Tasks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/MFDSAssignment2.html#my-submission","text":"The actual Assignment can be found here The code along with the metrics collected can be found here tags: !MathematicalFoundationsIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html","text":"Mid Sem MFDS Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !MathematicalFoundationsIndex QuestionPapers","title":"Mid Sem MFDS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html#mid-sem-mfds-paper","text":"","title":"Mid Sem MFDS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Mid%20Sem%20Paper%20MFDS.html#question-4","text":"tags: !MathematicalFoundationsIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html","text":"Octave Cheat Sheet # Define a 3x3 matrix # A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] Matrix properties # A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] det ( A ) Norms # A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] norm ( A , 1 ) norm ( A , 2 ) norm ( A , 'fro' ) norm ( A , 'inf' ) Inverse of a matrix # A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] inv ( A ) Determinant of a matrix # A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] det ( A ) LU decompose a matrix # # LU decompose withoput pivot function [L, U] = lu_nopivot ( A ) n = size ( A , 1 ); % Obtain number of rows (should equal number of columns) L = eye ( n ); % Start L off as identity and populate the lower triangular half slowly for k = 1 : n % For each row k, access columns from k+1 to the end and divide by % the diagonal coefficient at A(k ,k) L ( k + 1 : n , k ) = A ( k + 1 : n , k ) / A ( k , k ); % For each row k+1 to the end, perform Gaussian elimination % In the end, A will contain U for l = k + 1 : n A ( l , :) = A ( l , :) - L ( l , k ) * A ( k , :); end end U = A ; end # LU decompose [ L , U , I ] = lu ( A ) Solve linear system of equations # A = [ 2 , 1 , - 2 ; 1 , - 1 , - 1 ; 1 , 1 , 3 ] B = [ 3 ; 0 ; 12 ] A \\ B %%%%%%%%%%%%%%%%%%%%% % ans = % % % % 3.5000 % % 1.0000 % % 2.5000 % %%%%%%%%%%%%%%%%%%%%%","title":"Octave Cheat Sheet"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#octave-cheat-sheet","text":"","title":"Octave Cheat Sheet"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#define-a-3x3-matrix","text":"A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ]","title":"Define a 3x3 matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#matrix-properties","text":"A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] det ( A )","title":"Matrix properties"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#norms","text":"A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] norm ( A , 1 ) norm ( A , 2 ) norm ( A , 'fro' ) norm ( A , 'inf' )","title":"Norms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#inverse-of-a-matrix","text":"A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] inv ( A )","title":"Inverse of a matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#determinant-of-a-matrix","text":"A = [ - 1 , 1 , 2 ; 3 , - 1 , 1 ; - 1 , 3 , 4 ] det ( A )","title":"Determinant of a matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#lu-decompose-a-matrix","text":"# LU decompose withoput pivot function [L, U] = lu_nopivot ( A ) n = size ( A , 1 ); % Obtain number of rows (should equal number of columns) L = eye ( n ); % Start L off as identity and populate the lower triangular half slowly for k = 1 : n % For each row k, access columns from k+1 to the end and divide by % the diagonal coefficient at A(k ,k) L ( k + 1 : n , k ) = A ( k + 1 : n , k ) / A ( k , k ); % For each row k+1 to the end, perform Gaussian elimination % In the end, A will contain U for l = k + 1 : n A ( l , :) = A ( l , :) - L ( l , k ) * A ( k , :); end end U = A ; end # LU decompose [ L , U , I ] = lu ( A )","title":"LU decompose a matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/OctaveCheatSheet.html#solve-linear-system-of-equations","text":"A = [ 2 , 1 , - 2 ; 1 , - 1 , - 1 ; 1 , 1 , 3 ] B = [ 3 ; 0 ; 12 ] A \\ B %%%%%%%%%%%%%%%%%%%%% % ans = % % % % 3.5000 % % 1.0000 % % 2.5000 % %%%%%%%%%%%%%%%%%%%%%","title":"Solve linear system of equations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week1MFDS.html","text":"Week 1 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 25/Jul/2021 Topics Covered # Matrices and their types REF and RREF Rank, its computation and properties Determinent, it's computation and properties Consistency and inconsistency of linear systems Modelling using linear equations Vector and matrix norms Matrices and their types # A rectangular array of numbers or functions enclosed in brrackets are considered as matrices (Matrix for singular). All of the following are valid matrices: \\[A = \\begin{bmatrix}a & b & c & d\\\\e & f & g & h\\end{bmatrix},B = \\begin{bmatrix}a & b & c\\\\d & e & f\\\\g & h & i\\end{bmatrix},C = \\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix},D = \\begin{bmatrix}a & b & c\\end{bmatrix}\\] Each element within the matrix ( \\(a\\) , \\(b\\) , \\(c\\) etc) are called entities Each matrix has a size that is represented as \\(n\\ x\\ m\\) where \\(n\\) is the number of rows and \\(m\\) is the number of columns. In the above example the array \\(A\\) has a size of \\(2\\ x\\ 4\\) out of the last two matrices in the above example the matrix \\(C\\) is what we call a column matrix and \\(D\\) is called a row matrix. The other word for it is column/row Vector and Vectors are usually denoted with lower case characters like this: \\[a = \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix},b = \\begin{bmatrix}4 & 5 & 6\\end{bmatrix}\\] Two matrices are equal if both have the same size (same number of rows and columns) and every element in one matrix matches with every element in the other (including their positions) Matrix operations # Addition : Two matrices can be added if both the matrices have the same size . The addition is merely adding the same values that have the same position on both matrices. Consider the following example \\[ \\begin{aligned} A = \\begin{bmatrix}1 & 2 & 3 & 4\\\\5 & 6 & 7 & 8\\end{bmatrix}, B = \\begin{bmatrix}9 & 10 & 11 & 12\\\\13 & 14 & 15 & 16\\end{bmatrix} \\\\ \\\\ C = A + B = \\begin{bmatrix}1 & 2 & 3 & 4\\\\5 & 6 & 7 & 8\\end{bmatrix} + \\begin{bmatrix}9 & 10 & 11 & 12\\\\13 & 14 & 15 & 16\\end{bmatrix} \\\\ \\\\ C = \\begin{bmatrix}1 + 9 & 2 + 10 & 3 + 11 & 4 + 12\\\\5 + 13 & 6 + 14 & 7 + 15 & 8 + 16\\end{bmatrix} \\\\ \\\\ C = \\begin{bmatrix}10 & 12 & 14 & 16\\\\18 & 20 & 22 & 24\\end{bmatrix} \\end{aligned} \\] Addition adhere to the following rules: \\[ A + B = B + A \\] \\[ (A + B) + C = A + (B + C) \\] \\[ A + 0 = A \\] \\[ A + (-A) = 0 \\] Multiplication : Two matrices can only be multiplied if the number of columns in the first part of the product equals the number of rows in the second part of the product to yield a product matrix with a size of the number of rows of the first part and the number of columns of the second part. The size of matrices after a product would look like this: \\[A_(m*p) * B_(p*n) = C_(m*n)\\] The formulae for calculating the entities within the product matrix are as follows, given that \\(i\\) and \\(j\\) are the row and column number that identifies element of that matrix: \\[A\\ is\\ an\\ array\\ with\\ a_{ij}\\ for\\ entities\\ and\\ have\\ size\\ m*n\\] \\[B\\ is\\ an\\ array\\ with\\ b_{ij}\\ for\\ entites\\ and\\ have\\ size\\ n*p\\] \\[C\\ is\\ the\\ product\\ array\\ with\\ c_{ij}\\ for\\ entites\\ and\\ have\\ size\\ n*m, then\\] \\[c_{ij} = \\begin{equation}\\sum_{k = 1}^{n}a_{ik}b_{kj}\\end{equation}\\] \\[where\\ j = 1, . . . , m\\ and\\ k = 1, . . . , p\\] Tags: !MathematicalFoundationsIndex","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week1MFDS.html#week-1","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 25/Jul/2021","title":"Week 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week1MFDS.html#topics-covered","text":"Matrices and their types REF and RREF Rank, its computation and properties Determinent, it's computation and properties Consistency and inconsistency of linear systems Modelling using linear equations Vector and matrix norms","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week1MFDS.html#matrices-and-their-types","text":"A rectangular array of numbers or functions enclosed in brrackets are considered as matrices (Matrix for singular). All of the following are valid matrices: \\[A = \\begin{bmatrix}a & b & c & d\\\\e & f & g & h\\end{bmatrix},B = \\begin{bmatrix}a & b & c\\\\d & e & f\\\\g & h & i\\end{bmatrix},C = \\begin{bmatrix}a\\\\b\\\\c\\end{bmatrix},D = \\begin{bmatrix}a & b & c\\end{bmatrix}\\] Each element within the matrix ( \\(a\\) , \\(b\\) , \\(c\\) etc) are called entities Each matrix has a size that is represented as \\(n\\ x\\ m\\) where \\(n\\) is the number of rows and \\(m\\) is the number of columns. In the above example the array \\(A\\) has a size of \\(2\\ x\\ 4\\) out of the last two matrices in the above example the matrix \\(C\\) is what we call a column matrix and \\(D\\) is called a row matrix. The other word for it is column/row Vector and Vectors are usually denoted with lower case characters like this: \\[a = \\begin{bmatrix}1\\\\2\\\\3\\end{bmatrix},b = \\begin{bmatrix}4 & 5 & 6\\end{bmatrix}\\] Two matrices are equal if both have the same size (same number of rows and columns) and every element in one matrix matches with every element in the other (including their positions)","title":"Matrices and their types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week1MFDS.html#matrix-operations","text":"Addition : Two matrices can be added if both the matrices have the same size . The addition is merely adding the same values that have the same position on both matrices. Consider the following example \\[ \\begin{aligned} A = \\begin{bmatrix}1 & 2 & 3 & 4\\\\5 & 6 & 7 & 8\\end{bmatrix}, B = \\begin{bmatrix}9 & 10 & 11 & 12\\\\13 & 14 & 15 & 16\\end{bmatrix} \\\\ \\\\ C = A + B = \\begin{bmatrix}1 & 2 & 3 & 4\\\\5 & 6 & 7 & 8\\end{bmatrix} + \\begin{bmatrix}9 & 10 & 11 & 12\\\\13 & 14 & 15 & 16\\end{bmatrix} \\\\ \\\\ C = \\begin{bmatrix}1 + 9 & 2 + 10 & 3 + 11 & 4 + 12\\\\5 + 13 & 6 + 14 & 7 + 15 & 8 + 16\\end{bmatrix} \\\\ \\\\ C = \\begin{bmatrix}10 & 12 & 14 & 16\\\\18 & 20 & 22 & 24\\end{bmatrix} \\end{aligned} \\] Addition adhere to the following rules: \\[ A + B = B + A \\] \\[ (A + B) + C = A + (B + C) \\] \\[ A + 0 = A \\] \\[ A + (-A) = 0 \\] Multiplication : Two matrices can only be multiplied if the number of columns in the first part of the product equals the number of rows in the second part of the product to yield a product matrix with a size of the number of rows of the first part and the number of columns of the second part. The size of matrices after a product would look like this: \\[A_(m*p) * B_(p*n) = C_(m*n)\\] The formulae for calculating the entities within the product matrix are as follows, given that \\(i\\) and \\(j\\) are the row and column number that identifies element of that matrix: \\[A\\ is\\ an\\ array\\ with\\ a_{ij}\\ for\\ entities\\ and\\ have\\ size\\ m*n\\] \\[B\\ is\\ an\\ array\\ with\\ b_{ij}\\ for\\ entites\\ and\\ have\\ size\\ n*p\\] \\[C\\ is\\ the\\ product\\ array\\ with\\ c_{ij}\\ for\\ entites\\ and\\ have\\ size\\ n*m, then\\] \\[c_{ij} = \\begin{equation}\\sum_{k = 1}^{n}a_{ik}b_{kj}\\end{equation}\\] \\[where\\ j = 1, . . . , m\\ and\\ k = 1, . . . , p\\] Tags: !MathematicalFoundationsIndex","title":"Matrix operations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html","text":"Week 2 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 01/Aug/2021 Topics Covered # Vector Norms Common Norms Equivalence of Norms Matrix Norms Solution of Linear systems Gauss elimination methods Pitfalls of Gauss Elimination Algorithms Division By Zero Round-off Errors Operations Count for Gaussian Elimination Ill Conditioned Systems Condition Number Vector Norms # A function \\(|| . || : R^n \\rightarrow R\\) is a vector norm if it has the following properties: 1. \\(||x|| \\ge 0\\) for any vector \\(x \\in R^n\\) , and \\(||x|| = 0\\) if and only if \\(x = 0\\) . 2. \\(||\\alpha x|| = |\\alpha|\\ ||x||\\) for any vector \\(x \\in R^n\\) , and any scalar \\(\\alpha \\in R\\) . 3. \\(||x+y|| \\le ||x|| + ||y||\\) for any vectors \\(x,\\ y \\in R^n\\) . This property is called the triangle inequality (Sum of two sides is greater than the third) 4. Also to be noted is if the dimension value \\(n=1\\) then the modulus value function ( \\(|x|\\) ) is a vector norm. (where mod function is \\(|x| = \\max(x,-x)\\) ) Common Norms # The most commonly used vector norm is belong to the family of \\(l_p\\) norms, which are defined by: \\[||x||_p = \\begin{equation}(\\sum_{i = 1}^{n}|x|^p)^{1/p}\\end{equation}\\] The following \\(l_p\\) norms are more used than others: 1. \\(p=1:\\ The\\ l_1\\ norm\\ ||x||_1 = |x_1| + |x_2| + \\dots + |x_n|\\) 2. \\(p=2:\\ The\\ l_2\\ norm\\ or\\ Euclidean\\ norm ||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\sqrt{x^Tx}\\) 3. \\(p=1:\\ The\\ l_\\infty\\ norm\\ ||x||_{\\infty} = \\max_{1\\le i\\le n}|x_i|\\) Equivalence of Norms # We say that two vector norms are equivalent if there exists constants \\(C_1\\) and \\(C_2\\) , that are independent of \\(x\\) , such that for any vector \\(x \\in R^n\\) \\[C_1||x||_\\alpha \\le ||x||_\\beta \\le C_2||x||_\\alpha\\] Remember that the above equation does not show any relation between \\(C_1\\) or \\(C_2\\) or \\(\\alpha\\) or \\(\\beta\\) Consider the above example, we take the two forms of \\(l_p\\) norms, the \\(l_1\\) and the \\(l_\\infty\\) . You can see that each term of \\(||x||_1\\) is \\(\\le \\ ||x||_\\infty\\) hence we can say that the total sum \\(||x||_1 \\le n||x||_\\infty\\) . This shows the equivalence of \\(l_1\\) and \\(l_\\infty\\) norm. Matrix Norms # Some commonly used matrix norms are: 1. Matrix norm corresponding to vector \\(1\\) -norm is maximum modulus/absolute column sum: \\[||A||_1 = \\max_{j}(\\begin{equation} \\sum_{i=1}^{n} |a_{ij}| \\end{equation})\\] You can see that we take the maximum of the absolute sum of the columns Matrix norm corresponding to vector \\(\\infty\\) -norm is maximum modulus/absolute column sum: \\[||A||_\\infty = \\max_{i}(\\begin{equation} \\sum_{j=1}^{n} |a_{ij}| \\end{equation})\\] You can see that we take the maximum of the absolute sum of the columns Matrix norm corresponding to vector \\(2\\) -norm is given as: \\[||A||_2 = ||A||_F = \\sqrt{\\sum_{i=1}^{M}\\sum_{j=1}^{N} |a_{ij}|^2}\\] Question asked : What is the significance of a norm for a matrix? Visualizing the above through Octave commands: >> A = [ 2 , 1 , 1 ; 3 , 2 , 1 ; - 2 , 0 , 1 ] A = 2 1 1 3 2 1 - 2 0 1 >> norm ( A , 1 ) ans = 7 >> norm ( A , 'inf' ) ans = 6 >> norm ( A , 'fro' ) ans = 5 >> norm ( A , 2 ) ans = 4.6758 Question to ask : if \\(||A||_2 = |A||_F\\) why are the results different in the above octave result? Solution of Linear systems # Matrix form of Linear Systems We can take m linear equations and write them as a single Vector equation like \\[Ax=b\\] Where coefficient matrix \\(A = [a_{jk}]\\) is the \\(m\\ x\\ n\\) matrix Where all the variables are in the vector \\(x = [x_k]\\) with size \\(n\\ x\\ 1\\) Where all the constants are in vector b \\(b = [b_j]\\) with size \\(m\\ x\\ 1\\) Gauss Elimination # Consider the above image, at the end of gauss elimination, the row echelon form would hold a rank say \\(r\\) , then we can say: 1. The matrix is a upper triangular form 2. The first \\(r\\) rows are non zero 3. Exactly \\(m-r\\) rows would be zero rows 4. The rhs can have two possibilities: 1. The rhs has exactly \\(m-r\\) zero rows: This means there is atleast one solution for the linear system of equations 2. The rhs has less than \\(m-r\\) zero rows: This means the equations are inconsistent 5. If the system is consistent and the \\(r = n\\) then there is exactly one solution 6. In case of infinitely many solutions, take arbiterary values for \\(x_{r+1},\\dots x_n\\) , then solve the \\(r\\) th equation for \\(x_r\\) , then the \\((r-1)\\) st equaation for \\(x_{r-1}\\) , and so on up the line till the solutions are found 7. The complexity for calculating this is \\(O(n^3)\\) , where \\(n\\) is the number of rows Pitfalls of Gauss Elimination Algorithms # Division By Zero # It is possible that in the above considered example, the denominator can end up being a zero value. For example consider the example below: Here the coefficient of \\(x_1\\) in the first equation is 0, hence to avoid the above issue we can interchange rows \\(R_1 \\rightarrow R_2\\) . This is called pivoting and that will make it easier to bring about the REF form for the matrix. Round-off Errors # Because computers can carry limited number of significant figures, round off errors will occur and they will propagate for every iteration. This problem becomes very apparent when the number of equations become very large. To avoid this one must use double-precision numbers. Albeit it being slow, the result will be more correct Operations Count for Gaussian Elimination # The details for the operation counts for several operations done is explained in the page here: GaussianEliminationPython#Operations Count Ill Conditioned Systems # Consider the following system of linear equations: \\(x_1 + 2x_2 = 10\\) \\(1.1x_1 + 2x_2 = 10.4\\) We get the values of \\(x_1 = 4.0\\) and \\(x_2 = 3.0\\) If let us say that when someone modeled this equation they had made a miscalculation leading to changing one of the coefficients of \\(x_1\\) from \\(1.1\\) to \\(1.05\\) making the above system of equations as follows: \\(x_1 + 2x_2 = 10\\) \\(1.05x_1 + 2x_2 = 10.4\\) We get the values of \\(x_1 = 8.0\\) and \\(x_2 = 1.0\\) From the above two results you can see that there is a big variance in the results when there is a small change in the coefficients, such system of equations are said to be ill conditioned Condition Number # The condition number is a metric that is used to see if a system of equations is ill informed or not. Condition number of a non singular matrix A is defined as: \\[\\kappa(A) = ||A|| ||A^{-1}||\\] By convention \\(\\kappa(A) = \\infty\\) if \\(A\\) is singular (A matrix whose determinant is 0, and hence has no inverse) Example: \\[A = \\begin{bmatrix}2 & -1 & 1\\\\1 & 0 & 1\\\\3 & -1 & 4\\end{bmatrix},\\ ||A||_1 = 6,\\ ||A||_\\infty = 8\\] \\[A^{-1} = \\begin{bmatrix}0.5 & 1.5 & -0.5\\\\-0.5 & 2.5 & -0.5\\\\-0.5 & -0.5 & 0.5\\end{bmatrix},\\ ||A^{-1}||_1 = 4.5,\\ ||A^{-1}||_\\infty = 3.5\\] \\[\\kappa_1(A) = 6 * 4.5\\] \\[\\kappa_\\infty(A) = 8 * 3.5\\] Once conditions are calculated, we can say that whenever the condition number is in the range of 4 to 5 then they are well informed but if the condition is around a 100 or so then they are more ill conditioned in nature . There is no clear distinction between well informed and ill informed system of equations Tags: !MathematicalFoundationsIndex","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#week-2","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 01/Aug/2021","title":"Week 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#topics-covered","text":"Vector Norms Common Norms Equivalence of Norms Matrix Norms Solution of Linear systems Gauss elimination methods Pitfalls of Gauss Elimination Algorithms Division By Zero Round-off Errors Operations Count for Gaussian Elimination Ill Conditioned Systems Condition Number","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#vector-norms","text":"A function \\(|| . || : R^n \\rightarrow R\\) is a vector norm if it has the following properties: 1. \\(||x|| \\ge 0\\) for any vector \\(x \\in R^n\\) , and \\(||x|| = 0\\) if and only if \\(x = 0\\) . 2. \\(||\\alpha x|| = |\\alpha|\\ ||x||\\) for any vector \\(x \\in R^n\\) , and any scalar \\(\\alpha \\in R\\) . 3. \\(||x+y|| \\le ||x|| + ||y||\\) for any vectors \\(x,\\ y \\in R^n\\) . This property is called the triangle inequality (Sum of two sides is greater than the third) 4. Also to be noted is if the dimension value \\(n=1\\) then the modulus value function ( \\(|x|\\) ) is a vector norm. (where mod function is \\(|x| = \\max(x,-x)\\) )","title":"Vector Norms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#common-norms","text":"The most commonly used vector norm is belong to the family of \\(l_p\\) norms, which are defined by: \\[||x||_p = \\begin{equation}(\\sum_{i = 1}^{n}|x|^p)^{1/p}\\end{equation}\\] The following \\(l_p\\) norms are more used than others: 1. \\(p=1:\\ The\\ l_1\\ norm\\ ||x||_1 = |x_1| + |x_2| + \\dots + |x_n|\\) 2. \\(p=2:\\ The\\ l_2\\ norm\\ or\\ Euclidean\\ norm ||x||_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2} = \\sqrt{x^Tx}\\) 3. \\(p=1:\\ The\\ l_\\infty\\ norm\\ ||x||_{\\infty} = \\max_{1\\le i\\le n}|x_i|\\)","title":"Common Norms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#equivalence-of-norms","text":"We say that two vector norms are equivalent if there exists constants \\(C_1\\) and \\(C_2\\) , that are independent of \\(x\\) , such that for any vector \\(x \\in R^n\\) \\[C_1||x||_\\alpha \\le ||x||_\\beta \\le C_2||x||_\\alpha\\] Remember that the above equation does not show any relation between \\(C_1\\) or \\(C_2\\) or \\(\\alpha\\) or \\(\\beta\\) Consider the above example, we take the two forms of \\(l_p\\) norms, the \\(l_1\\) and the \\(l_\\infty\\) . You can see that each term of \\(||x||_1\\) is \\(\\le \\ ||x||_\\infty\\) hence we can say that the total sum \\(||x||_1 \\le n||x||_\\infty\\) . This shows the equivalence of \\(l_1\\) and \\(l_\\infty\\) norm.","title":"Equivalence of Norms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#matrix-norms","text":"Some commonly used matrix norms are: 1. Matrix norm corresponding to vector \\(1\\) -norm is maximum modulus/absolute column sum: \\[||A||_1 = \\max_{j}(\\begin{equation} \\sum_{i=1}^{n} |a_{ij}| \\end{equation})\\] You can see that we take the maximum of the absolute sum of the columns Matrix norm corresponding to vector \\(\\infty\\) -norm is maximum modulus/absolute column sum: \\[||A||_\\infty = \\max_{i}(\\begin{equation} \\sum_{j=1}^{n} |a_{ij}| \\end{equation})\\] You can see that we take the maximum of the absolute sum of the columns Matrix norm corresponding to vector \\(2\\) -norm is given as: \\[||A||_2 = ||A||_F = \\sqrt{\\sum_{i=1}^{M}\\sum_{j=1}^{N} |a_{ij}|^2}\\] Question asked : What is the significance of a norm for a matrix? Visualizing the above through Octave commands: >> A = [ 2 , 1 , 1 ; 3 , 2 , 1 ; - 2 , 0 , 1 ] A = 2 1 1 3 2 1 - 2 0 1 >> norm ( A , 1 ) ans = 7 >> norm ( A , 'inf' ) ans = 6 >> norm ( A , 'fro' ) ans = 5 >> norm ( A , 2 ) ans = 4.6758 Question to ask : if \\(||A||_2 = |A||_F\\) why are the results different in the above octave result?","title":"Matrix Norms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#solution-of-linear-systems","text":"Matrix form of Linear Systems We can take m linear equations and write them as a single Vector equation like \\[Ax=b\\] Where coefficient matrix \\(A = [a_{jk}]\\) is the \\(m\\ x\\ n\\) matrix Where all the variables are in the vector \\(x = [x_k]\\) with size \\(n\\ x\\ 1\\) Where all the constants are in vector b \\(b = [b_j]\\) with size \\(m\\ x\\ 1\\)","title":"Solution of Linear systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#gauss-elimination","text":"Consider the above image, at the end of gauss elimination, the row echelon form would hold a rank say \\(r\\) , then we can say: 1. The matrix is a upper triangular form 2. The first \\(r\\) rows are non zero 3. Exactly \\(m-r\\) rows would be zero rows 4. The rhs can have two possibilities: 1. The rhs has exactly \\(m-r\\) zero rows: This means there is atleast one solution for the linear system of equations 2. The rhs has less than \\(m-r\\) zero rows: This means the equations are inconsistent 5. If the system is consistent and the \\(r = n\\) then there is exactly one solution 6. In case of infinitely many solutions, take arbiterary values for \\(x_{r+1},\\dots x_n\\) , then solve the \\(r\\) th equation for \\(x_r\\) , then the \\((r-1)\\) st equaation for \\(x_{r-1}\\) , and so on up the line till the solutions are found 7. The complexity for calculating this is \\(O(n^3)\\) , where \\(n\\) is the number of rows","title":"Gauss Elimination"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#pitfalls-of-gauss-elimination-algorithms","text":"","title":"Pitfalls of Gauss Elimination Algorithms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#division-by-zero","text":"It is possible that in the above considered example, the denominator can end up being a zero value. For example consider the example below: Here the coefficient of \\(x_1\\) in the first equation is 0, hence to avoid the above issue we can interchange rows \\(R_1 \\rightarrow R_2\\) . This is called pivoting and that will make it easier to bring about the REF form for the matrix.","title":"Division By Zero"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#round-off-errors","text":"Because computers can carry limited number of significant figures, round off errors will occur and they will propagate for every iteration. This problem becomes very apparent when the number of equations become very large. To avoid this one must use double-precision numbers. Albeit it being slow, the result will be more correct","title":"Round-off Errors"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#operations-count-for-gaussian-elimination","text":"The details for the operation counts for several operations done is explained in the page here: GaussianEliminationPython#Operations Count","title":"Operations Count for Gaussian Elimination"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#ill-conditioned-systems","text":"Consider the following system of linear equations: \\(x_1 + 2x_2 = 10\\) \\(1.1x_1 + 2x_2 = 10.4\\) We get the values of \\(x_1 = 4.0\\) and \\(x_2 = 3.0\\) If let us say that when someone modeled this equation they had made a miscalculation leading to changing one of the coefficients of \\(x_1\\) from \\(1.1\\) to \\(1.05\\) making the above system of equations as follows: \\(x_1 + 2x_2 = 10\\) \\(1.05x_1 + 2x_2 = 10.4\\) We get the values of \\(x_1 = 8.0\\) and \\(x_2 = 1.0\\) From the above two results you can see that there is a big variance in the results when there is a small change in the coefficients, such system of equations are said to be ill conditioned","title":"Ill Conditioned Systems"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week2MFDS.html#condition-number","text":"The condition number is a metric that is used to see if a system of equations is ill informed or not. Condition number of a non singular matrix A is defined as: \\[\\kappa(A) = ||A|| ||A^{-1}||\\] By convention \\(\\kappa(A) = \\infty\\) if \\(A\\) is singular (A matrix whose determinant is 0, and hence has no inverse) Example: \\[A = \\begin{bmatrix}2 & -1 & 1\\\\1 & 0 & 1\\\\3 & -1 & 4\\end{bmatrix},\\ ||A||_1 = 6,\\ ||A||_\\infty = 8\\] \\[A^{-1} = \\begin{bmatrix}0.5 & 1.5 & -0.5\\\\-0.5 & 2.5 & -0.5\\\\-0.5 & -0.5 & 0.5\\end{bmatrix},\\ ||A^{-1}||_1 = 4.5,\\ ||A^{-1}||_\\infty = 3.5\\] \\[\\kappa_1(A) = 6 * 4.5\\] \\[\\kappa_\\infty(A) = 8 * 3.5\\] Once conditions are calculated, we can say that whenever the condition number is in the range of 4 to 5 then they are well informed but if the condition is around a 100 or so then they are more ill conditioned in nature . There is no clear distinction between well informed and ill informed system of equations Tags: !MathematicalFoundationsIndex","title":"Condition Number"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html","text":"Week 3 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 08/Aug/2021 Topics Covered # Gauss Elimination Analysis (cont.) Corollary Iterative methods Gauss Jacobi Gauss Seidel Gauss Elimination Analysis (cont.) # A time analysis of the algorithm for different size of inputs is shown below: Algorithm n = 1000 n = 10000 Elimination 0.7 s 11 min Back substitution 0.001 s 0.1 s Corollary # Doolittle L Crout Method U Cholesky's Method \\(U = L^T\\) when A is symmetric and positive definite - A is written as \\(A = U^T U\\) . Hence we may have \\(U^T Ux = b\\) Iterative methods # Gauss Jacobi # Computations for each element can be done in parallel since each step independent Convergence is generally faster than Jacobi method Gauss Seidel # The gauss seidel method can be applied to any matrix with non zero elements on diagonal, but convergence is not guaranteed Computations for each element cannot be done in parallel since each step depends on the previous calculation Convergence is generally faster than Jacobi method Tags: !MathematicalFoundationsIndex","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#week-3","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 08/Aug/2021","title":"Week 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#topics-covered","text":"Gauss Elimination Analysis (cont.) Corollary Iterative methods Gauss Jacobi Gauss Seidel","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#gauss-elimination-analysis-cont","text":"A time analysis of the algorithm for different size of inputs is shown below: Algorithm n = 1000 n = 10000 Elimination 0.7 s 11 min Back substitution 0.001 s 0.1 s","title":"Gauss Elimination Analysis (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#corollary","text":"Doolittle L Crout Method U Cholesky's Method \\(U = L^T\\) when A is symmetric and positive definite - A is written as \\(A = U^T U\\) . Hence we may have \\(U^T Ux = b\\)","title":"Corollary"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#iterative-methods","text":"","title":"Iterative methods"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#gauss-jacobi","text":"Computations for each element can be done in parallel since each step independent Convergence is generally faster than Jacobi method","title":"Gauss Jacobi"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week3MFDS.html#gauss-seidel","text":"The gauss seidel method can be applied to any matrix with non zero elements on diagonal, but convergence is not guaranteed Computations for each element cannot be done in parallel since each step depends on the previous calculation Convergence is generally faster than Jacobi method Tags: !MathematicalFoundationsIndex","title":"Gauss Seidel"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html","text":"Week 4 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 22/Aug/2021 Topics Covered # Field Vector Space Inner Product Linear Dependence and Independence of Vectors Basis and Dimension Field # Axioms Vector Space # Axioms: Inner Product # Linear Dependence and Independence of Vectors # S_1 = {1rs Note, 5rs Note} Here S is not independent since 5 1rs notes can become 5rs S_2 = {Coffee powder, Water, Milk, Sugar} Here S is linearly independent Basis and Dimension # Tags: !MathematicalFoundationsIndex","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#week-4","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 22/Aug/2021","title":"Week 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#topics-covered","text":"Field Vector Space Inner Product Linear Dependence and Independence of Vectors Basis and Dimension","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#field","text":"Axioms","title":"Field"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#vector-space","text":"Axioms:","title":"Vector Space"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#inner-product","text":"","title":"Inner Product"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#linear-dependence-and-independence-of-vectors","text":"S_1 = {1rs Note, 5rs Note} Here S is not independent since 5 1rs notes can become 5rs S_2 = {Coffee powder, Water, Milk, Sugar} Here S is linearly independent","title":"Linear Dependence and Independence of Vectors"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week4MFDS.html#basis-and-dimension","text":"Tags: !MathematicalFoundationsIndex","title":"Basis and Dimension"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html","text":"Week 5 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 29/Aug/2021 Topics Covered # S LI, LS(s) = V S is a basus No. of elements of S is dimension Several bases for V but dimenstion is the same Any set that contains 0 is LD Any non zero vector is LI Construction of Basis \\(S = {v_1}\\) \\(v_1 \\ne 0\\) Span S = Example Consider a 3d space of x, y, z, and a span set as: S = {(1, 0, 0)} We can say that this does not span the entire spce, but it does cover the x axis Now if we take: S = {(1, 0, 0), (0, 1, 0), (0, 0, 1)} This set S is the Basis of Row space and common space # Row Space # Column Space # Null Space/ Solution Space # Example # Theorem Nullity and Rank of Matrix # Example # Linear Transformation # Range and Kernel # Rank Nullity Theorem Example # Example 1: Example 2: Tags: !MathematicalFoundationsIndex","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#week-5","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 29/Aug/2021","title":"Week 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#topics-covered","text":"S LI, LS(s) = V S is a basus No. of elements of S is dimension Several bases for V but dimenstion is the same Any set that contains 0 is LD Any non zero vector is LI Construction of Basis \\(S = {v_1}\\) \\(v_1 \\ne 0\\) Span S = Example Consider a 3d space of x, y, z, and a span set as: S = {(1, 0, 0)} We can say that this does not span the entire spce, but it does cover the x axis Now if we take: S = {(1, 0, 0), (0, 1, 0), (0, 0, 1)} This set S is the Basis of","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#row-space-and-common-space","text":"","title":"Row space and common space"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#row-space","text":"","title":"Row Space"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#column-space","text":"","title":"Column Space"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#null-space-solution-space","text":"","title":"Null Space/ Solution Space"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#example","text":"Theorem","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#nullity-and-rank-of-matrix","text":"","title":"Nullity and Rank of Matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#example_1","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#linear-transformation","text":"","title":"Linear Transformation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#range-and-kernel","text":"","title":"Range and Kernel"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week5MFDS.html#rank-nullity-theorem-example","text":"Example 1: Example 2: Tags: !MathematicalFoundationsIndex","title":"Rank Nullity Theorem Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html","text":"Week 6 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 5/Sep/2021 Topics Covered # Eigen Values and Eigen Vectors # >> A = [ - 5 , 2 ; 2 , - 2 ] A = - 5 2 2 - 2 >> eig ( A ) ans = - 6 - 1 >> [ u , v ] = eigs ( A ) u = - 0.8944 - 0.4472 0.4472 - 0.8944 v = Diagonal Matrix - 6 0 0 - 1 >> Example Problem # Jordan Canonical Form # Gerschgorgin's Theorem # Algebraic and Geometric Multiplicity # Tags: !MathematicalFoundationsIndex","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#week-6","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 5/Sep/2021","title":"Week 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#eigen-values-and-eigen-vectors","text":">> A = [ - 5 , 2 ; 2 , - 2 ] A = - 5 2 2 - 2 >> eig ( A ) ans = - 6 - 1 >> [ u , v ] = eigs ( A ) u = - 0.8944 - 0.4472 0.4472 - 0.8944 v = Diagonal Matrix - 6 0 0 - 1 >>","title":"Eigen Values and Eigen Vectors"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#example-problem","text":"","title":"Example Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#jordan-canonical-form","text":"","title":"Jordan Canonical Form"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#gerschgorgins-theorem","text":"","title":"Gerschgorgin's Theorem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS.html#algebraic-and-geometric-multiplicity","text":"Tags: !MathematicalFoundationsIndex","title":"Algebraic and Geometric Multiplicity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html","text":"Week 6 (cont.) # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 7/Sep/2021 Topics Covered # Special Matrices # Similarity Of Matrices # >> A = rand ( 4 , 4 ) A = 5.8181e-01 9.6943e-01 4.2533e-01 6.4573e-01 5.4844e-01 3.2016e-01 9.8479e-01 7.1938e-02 9.0286e-01 8.2269e-01 6.9796e-02 2.7225e-01 3.1388e-01 6.8389e-03 1.9440e-01 5.5537e-01 >> eig ( A ) ans = 2.0028 + 0 i - 0.5037 + 0.2040 i - 0.5037 - 0.2040 i 0.5318 + 0 i >> P = [ 6 , 10 , 11 , 2 ; 2 , 3 , 5 , 6 ; 19. 21. 61 , 63 ; 39 , 37 , 79 , 83 ] P = 6 10 11 2 2 3 5 6 19 21 61 63 39 37 79 83 >> det ( P ) ans = 1.1352e+04 >> B = inv ( P ) * A * P B = - 13.8730 - 16.5433 - 39.9569 - 38.2545 19.8346 23.1043 56.8459 55.2196 - 7.9618 - 9.4378 - 25.1538 - 24.5634 5.5832 6.7917 18.0890 17.4496 >> eig ( B ) ans = 2.0028 + 0 i 0.5318 + 0 i - 0.5037 + 0.2040 i - 0.5037 - 0.2040 i We see that \\(A\\) and \\(B\\) both share the same Eigen values, so \\(A\\) and \\(B\\) are similar matrices So for a random matrix \\(P\\) , we can find a similar matrix for \\(A\\) by doing: \\(B = P^{-1} . A . P\\) Diagonalization # These 3 vectors are independent so, Dominant Eigen Value # Rayleigh's Quotient # Go through the excel sheet Power Method for # Convergence of Power Method # Tags: !MathematicalFoundationsIndex","title":"Week 6 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#week-6-cont","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 7/Sep/2021","title":"Week 6 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#special-matrices","text":"","title":"Special Matrices"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#similarity-of-matrices","text":">> A = rand ( 4 , 4 ) A = 5.8181e-01 9.6943e-01 4.2533e-01 6.4573e-01 5.4844e-01 3.2016e-01 9.8479e-01 7.1938e-02 9.0286e-01 8.2269e-01 6.9796e-02 2.7225e-01 3.1388e-01 6.8389e-03 1.9440e-01 5.5537e-01 >> eig ( A ) ans = 2.0028 + 0 i - 0.5037 + 0.2040 i - 0.5037 - 0.2040 i 0.5318 + 0 i >> P = [ 6 , 10 , 11 , 2 ; 2 , 3 , 5 , 6 ; 19. 21. 61 , 63 ; 39 , 37 , 79 , 83 ] P = 6 10 11 2 2 3 5 6 19 21 61 63 39 37 79 83 >> det ( P ) ans = 1.1352e+04 >> B = inv ( P ) * A * P B = - 13.8730 - 16.5433 - 39.9569 - 38.2545 19.8346 23.1043 56.8459 55.2196 - 7.9618 - 9.4378 - 25.1538 - 24.5634 5.5832 6.7917 18.0890 17.4496 >> eig ( B ) ans = 2.0028 + 0 i 0.5318 + 0 i - 0.5037 + 0.2040 i - 0.5037 - 0.2040 i We see that \\(A\\) and \\(B\\) both share the same Eigen values, so \\(A\\) and \\(B\\) are similar matrices So for a random matrix \\(P\\) , we can find a similar matrix for \\(A\\) by doing: \\(B = P^{-1} . A . P\\)","title":"Similarity Of Matrices"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#diagonalization","text":"These 3 vectors are independent so,","title":"Diagonalization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#dominant-eigen-value","text":"","title":"Dominant Eigen Value"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#rayleighs-quotient","text":"Go through the excel sheet","title":"Rayleigh's Quotient"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#power-method-for","text":"","title":"Power Method for"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week6MFDS2.html#convergence-of-power-method","text":"Tags: !MathematicalFoundationsIndex","title":"Convergence of Power Method"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html","text":"Week 7 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 12/Sep/2021 Topics Covered # Inner Product Spaces # Orthogonality # Gram Schmidt Orthogonalization Process # QR Decomposition # Transformation of circle under matrix operation # A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }% Singular Value Decomposition # Evaluation of U and V # See notebook Comparison between Eigenvalue decomposition and SVD # Left and right singular matrix # Summation form of SVD # Face Recognition # Average Faces # Eigen Faces # Image Keys # Dimensionality Reduction # Tags: !MathematicalFoundationsIndex","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#week-7","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 12/Sep/2021","title":"Week 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#inner-product-spaces","text":"","title":"Inner Product Spaces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#orthogonality","text":"","title":"Orthogonality"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#gram-schmidt-orthogonalization-process","text":"","title":"Gram Schmidt Orthogonalization Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#qr-decomposition","text":"","title":"QR Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#transformation-of-circle-under-matrix-operation","text":"A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }%","title":"Transformation of circle under matrix operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#singular-value-decomposition","text":"","title":"Singular Value Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#evaluation-of-u-and-v","text":"See notebook","title":"Evaluation of U and V"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#comparison-between-eigenvalue-decomposition-and-svd","text":"","title":"Comparison between Eigenvalue decomposition and SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#left-and-right-singular-matrix","text":"","title":"Left and right singular matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#summation-form-of-svd","text":"","title":"Summation form of SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#face-recognition","text":"","title":"Face Recognition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#average-faces","text":"","title":"Average Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#eigen-faces","text":"","title":"Eigen Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#image-keys","text":"","title":"Image Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week7MFDS.html#dimensionality-reduction","text":"Tags: !MathematicalFoundationsIndex","title":"Dimensionality Reduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html","text":"Week 8 # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 12/Sep/2021 Topics Covered # Inner Product Spaces # Orthogonality # Gram Schmidt Orthogonalization Process # QR Decomposition # Transformation of circle under matrix operation # A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }% Singular Value Decomposition # Evaluation of U and V # See notebook Comparison between Eigenvalue decomposition and SVD # Left and right singular matrix # Summation form of SVD # Face Recognition # Average Faces # Eigen Faces # Image Keys # Dimensionality Reduction # Tags: !MathematicalFoundationsIndex","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#week-8","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 12/Sep/2021","title":"Week 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#inner-product-spaces","text":"","title":"Inner Product Spaces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#orthogonality","text":"","title":"Orthogonality"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#gram-schmidt-orthogonalization-process","text":"","title":"Gram Schmidt Orthogonalization Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#qr-decomposition","text":"","title":"QR Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#transformation-of-circle-under-matrix-operation","text":"A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }%","title":"Transformation of circle under matrix operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#singular-value-decomposition","text":"","title":"Singular Value Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#evaluation-of-u-and-v","text":"See notebook","title":"Evaluation of U and V"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#comparison-between-eigenvalue-decomposition-and-svd","text":"","title":"Comparison between Eigenvalue decomposition and SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#left-and-right-singular-matrix","text":"","title":"Left and right singular matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#summation-form-of-svd","text":"","title":"Summation form of SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#face-recognition","text":"","title":"Face Recognition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#average-faces","text":"","title":"Average Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#eigen-faces","text":"","title":"Eigen Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#image-keys","text":"","title":"Image Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS.html#dimensionality-reduction","text":"Tags: !MathematicalFoundationsIndex","title":"Dimensionality Reduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html","text":"Week 8 (cont.) # Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 19/Sep/2021 Topics Covered # Inner Product Spaces # Orthogonality # Gram Schmidt Orthogonalization Process # QR Decomposition # Transformation of circle under matrix operation # A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }% Singular Value Decomposition # Evaluation of U and V # See notebook Comparison between Eigenvalue decomposition and SVD # Left and right singular matrix # Summation form of SVD # Face Recognition # Average Faces # Eigen Faces # Image Keys # Dimensionality Reduction # Tags: !MathematicalFoundationsIndex","title":"Week 8 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#week-8-cont","text":"Lecturer : G Venkiteswaran, Faculty for BITS Pilani Date : 19/Sep/2021","title":"Week 8 (cont.)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#topics-covered","text":"","title":"Topics Covered"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#inner-product-spaces","text":"","title":"Inner Product Spaces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#orthogonality","text":"","title":"Orthogonality"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#gram-schmidt-orthogonalization-process","text":"","title":"Gram Schmidt Orthogonalization Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#qr-decomposition","text":"","title":"QR Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#transformation-of-circle-under-matrix-operation","text":"A = [ 1 2 ; 3 4] %{ A = 1 2 3 4 }% x1 = [1 2]' %{ x1 = 1 2 }% x2 = [-2 1]' %{ x2 = -2 1 }% y1 = A * x1 %{ y1 = 5 11 }% y2 = A * x2 %{ y2 = 0 -2 }%","title":"Transformation of circle under matrix operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#singular-value-decomposition","text":"","title":"Singular Value Decomposition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#evaluation-of-u-and-v","text":"See notebook","title":"Evaluation of U and V"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#comparison-between-eigenvalue-decomposition-and-svd","text":"","title":"Comparison between Eigenvalue decomposition and SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#left-and-right-singular-matrix","text":"","title":"Left and right singular matrix"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#summation-form-of-svd","text":"","title":"Summation form of SVD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#face-recognition","text":"","title":"Face Recognition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#average-faces","text":"","title":"Average Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#eigen-faces","text":"","title":"Eigen Faces"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#image-keys","text":"","title":"Image Keys"},{"location":"Knowledge%20Base/All%20Semesters/Semester%201/Mathematical%20Foundations%20for%20Data%20Science/Week8MFDS2.html#dimensionality-reduction","text":"Tags: !MathematicalFoundationsIndex","title":"Dimensionality Reduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/%21Semester2Index.html","text":"Semester 2 Index # Linked pages to subjects # Advanced Statistics: !ASIndex Applied Machine Learning: !AMLIndex Object Oriented Analysis and Design: !OOADIndex Software Architectures: !SoftwareArchitecturesIndex Lecturer Contact Details # Advanced Statistics : YVK Ravi Kumar Applied Machine Learning : Swarna Chaudhary Software Architectures : HS Jabbal Tags: !AllSemestersIndex","title":"Semester 2 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/%21Semester2Index.html#semester-2-index","text":"","title":"Semester 2 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/%21Semester2Index.html#linked-pages-to-subjects","text":"Advanced Statistics: !ASIndex Applied Machine Learning: !AMLIndex Object Oriented Analysis and Design: !OOADIndex Software Architectures: !SoftwareArchitecturesIndex","title":"Linked pages to subjects"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/%21Semester2Index.html#lecturer-contact-details","text":"Advanced Statistics : YVK Ravi Kumar Applied Machine Learning : Swarna Chaudhary Software Architectures : HS Jabbal Tags: !AllSemestersIndex","title":"Lecturer Contact Details"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/%21ASIndex.html","text":"Advanced Statistics # Assignments # Assignment 1 ASAssignment1 Question Papers # Mid Sem Paper AS End Sem Paper AS Combined Notes # The notes for all the sessions taken can be found in this PDF Tags: !Semester2Index","title":"Advanced Statistics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/%21ASIndex.html#advanced-statistics","text":"","title":"Advanced Statistics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/%21ASIndex.html#assignments","text":"Assignment 1 ASAssignment1","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/%21ASIndex.html#question-papers","text":"Mid Sem Paper AS End Sem Paper AS","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/%21ASIndex.html#combined-notes","text":"The notes for all the sessions taken can be found in this PDF Tags: !Semester2Index","title":"Combined Notes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/ASAssignment1.html","text":"Advanced Statistics Assignment 1 # Each question carries 2.5 marks (2.5 \u00d7 4=10 Marks) 1) Submissions are individual 2) Solve these on paper, scan and upload 3) Plagiarism results in zero marks 4) Write your name, BITS ID on each page Q1. Feedback scores of three professors in four different courses taught by them are given below. Test the hypothesis that the feedback of professors is same for all the courses at 1% level of significance. Professor Course A Course B Course C Course D X 4.5 4.2 3.3 3.6 Y 3.8 4.1 2.7 4 Z 4 3.4 3.4 3 Q.2. Consider the following data. PROCESS SAMPLE SIZE MEAN LIFE STANDARD DEVIATION A 20 20,400 100 B 25 21,800 100 Test the hypothesis that average life of the two processes are significantly different at 5% level of significance. Q.3. Calculate the Correlation coefficient between the sample of ISM Quiz-1 marks and Assignment-1 Marks as below and also interpret the result. ---------------- --- --- --- --- --- --- --- --- --- --- Student 1 2 3 4 5 6 7 8 9 10 Quiz Marks 5 4.5 3 5 4.5 4.5 4.5 5 5 4.5 Assignment Marks 8.5 9 5.5 9.5 9 9.5 10 9.5 9 9 Student 11 12 13 14 15 16 17 18 19 20 Quiz Marks 4.5 4.5 5 3.5 4.5 5 5 4.5 5 4.5 Assignment Marks 10 10 8.5 9 7.5 9.5 9.5 9.5 9 9.5 Q.4.A manager of a Merchandising firm wishes to test whether its three salesmen X, Y and Z tend to make sales of the same size or whether they differ in their selling abilities. During a week there have been 14 sale call; X made 5 calls, Y made 4 calls and Z made 5 calls. Following are the weekly sales record of three salesmen: --- --- --- --- --- --- X 5 4 7 8 6 Y 3 7 4 6 - Z 5 3 5 4 3 Perform the analysis of variance and draw your conclusion. My Submission # My submission can be found here tags: !ASIndex Assignments","title":"Advanced Statistics Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/ASAssignment1.html#advanced-statistics-assignment-1","text":"Each question carries 2.5 marks (2.5 \u00d7 4=10 Marks) 1) Submissions are individual 2) Solve these on paper, scan and upload 3) Plagiarism results in zero marks 4) Write your name, BITS ID on each page Q1. Feedback scores of three professors in four different courses taught by them are given below. Test the hypothesis that the feedback of professors is same for all the courses at 1% level of significance. Professor Course A Course B Course C Course D X 4.5 4.2 3.3 3.6 Y 3.8 4.1 2.7 4 Z 4 3.4 3.4 3 Q.2. Consider the following data. PROCESS SAMPLE SIZE MEAN LIFE STANDARD DEVIATION A 20 20,400 100 B 25 21,800 100 Test the hypothesis that average life of the two processes are significantly different at 5% level of significance. Q.3. Calculate the Correlation coefficient between the sample of ISM Quiz-1 marks and Assignment-1 Marks as below and also interpret the result. ---------------- --- --- --- --- --- --- --- --- --- --- Student 1 2 3 4 5 6 7 8 9 10 Quiz Marks 5 4.5 3 5 4.5 4.5 4.5 5 5 4.5 Assignment Marks 8.5 9 5.5 9.5 9 9.5 10 9.5 9 9 Student 11 12 13 14 15 16 17 18 19 20 Quiz Marks 4.5 4.5 5 3.5 4.5 5 5 4.5 5 4.5 Assignment Marks 10 10 8.5 9 7.5 9.5 9.5 9.5 9 9.5 Q.4.A manager of a Merchandising firm wishes to test whether its three salesmen X, Y and Z tend to make sales of the same size or whether they differ in their selling abilities. During a week there have been 14 sale call; X made 5 calls, Y made 4 calls and Z made 5 calls. Following are the weekly sales record of three salesmen: --- --- --- --- --- --- X 5 4 7 8 6 Y 3 7 4 6 - Z 5 3 5 4 3 Perform the analysis of variance and draw your conclusion.","title":"Advanced Statistics Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/ASAssignment1.html#my-submission","text":"My submission can be found here tags: !ASIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html","text":"End Sem AS Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # tags: !ASIndex QuestionPapers","title":"End Sem AS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#end-sem-as-paper","text":"","title":"End Sem AS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/End%20Sem%20Paper%20AS.html#question-5","text":"tags: !ASIndex QuestionPapers","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html","text":"Mid Sem AS Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # tags: !ASIndex QuestionPapers","title":"Mid Sem AS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#mid-sem-as-paper","text":"","title":"Mid Sem AS Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Advanced%20Statistics/Mid%20Sem%20Paper%20AS.html#question-5","text":"tags: !ASIndex QuestionPapers","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html","text":"Applied Machine Learning # Course Plan # Modules # Lab Plan # Books and resources # University of California Datasets Kaggle Datasets Indian Government Datasets Evaluative Components # Assignments # Assignment 1 AMLAssignment1 Question Papers # Mid Sem Paper AML End Sem Paper AML Live Course Content # Module 1 Module1AML Module 2 Module2AML Module 3 Module3AML Module 5 Module5AML Module 6 Module6AML Module 7 Module7AML Jupyter Notebooks # Data Preprocessing Notebook DataPreprocessingNotebook Data Visualizing Notebook DataVisualisationNotebook Data Cleaning Introduction DataCleaningNotebook Tags: !Semester2Index","title":"Applied Machine Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#applied-machine-learning","text":"","title":"Applied Machine Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#course-plan","text":"","title":"Course Plan"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#modules","text":"","title":"Modules"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#lab-plan","text":"","title":"Lab Plan"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#books-and-resources","text":"University of California Datasets Kaggle Datasets Indian Government Datasets","title":"Books and resources"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#evaluative-components","text":"","title":"Evaluative Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#assignments","text":"Assignment 1 AMLAssignment1","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#question-papers","text":"Mid Sem Paper AML End Sem Paper AML","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#live-course-content","text":"Module 1 Module1AML Module 2 Module2AML Module 3 Module3AML Module 5 Module5AML Module 6 Module6AML Module 7 Module7AML","title":"Live Course Content"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/%21AMLIndex.html#jupyter-notebooks","text":"Data Preprocessing Notebook DataPreprocessingNotebook Data Visualizing Notebook DataVisualisationNotebook Data Cleaning Introduction DataCleaningNotebook Tags: !Semester2Index","title":"Jupyter Notebooks"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html","text":"Applied ML Assignment 1 # (This notebook along with it's PDF version can be found in this Github Repo ) # Heart Attack Analaysis # Introduction # A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. A blood clot can form and block your arteries, causing a heart attack. This Heart Attack Analysis helps to understand the chance of attack occurrence in persons based on varied health conditions. Dataset # The dataset is Heart_Attack_Analysis_Data.csv . It has been uploaded to elearn. This dataset contains data about some hundreds of patients mentioning: - Age - Sex - Exercise Include Angina(1=YES, 0=NO) - CP_Type (Chest Pain) (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic) - ECG Results - Blood Pressure - Cholesterol - Blood Sugar - Family History (Number of persons affected in the family) - Maximum Heart Rate - Target (0 = LESS CHANCE , 1 = MORE CHANCE) Aim # Building a Predictive Model using Na\u00efve Bayesian Approach (Which features decide heart attack?) Comment on the performance of this model using AUC-ROC, Precision, Recall, F_score, Accuracy You need to 1. Preprocess the data to enhance quality 2. Carry out descriptive summarization of data and make observations 3. Identify relevant, irrelevant attributes for building model. 4. Use data visualization tools and make observations 5. Carry out the chosen analytic task. Show results including intermediate results, as needed 6. Evaluate the solution Following are some points for you to take note of, while doing the assignment in Jupyter Notebook: - State all your assumptions clearly - List all intermediate steps and learnings - Mention your observations/findings Submission Plan # The following will be done in this notebook: 1. Verify the datatypes of the values given in the dataset and validate with the information given in the document. 2. Check for invalid values based on domain knowledge by checking if values are present in humanly possible ranges. 3. Figure out which columns are numeric and categorical based on the unique values each column has and based on information given in the assignment document. 4. Check for trends among numerical features and among categorical features to see if feature reduction can be done (via pairplots etc) 5. Check which numerical attributes are relevant and which are irrelevant and drop irrelevant ones. 6. Scale numerical attributes with a standard scaler. 7. Train a gnb MODEL 1 where it is fit with data where only scaling is done to numerical data. 8. Check for outliers via boxplots and remove them with IQR method. 9. Train a gnb MODEL 2 where it is fit with data where scaling is done to numerical data, and outliers are removed using the IQR method. 10. One hot encode categorical data. 11. Train a gnb MODEL 3 where it is fit with data where scaling is done to numerical data, outliers are removed using the IQR method and one hot encoding is done to categorical data. We will then compare the accuracy, Precision, Recall, F-Score and AOC-ROC of three models trained . My Submission # Importing necessary packages # import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats as stats import seaborn as sb import warnings from sklearn.metrics import accuracy_score , recall_score , precision_score , f1_score , confusion_matrix , ConfusionMatrixDisplay , precision_recall_fscore_support , roc_auc_score , roc_curve from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.preprocessing import StandardScaler warnings . filterwarnings ( 'ignore' ) Viewing Data # Check the shape of the dataframe loaded into memory from the CSV file and see the datatypes used in the dataset given df = pd . read_csv ( \"./Heart_Attack_Analysis_Data.csv\" ) print ( \"Dataframe Shape: {} \" . format ( df . shape )) print ( \"---------------------------------- \\n \" ) print ( \"With following data types: \\n \" ) df . info () print ( \"---------------------------------- \\n \" ) print ( \"First 5 rows of Dataframe:\" ) df . head () Dataframe Shape: (303, 11) ---------------------------------- With following data types: <class 'pandas.core.frame.DataFrame'> RangeIndex: 303 entries, 0 to 302 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Age 303 non-null int64 1 Sex 303 non-null int64 2 CP_Type 303 non-null int64 3 BloodPressure 303 non-null int64 4 Cholestrol 303 non-null int64 5 BloodSugar 303 non-null int64 6 ECG 303 non-null int64 7 MaxHeartRate 303 non-null int64 8 ExerciseAngina 303 non-null int64 9 FamilyHistory 303 non-null int64 10 Target 303 non-null int64 dtypes: int64(11) memory usage: 26.2 KB ---------------------------------- First 5 rows of Dataframe: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure Cholestrol BloodSugar ECG MaxHeartRate ExerciseAngina FamilyHistory Target 0 63 1 3 145 233 1 0 150 0 2 1 1 37 1 2 130 250 0 1 187 0 1 1 2 41 0 1 130 204 0 0 172 0 0 1 3 56 1 1 120 236 0 1 178 0 1 1 4 57 0 0 120 354 0 1 163 1 0 1 print ( \"Stats on Dataframe:\" ) df . describe () Stats on Dataframe: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure Cholestrol BloodSugar ECG MaxHeartRate ExerciseAngina FamilyHistory Target count 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 mean 54.366337 0.683168 0.966997 131.623762 246.264026 0.148515 0.528053 149.646865 0.326733 1.204620 0.544554 std 9.082101 0.466011 1.032052 17.538143 51.830751 0.356198 0.525860 22.905161 0.469794 1.096825 0.498835 min 29.000000 0.000000 0.000000 94.000000 126.000000 0.000000 0.000000 71.000000 0.000000 0.000000 0.000000 25% 47.500000 0.000000 0.000000 120.000000 211.000000 0.000000 0.000000 133.500000 0.000000 0.000000 0.000000 50% 55.000000 1.000000 1.000000 130.000000 240.000000 0.000000 1.000000 153.000000 0.000000 1.000000 1.000000 75% 61.000000 1.000000 2.000000 140.000000 274.500000 0.000000 1.000000 166.000000 1.000000 2.000000 1.000000 max 77.000000 1.000000 3.000000 200.000000 564.000000 1.000000 2.000000 202.000000 1.000000 5.000000 1.000000 All columns are integer type and all columns have a value for all rows (303 not null), which means that all values are filled and there are no missing values. Data Preprocessing # Now to check certain columns with humanly possible ranges based on domain knowledge # The following are the assumed ranges for these columns: 1. 0 < Age <= 100 years 2. 90 <= BloodPressure <= 200 3. 60 <= MaxHeartRate <= 220 print ( \" \\n Minimum Age = {} \" . format ( df [ \"Age\" ] . min ())) print ( \"Maximum Age = {} \" . format ( df [ \"Age\" ] . max ())) print ( \" \\n Minimum Blood Pressure = {} \" . format ( df [ \"BloodPressure\" ] . min ())) print ( \"Maximum Blood Pressure = {} \" . format ( df [ \"BloodPressure\" ] . max ())) print ( \" \\n Minimum Heart Rate = {} \" . format ( df [ \"MaxHeartRate\" ] . min ())) print ( \"Maximum Heart Rate = {} \" . format ( df [ \"MaxHeartRate\" ] . max ())) Minimum Age = 29 Maximum Age = 77 Minimum Blood Pressure = 94 Maximum Blood Pressure = 200 Minimum Heart Rate = 71 Maximum Heart Rate = 202 All of the mentioned columns have values within acceptable ranges. Checking number of unique values for each column # Column Name -> Unique Number count for column in list ( df . columns ): print ( \" {} -> {} \" . format ( column , df [ column ] . value_counts () . shape [ 0 ])) Age -> 41 Sex -> 2 CP_Type -> 4 BloodPressure -> 49 Cholestrol -> 152 BloodSugar -> 2 ECG -> 3 MaxHeartRate -> 91 ExerciseAngina -> 2 FamilyHistory -> 6 Target -> 2 Taking columns that have a maximum of 4 unique values (And based on information given in assignment document) as categorical and the rest as numeric: category_list = [ \"Sex\" , \"CP_Type\" , \"BloodSugar\" , \"ECG\" , \"ExerciseAngina\" ] numeric_list = [ \"Age\" , \"BloodPressure\" , \"Cholestrol\" , \"MaxHeartRate\" , \"FamilyHistory\" ] Checking for trends in numeric features through Pair Plots # df_number = df . loc [:, numeric_list ] df_number [ \"Target\" ] = df [ \"Target\" ] sb . pairplot ( df_number , hue = \"Target\" , palette = \"husl\" ) plt . show () The pair plots do not show any particular trends that can be used to reduce numerical features. We do see a slight relation between age and max heart rate but the plot is scattered enough to not relate them together. Checking frequncy of each categorical feature wrt target column to check how well it is balanced # df_category = df . loc [:, category_list ] df_category [ \"Target\" ] = df [ \"Target\" ] for i in category_list : plt . figure () sb . countplot ( x = i , data = df_category , hue = \"Target\" , palette = \"husl\" ) plt . title ( i ) Here we see that there is very little rows that have ECG value = 2 and similarly very little rows for BloodSugar value = 1. # Checking relevant numerical features # To do this we shall use the f_oneway function from scipy.stats . This function performs one-way ANOVA(Analysis of Variance) to test the null hypothesis that two groups of data have the same population mean. A feature is only relevant if the sample from a particular feature for a target category is statistically very different from another sample from the same feature for another target category. Checking relevance of Age # result = stats . f_oneway ( df [ \"Age\" ][ df [ \"Target\" ] == 0 ], df [ \"Age\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 7.524801303442268e-05 The pvalue is < 0.05. This shows that the means of the two distributions (One with Age wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence Age is relevant Checking relevance of BloodPressure # result = stats . f_oneway ( df [ \"BloodPressure\" ][ df [ \"Target\" ] == 0 ], df [ \"BloodPressure\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.011546059200233376 The pvalue is < 0.05. This shows that the means of the two distributions (One with BloodPressure wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence BloodPressure is relevant Checking relevance of Cholestrol # result = stats . f_oneway ( df [ \"Cholestrol\" ][ df [ \"Target\" ] == 0 ], df [ \"Cholestrol\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.1387903269560108 The pvalue is > 0.05. This shows that the means of the two distributions (One with Cholestrol wrt less chance of getting heart attack and the other with more chance of getting heart attack) are not significantly different statistically, hence Cholestrol is irrelevant Checking relevance of MaxHeartRate # result = stats . f_oneway ( df [ \"MaxHeartRate\" ][ df [ \"Target\" ] == 0 ], df [ \"MaxHeartRate\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 1.6973376386560805e-14 The pvalue is < 0.05. This shows that the means of the two distributions (One with MaxHeartRate wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence MaxHeartRate is relevant Checking relevance of FamilyHistory # result = stats . f_oneway ( df [ \"FamilyHistory\" ][ df [ \"Target\" ] == 0 ], df [ \"FamilyHistory\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.6172651404419242 The pvalue is > 0.05. This shows that the means of the two distributions (One with FamilyHistory wrt less chance of getting heart attack and the other with more chance of getting heart attack) are not significantly different statistically, hence FamilyHistory is irrelevant Dropping the irrelevant features # df . drop ([ \"Cholestrol\" ], axis = 1 , inplace = True ) df . drop ([ \"FamilyHistory\" ], axis = 1 , inplace = True ) numeric_list . remove ( \"Cholestrol\" ) numeric_list . remove ( \"FamilyHistory\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 63 1 3 145 1 0 150 0 1 1 37 1 2 130 0 1 187 0 1 2 41 0 1 130 0 0 172 0 1 3 56 1 1 120 0 1 178 0 1 4 57 0 0 120 0 1 163 1 1 Scaling the numeric attributes in the dataframe with a standard scaler # scaler = StandardScaler () df [ numeric_list ] = scaler . fit_transform ( df [ numeric_list ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 0.952197 1 3 0.763956 1 0 0.015443 0 1 1 -1.915313 1 2 -0.092738 0 1 1.633471 0 1 2 -1.474158 0 1 -0.092738 0 0 0.977514 0 1 3 0.180175 1 1 -0.663867 0 1 1.239897 0 1 4 0.290464 0 0 -0.663867 0 1 0.583939 1 1 Model 1 # Training a Guassian Naive Bayes model with the data (With scaling done to numerical features) # df1 = df . copy () X = df1 . drop ([ \"Target\" ], axis = 1 ) y = df1 [[ \"Target\" ]] Split X and y to training and test data # X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (242, 8) y_train: (242, 1) X_test: (61, 8) y_test: (61, 1) Prediction Analysis: # The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. Scale the numerical features with a standard scaler 2. Split the data into training and test data with a 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 1 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_auc = roc_auc_score ( y_test , ns_probs ) ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_auc = roc_auc_score ( y_test , gnb_probs ) gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 1 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 85.71% Precision Score: 82.50% Recall Score: 82.50% F Score: 82.50% Accuracy Score: 77.05% ---------------------------------- Confusion matrix: Investigating dataset for outliers # Analyzing the boxplot for scaled numeric attributes to check for outliers # plt . figure ( figsize = ( 10 , 8 )) sb . boxplot ( data = df [ numeric_list ], palette = \"husl\" ) plt . show () print ( \" \\n ---------------------------------- \\n \" ) ---------------------------------- We can see some outlier values for blood pressure and max heart rate. we can drop these outliers using the IQR method. Dropping outliers with IQR method # Going with \\(+/- 1.6 \\times IQR\\) to accomodate data upto \\(3\\sigma\\) from the mean to remove the outliers. print ( \"Original shape of dataframe: {} \" . format ( df . shape )) for i in numeric_list : Q25 = np . percentile ( df . loc [:, i ], 25 ) Q75 = np . percentile ( df . loc [:, i ], 75 ) IQR = Q75 - Q25 upper_bound = np . where ( df . loc [:, i ] >= ( Q75 + 1.6 * IQR )) lower_bound = np . where ( df . loc [:, i ] <= ( Q25 - 1.6 * IQR )) df . drop ( upper_bound [ 0 ], inplace = True ) df . drop ( lower_bound [ 0 ], inplace = True ) print ( \"Shape of dataframe after dropping outliers: {} \" . format ( df . shape )) Original shape of dataframe: (303, 9) Shape of dataframe after dropping outliers: (293, 9) Model 2 # Training a Guassian Naive Bayes model with the data (With scaling done to numerical features and removing outliers) # df2 = df . copy () X = df2 . drop ([ \"Target\" ], axis = 1 ) y = df2 [[ \"Target\" ]] Split X and y to training and test data # X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (234, 8) y_train: (234, 1) X_test: (59, 8) y_test: (59, 1) Prediction Analysis: # The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. Scale the numerical features with a standard scaler 2. Remove outliers 1.6 times IQR below and above the Q1 and Q3 respectively 3. Split the data into training and test data with a 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 2 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 2 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 85.92% Precision Score: 84.21% Recall Score: 80.00% F Score: 82.05% Accuracy Score: 76.27% ---------------------------------- Confusion matrix: Finding correlation between features through a heatmap # corr_features = set () corr_matrix = df . corr () plt . figure ( figsize = ( 10 , 8 )) sb . heatmap ( corr_matrix , annot = True , cmap = \"magma\" ) plt . show () for i in range ( len ( corr_matrix . columns )): for j in range ( i ): if abs ( corr_matrix . iloc [ i , j ]) > 0.5 : colname = corr_matrix . columns [ i ] corr_features . add ( colname ) print ( \" \\n ---------------------------------- \\n \" ) print ( \"The number of correlating features: {} \" . format ( len ( corr_features ))) print ( \"The correlating features are: {} \" . format ( corr_features )) print ( \" \\n ---------------------------------- \\n \" ) ---------------------------------- The number of correlating features: 0 The correlating features are: set() ---------------------------------- None of the features seem to correlate with each other and hence we cannot do feature reduction Model 3 # Training a Guassian Naive Bayes model with the data (With scaling done to numerical features, outliers removed and one hot encoding categorical features) # Original dataframe: # df3 = df . copy () df3 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 0.952197 1 3 0.763956 1 0 0.015443 0 1 1 -1.915313 1 2 -0.092738 0 1 1.633471 0 1 2 -1.474158 0 1 -0.092738 0 0 0.977514 0 1 3 0.180175 1 1 -0.663867 0 1 1.239897 0 1 4 0.290464 0 0 -0.663867 0 1 0.583939 1 1 One hot encoded dataframe: # df3 = pd . get_dummies ( df3 , columns = category_list , drop_first = True ) df3 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age BloodPressure MaxHeartRate Target Sex_1 CP_Type_1 CP_Type_2 CP_Type_3 BloodSugar_1 ECG_1 ECG_2 ExerciseAngina_1 0 0.952197 0.763956 0.015443 1 1 0 0 1 1 0 0 0 1 -1.915313 -0.092738 1.633471 1 1 0 1 0 0 1 0 0 2 -1.474158 -0.092738 0.977514 1 0 1 0 0 0 0 0 0 3 0.180175 -0.663867 1.239897 1 1 1 0 0 0 1 0 0 4 0.290464 -0.663867 0.583939 1 0 0 0 0 0 1 0 1 X = df3 . drop ([ \"Target\" ], axis = 1 ) y = df3 [[ \"Target\" ]] Split X and y to training and test data # X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (234, 11) y_train: (234, 1) X_test: (59, 11) y_test: (59, 1) Prediction Analysis: # The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. One hot encode categorical features 2. Scale the numerical features with a standard scaler 3. Remove outliers 1.6 times IQR below and above the Q1 and Q3 respectively 4. Split the data into training and test data with 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 3 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 3 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 90.53% Precision Score: 85.00% Recall Score: 85.00% F Score: 85.00% Accuracy Score: 79.66% ---------------------------------- Confusion matrix: Comparing the results of the three models: # From the results obtained from above prediction analysis, we can tabulate them together below: Model/Metric AUC-ROC Precision Recall F_Score Accuracy Model 1 85.71% 82.50% 82.50% 82.05% 77.05% Model 2 85.92% 84.21% 80.00% 82.05% 76.27% Model 3 90.53% 85.00% 85.00% 85.00% 79.66% This shows that the maximum Accuracy is obtained when scaling is applied on numerical attributes with outliers being dropped and one hot encoding is done to the categorical features (Model 3) . We can also see that Model 2 has lesser accuracy but higher Precision than Model 1 . This shows that Model 1 is overfitted. We know that: \\(Precision = \\frac{True Positive}{True Positive + False Positive}\\) and \\(Recall = \\frac{True Positive}{True Positive + False Negative}\\) Upon viewing the confusion matrix for each model it is clear that lesser false negatives and false positives are recorded in Model 3 in comparison to Model 1 and Model 2 which attributes to the better Precision and Recall scores in Model 3 . Finally, we know that: \\(F = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\) This goes to show why Model 3 has better F score since its recall and precision is better than the other two models. Tags: !AMLIndex Assignments","title":"Applied ML Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#applied-ml-assignment-1","text":"","title":"Applied ML Assignment 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#this-notebook-along-with-its-pdf-version-can-be-found-in-this-github-repo","text":"","title":"(This notebook along with it's PDF version can be found in this Github Repo)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#heart-attack-analaysis","text":"","title":"Heart Attack Analaysis"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#introduction","text":"A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. A blood clot can form and block your arteries, causing a heart attack. This Heart Attack Analysis helps to understand the chance of attack occurrence in persons based on varied health conditions.","title":"Introduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#dataset","text":"The dataset is Heart_Attack_Analysis_Data.csv . It has been uploaded to elearn. This dataset contains data about some hundreds of patients mentioning: - Age - Sex - Exercise Include Angina(1=YES, 0=NO) - CP_Type (Chest Pain) (Value 1: typical angina, Value 2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic) - ECG Results - Blood Pressure - Cholesterol - Blood Sugar - Family History (Number of persons affected in the family) - Maximum Heart Rate - Target (0 = LESS CHANCE , 1 = MORE CHANCE)","title":"Dataset"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#aim","text":"Building a Predictive Model using Na\u00efve Bayesian Approach (Which features decide heart attack?) Comment on the performance of this model using AUC-ROC, Precision, Recall, F_score, Accuracy You need to 1. Preprocess the data to enhance quality 2. Carry out descriptive summarization of data and make observations 3. Identify relevant, irrelevant attributes for building model. 4. Use data visualization tools and make observations 5. Carry out the chosen analytic task. Show results including intermediate results, as needed 6. Evaluate the solution Following are some points for you to take note of, while doing the assignment in Jupyter Notebook: - State all your assumptions clearly - List all intermediate steps and learnings - Mention your observations/findings","title":"Aim"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#submission-plan","text":"The following will be done in this notebook: 1. Verify the datatypes of the values given in the dataset and validate with the information given in the document. 2. Check for invalid values based on domain knowledge by checking if values are present in humanly possible ranges. 3. Figure out which columns are numeric and categorical based on the unique values each column has and based on information given in the assignment document. 4. Check for trends among numerical features and among categorical features to see if feature reduction can be done (via pairplots etc) 5. Check which numerical attributes are relevant and which are irrelevant and drop irrelevant ones. 6. Scale numerical attributes with a standard scaler. 7. Train a gnb MODEL 1 where it is fit with data where only scaling is done to numerical data. 8. Check for outliers via boxplots and remove them with IQR method. 9. Train a gnb MODEL 2 where it is fit with data where scaling is done to numerical data, and outliers are removed using the IQR method. 10. One hot encode categorical data. 11. Train a gnb MODEL 3 where it is fit with data where scaling is done to numerical data, outliers are removed using the IQR method and one hot encoding is done to categorical data. We will then compare the accuracy, Precision, Recall, F-Score and AOC-ROC of three models trained .","title":"Submission Plan"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#my-submission","text":"","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#importing-necessary-packages","text":"import matplotlib.pyplot as plt import numpy as np import pandas as pd import scipy.stats as stats import seaborn as sb import warnings from sklearn.metrics import accuracy_score , recall_score , precision_score , f1_score , confusion_matrix , ConfusionMatrixDisplay , precision_recall_fscore_support , roc_auc_score , roc_curve from sklearn.model_selection import train_test_split from sklearn.naive_bayes import GaussianNB from sklearn.preprocessing import StandardScaler warnings . filterwarnings ( 'ignore' )","title":"Importing necessary packages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#viewing-data","text":"Check the shape of the dataframe loaded into memory from the CSV file and see the datatypes used in the dataset given df = pd . read_csv ( \"./Heart_Attack_Analysis_Data.csv\" ) print ( \"Dataframe Shape: {} \" . format ( df . shape )) print ( \"---------------------------------- \\n \" ) print ( \"With following data types: \\n \" ) df . info () print ( \"---------------------------------- \\n \" ) print ( \"First 5 rows of Dataframe:\" ) df . head () Dataframe Shape: (303, 11) ---------------------------------- With following data types: <class 'pandas.core.frame.DataFrame'> RangeIndex: 303 entries, 0 to 302 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Age 303 non-null int64 1 Sex 303 non-null int64 2 CP_Type 303 non-null int64 3 BloodPressure 303 non-null int64 4 Cholestrol 303 non-null int64 5 BloodSugar 303 non-null int64 6 ECG 303 non-null int64 7 MaxHeartRate 303 non-null int64 8 ExerciseAngina 303 non-null int64 9 FamilyHistory 303 non-null int64 10 Target 303 non-null int64 dtypes: int64(11) memory usage: 26.2 KB ---------------------------------- First 5 rows of Dataframe: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure Cholestrol BloodSugar ECG MaxHeartRate ExerciseAngina FamilyHistory Target 0 63 1 3 145 233 1 0 150 0 2 1 1 37 1 2 130 250 0 1 187 0 1 1 2 41 0 1 130 204 0 0 172 0 0 1 3 56 1 1 120 236 0 1 178 0 1 1 4 57 0 0 120 354 0 1 163 1 0 1 print ( \"Stats on Dataframe:\" ) df . describe () Stats on Dataframe: .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure Cholestrol BloodSugar ECG MaxHeartRate ExerciseAngina FamilyHistory Target count 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 mean 54.366337 0.683168 0.966997 131.623762 246.264026 0.148515 0.528053 149.646865 0.326733 1.204620 0.544554 std 9.082101 0.466011 1.032052 17.538143 51.830751 0.356198 0.525860 22.905161 0.469794 1.096825 0.498835 min 29.000000 0.000000 0.000000 94.000000 126.000000 0.000000 0.000000 71.000000 0.000000 0.000000 0.000000 25% 47.500000 0.000000 0.000000 120.000000 211.000000 0.000000 0.000000 133.500000 0.000000 0.000000 0.000000 50% 55.000000 1.000000 1.000000 130.000000 240.000000 0.000000 1.000000 153.000000 0.000000 1.000000 1.000000 75% 61.000000 1.000000 2.000000 140.000000 274.500000 0.000000 1.000000 166.000000 1.000000 2.000000 1.000000 max 77.000000 1.000000 3.000000 200.000000 564.000000 1.000000 2.000000 202.000000 1.000000 5.000000 1.000000 All columns are integer type and all columns have a value for all rows (303 not null), which means that all values are filled and there are no missing values.","title":"Viewing Data"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#now-to-check-certain-columns-with-humanly-possible-ranges-based-on-domain-knowledge","text":"The following are the assumed ranges for these columns: 1. 0 < Age <= 100 years 2. 90 <= BloodPressure <= 200 3. 60 <= MaxHeartRate <= 220 print ( \" \\n Minimum Age = {} \" . format ( df [ \"Age\" ] . min ())) print ( \"Maximum Age = {} \" . format ( df [ \"Age\" ] . max ())) print ( \" \\n Minimum Blood Pressure = {} \" . format ( df [ \"BloodPressure\" ] . min ())) print ( \"Maximum Blood Pressure = {} \" . format ( df [ \"BloodPressure\" ] . max ())) print ( \" \\n Minimum Heart Rate = {} \" . format ( df [ \"MaxHeartRate\" ] . min ())) print ( \"Maximum Heart Rate = {} \" . format ( df [ \"MaxHeartRate\" ] . max ())) Minimum Age = 29 Maximum Age = 77 Minimum Blood Pressure = 94 Maximum Blood Pressure = 200 Minimum Heart Rate = 71 Maximum Heart Rate = 202 All of the mentioned columns have values within acceptable ranges.","title":"Now to check certain columns with humanly possible ranges based on domain knowledge"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-number-of-unique-values-for-each-column","text":"Column Name -> Unique Number count for column in list ( df . columns ): print ( \" {} -> {} \" . format ( column , df [ column ] . value_counts () . shape [ 0 ])) Age -> 41 Sex -> 2 CP_Type -> 4 BloodPressure -> 49 Cholestrol -> 152 BloodSugar -> 2 ECG -> 3 MaxHeartRate -> 91 ExerciseAngina -> 2 FamilyHistory -> 6 Target -> 2 Taking columns that have a maximum of 4 unique values (And based on information given in assignment document) as categorical and the rest as numeric: category_list = [ \"Sex\" , \"CP_Type\" , \"BloodSugar\" , \"ECG\" , \"ExerciseAngina\" ] numeric_list = [ \"Age\" , \"BloodPressure\" , \"Cholestrol\" , \"MaxHeartRate\" , \"FamilyHistory\" ]","title":"Checking number of unique values for each column"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-for-trends-in-numeric-features-through-pair-plots","text":"df_number = df . loc [:, numeric_list ] df_number [ \"Target\" ] = df [ \"Target\" ] sb . pairplot ( df_number , hue = \"Target\" , palette = \"husl\" ) plt . show () The pair plots do not show any particular trends that can be used to reduce numerical features. We do see a slight relation between age and max heart rate but the plot is scattered enough to not relate them together.","title":"Checking for trends in numeric features through Pair Plots"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-frequncy-of-each-categorical-feature-wrt-target-column-to-check-how-well-it-is-balanced","text":"df_category = df . loc [:, category_list ] df_category [ \"Target\" ] = df [ \"Target\" ] for i in category_list : plt . figure () sb . countplot ( x = i , data = df_category , hue = \"Target\" , palette = \"husl\" ) plt . title ( i )","title":"Checking frequncy of each categorical feature wrt target column to check how well it is balanced"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#here-we-see-that-there-is-very-little-rows-that-have-ecg-value--2-and-similarly-very-little-rows-for-bloodsugar-value--1","text":"","title":"Here we see that there is very little rows that have ECG value = 2 and similarly very little rows for BloodSugar value = 1."},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevant-numerical-features","text":"To do this we shall use the f_oneway function from scipy.stats . This function performs one-way ANOVA(Analysis of Variance) to test the null hypothesis that two groups of data have the same population mean. A feature is only relevant if the sample from a particular feature for a target category is statistically very different from another sample from the same feature for another target category.","title":"Checking relevant numerical features"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevance-of-age","text":"result = stats . f_oneway ( df [ \"Age\" ][ df [ \"Target\" ] == 0 ], df [ \"Age\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 7.524801303442268e-05 The pvalue is < 0.05. This shows that the means of the two distributions (One with Age wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence Age is relevant","title":"Checking relevance of Age"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevance-of-bloodpressure","text":"result = stats . f_oneway ( df [ \"BloodPressure\" ][ df [ \"Target\" ] == 0 ], df [ \"BloodPressure\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.011546059200233376 The pvalue is < 0.05. This shows that the means of the two distributions (One with BloodPressure wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence BloodPressure is relevant","title":"Checking relevance of BloodPressure"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevance-of-cholestrol","text":"result = stats . f_oneway ( df [ \"Cholestrol\" ][ df [ \"Target\" ] == 0 ], df [ \"Cholestrol\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.1387903269560108 The pvalue is > 0.05. This shows that the means of the two distributions (One with Cholestrol wrt less chance of getting heart attack and the other with more chance of getting heart attack) are not significantly different statistically, hence Cholestrol is irrelevant","title":"Checking relevance of Cholestrol"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevance-of-maxheartrate","text":"result = stats . f_oneway ( df [ \"MaxHeartRate\" ][ df [ \"Target\" ] == 0 ], df [ \"MaxHeartRate\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 1.6973376386560805e-14 The pvalue is < 0.05. This shows that the means of the two distributions (One with MaxHeartRate wrt less chance of getting heart attack and the other with more chance of getting heart attack) are significantly different statistically, hence MaxHeartRate is relevant","title":"Checking relevance of MaxHeartRate"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#checking-relevance-of-familyhistory","text":"result = stats . f_oneway ( df [ \"FamilyHistory\" ][ df [ \"Target\" ] == 0 ], df [ \"FamilyHistory\" ][ df [ \"Target\" ] == 1 ]) result . pvalue 0.6172651404419242 The pvalue is > 0.05. This shows that the means of the two distributions (One with FamilyHistory wrt less chance of getting heart attack and the other with more chance of getting heart attack) are not significantly different statistically, hence FamilyHistory is irrelevant","title":"Checking relevance of FamilyHistory"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#dropping-the-irrelevant-features","text":"df . drop ([ \"Cholestrol\" ], axis = 1 , inplace = True ) df . drop ([ \"FamilyHistory\" ], axis = 1 , inplace = True ) numeric_list . remove ( \"Cholestrol\" ) numeric_list . remove ( \"FamilyHistory\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 63 1 3 145 1 0 150 0 1 1 37 1 2 130 0 1 187 0 1 2 41 0 1 130 0 0 172 0 1 3 56 1 1 120 0 1 178 0 1 4 57 0 0 120 0 1 163 1 1","title":"Dropping the irrelevant features"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#scaling-the-numeric-attributes-in-the-dataframe-with-a-standard-scaler","text":"scaler = StandardScaler () df [ numeric_list ] = scaler . fit_transform ( df [ numeric_list ]) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 0.952197 1 3 0.763956 1 0 0.015443 0 1 1 -1.915313 1 2 -0.092738 0 1 1.633471 0 1 2 -1.474158 0 1 -0.092738 0 0 0.977514 0 1 3 0.180175 1 1 -0.663867 0 1 1.239897 0 1 4 0.290464 0 0 -0.663867 0 1 0.583939 1 1","title":"Scaling the numeric attributes in the dataframe with a standard scaler"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#model-1","text":"","title":"Model 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#training-a-guassian-naive-bayes-model-with-the-data-with-scaling-done-to-numerical-features","text":"df1 = df . copy () X = df1 . drop ([ \"Target\" ], axis = 1 ) y = df1 [[ \"Target\" ]]","title":"Training a Guassian Naive Bayes model with the data (With scaling done to numerical features)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#split-x-and-y-to-training-and-test-data","text":"X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (242, 8) y_train: (242, 1) X_test: (61, 8) y_test: (61, 1)","title":"Split X and y to training and test data"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#prediction-analysis","text":"The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. Scale the numerical features with a standard scaler 2. Split the data into training and test data with a 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 1 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_auc = roc_auc_score ( y_test , ns_probs ) ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_auc = roc_auc_score ( y_test , gnb_probs ) gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 1 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 85.71% Precision Score: 82.50% Recall Score: 82.50% F Score: 82.50% Accuracy Score: 77.05% ---------------------------------- Confusion matrix:","title":"Prediction Analysis:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#investigating-dataset-for-outliers","text":"","title":"Investigating dataset for outliers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#analyzing-the-boxplot-for-scaled-numeric-attributes-to-check-for-outliers","text":"plt . figure ( figsize = ( 10 , 8 )) sb . boxplot ( data = df [ numeric_list ], palette = \"husl\" ) plt . show () print ( \" \\n ---------------------------------- \\n \" ) ---------------------------------- We can see some outlier values for blood pressure and max heart rate. we can drop these outliers using the IQR method.","title":"Analyzing the boxplot for scaled numeric attributes to check for outliers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#dropping-outliers-with-iqr-method","text":"Going with \\(+/- 1.6 \\times IQR\\) to accomodate data upto \\(3\\sigma\\) from the mean to remove the outliers. print ( \"Original shape of dataframe: {} \" . format ( df . shape )) for i in numeric_list : Q25 = np . percentile ( df . loc [:, i ], 25 ) Q75 = np . percentile ( df . loc [:, i ], 75 ) IQR = Q75 - Q25 upper_bound = np . where ( df . loc [:, i ] >= ( Q75 + 1.6 * IQR )) lower_bound = np . where ( df . loc [:, i ] <= ( Q25 - 1.6 * IQR )) df . drop ( upper_bound [ 0 ], inplace = True ) df . drop ( lower_bound [ 0 ], inplace = True ) print ( \"Shape of dataframe after dropping outliers: {} \" . format ( df . shape )) Original shape of dataframe: (303, 9) Shape of dataframe after dropping outliers: (293, 9)","title":"Dropping outliers with IQR method"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#model-2","text":"","title":"Model 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#training-a-guassian-naive-bayes-model-with-the-data-with-scaling-done-to-numerical-features-and-removing-outliers","text":"df2 = df . copy () X = df2 . drop ([ \"Target\" ], axis = 1 ) y = df2 [[ \"Target\" ]]","title":"Training a Guassian Naive Bayes model with the data (With scaling done to numerical features and removing outliers)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#split-x-and-y-to-training-and-test-data_1","text":"X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (234, 8) y_train: (234, 1) X_test: (59, 8) y_test: (59, 1)","title":"Split X and y to training and test data"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#prediction-analysis_1","text":"The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. Scale the numerical features with a standard scaler 2. Remove outliers 1.6 times IQR below and above the Q1 and Q3 respectively 3. Split the data into training and test data with a 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 2 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 2 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 85.92% Precision Score: 84.21% Recall Score: 80.00% F Score: 82.05% Accuracy Score: 76.27% ---------------------------------- Confusion matrix:","title":"Prediction Analysis:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#finding-correlation-between-features-through-a-heatmap","text":"corr_features = set () corr_matrix = df . corr () plt . figure ( figsize = ( 10 , 8 )) sb . heatmap ( corr_matrix , annot = True , cmap = \"magma\" ) plt . show () for i in range ( len ( corr_matrix . columns )): for j in range ( i ): if abs ( corr_matrix . iloc [ i , j ]) > 0.5 : colname = corr_matrix . columns [ i ] corr_features . add ( colname ) print ( \" \\n ---------------------------------- \\n \" ) print ( \"The number of correlating features: {} \" . format ( len ( corr_features ))) print ( \"The correlating features are: {} \" . format ( corr_features )) print ( \" \\n ---------------------------------- \\n \" ) ---------------------------------- The number of correlating features: 0 The correlating features are: set() ---------------------------------- None of the features seem to correlate with each other and hence we cannot do feature reduction","title":"Finding correlation between features through a heatmap"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#model-3","text":"","title":"Model 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#training-a-guassian-naive-bayes-model-with-the-data-with-scaling-done-to-numerical-features-outliers-removed-and-one-hot-encoding-categorical-features","text":"","title":"Training a Guassian Naive Bayes model with the data (With scaling done to numerical features, outliers removed and one hot encoding categorical features)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#original-dataframe","text":"df3 = df . copy () df3 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age Sex CP_Type BloodPressure BloodSugar ECG MaxHeartRate ExerciseAngina Target 0 0.952197 1 3 0.763956 1 0 0.015443 0 1 1 -1.915313 1 2 -0.092738 0 1 1.633471 0 1 2 -1.474158 0 1 -0.092738 0 0 0.977514 0 1 3 0.180175 1 1 -0.663867 0 1 1.239897 0 1 4 0.290464 0 0 -0.663867 0 1 0.583939 1 1","title":"Original dataframe:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#one-hot-encoded-dataframe","text":"df3 = pd . get_dummies ( df3 , columns = category_list , drop_first = True ) df3 . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Age BloodPressure MaxHeartRate Target Sex_1 CP_Type_1 CP_Type_2 CP_Type_3 BloodSugar_1 ECG_1 ECG_2 ExerciseAngina_1 0 0.952197 0.763956 0.015443 1 1 0 0 1 1 0 0 0 1 -1.915313 -0.092738 1.633471 1 1 0 1 0 0 1 0 0 2 -1.474158 -0.092738 0.977514 1 0 1 0 0 0 0 0 0 3 0.180175 -0.663867 1.239897 1 1 1 0 0 0 1 0 0 4 0.290464 -0.663867 0.583939 1 0 0 0 0 0 1 0 1 X = df3 . drop ([ \"Target\" ], axis = 1 ) y = df3 [[ \"Target\" ]]","title":"One hot encoded dataframe:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#split-x-and-y-to-training-and-test-data_2","text":"X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.20 , random_state = 3 ) print ( \"X_train: {} \" . format ( X_train . shape )) print ( \"y_train: {} \" . format ( y_train . shape )) print ( \"X_test: {} \" . format ( X_test . shape )) print ( \"y_test: {} \" . format ( y_test . shape )) X_train: (234, 11) y_train: (234, 1) X_test: (59, 11) y_test: (59, 1)","title":"Split X and y to training and test data"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#prediction-analysis_2","text":"The following is the analysis for a naive bayes model that was trained with the following done on the data: 1. One hot encode categorical features 2. Scale the numerical features with a standard scaler 3. Remove outliers 1.6 times IQR below and above the Q1 and Q3 respectively 4. Split the data into training and test data with 20% test data gnb = GaussianNB () gnb . fit ( X_train , y_train ) y_pred = gnb . predict ( X_test ) print ( \"Model 3 Results:\" ) print ( \"---------------------------------- \\n \" ) ns_probs = [ 0 for _ in range ( len ( y_test ))] ns_fpr , ns_tpr , _ = roc_curve ( y_test , ns_probs ) y_probs = gnb . predict_proba ( X_test ) gnb_probs = y_probs [:, 1 ] gnb_fpr , gnb_tpr , temp = roc_curve ( y_test , gnb_probs ) plt . plot ( ns_fpr , ns_tpr , linestyle = '--' , label = 'No Skill' ) plt . plot ( gnb_fpr , gnb_tpr , marker = '.' , label = 'Gaussian NB' ) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . legend () plt . show () print ( \"---------------------------------- \\n \" ) print ( \"AUC-ROC Score: {0:0.2f} %\" . format ( roc_auc_score ( y_test , gnb_probs ) * 100 )) print ( \"Precision Score: {0:0.2f} %\" . format ( precision_score ( y_test , y_pred ) * 100 )) print ( \"Recall Score: {0:0.2f} %\" . format ( recall_score ( y_test , y_pred ) * 100 )) print ( \"F Score: {0:0.2f} %\" . format ( f1_score ( y_test , y_pred ) * 100 )) print ( \"Accuracy Score: {0:0.2f} %\" . format ( accuracy_score ( y_test , y_pred ) * 100 )) print ( \" \\n ---------------------------------- \\n \" ) print ( \"Confusion matrix:\" ) cm = confusion_matrix ( y_pred , y_test ) ConfusionMatrixDisplay ( cm , display_labels = [ \"Less Chance\" , \"More Chance\" ]) . plot () plt . show () Model 3 Results: ---------------------------------- ---------------------------------- AUC-ROC Score: 90.53% Precision Score: 85.00% Recall Score: 85.00% F Score: 85.00% Accuracy Score: 79.66% ---------------------------------- Confusion matrix:","title":"Prediction Analysis:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/AMLAssignment1.html#comparing-the-results-of-the-three-models","text":"From the results obtained from above prediction analysis, we can tabulate them together below: Model/Metric AUC-ROC Precision Recall F_Score Accuracy Model 1 85.71% 82.50% 82.50% 82.05% 77.05% Model 2 85.92% 84.21% 80.00% 82.05% 76.27% Model 3 90.53% 85.00% 85.00% 85.00% 79.66% This shows that the maximum Accuracy is obtained when scaling is applied on numerical attributes with outliers being dropped and one hot encoding is done to the categorical features (Model 3) . We can also see that Model 2 has lesser accuracy but higher Precision than Model 1 . This shows that Model 1 is overfitted. We know that: \\(Precision = \\frac{True Positive}{True Positive + False Positive}\\) and \\(Recall = \\frac{True Positive}{True Positive + False Negative}\\) Upon viewing the confusion matrix for each model it is clear that lesser false negatives and false positives are recorded in Model 3 in comparison to Model 1 and Model 2 which attributes to the better Precision and Recall scores in Model 3 . Finally, we know that: \\(F = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\) This goes to show why Model 3 has better F score since its recall and precision is better than the other two models. Tags: !AMLIndex Assignments","title":"Comparing the results of the three models:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataCleaningNotebook.html","text":"Notebook for data cleaning # Reference: https://www.justintodata.com/data-cleaning-techniques-python-guide/#what-is-data-cleaning-and-why-is-it-important Data Cleaning : Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Methods & techniques in Python on how to find and clean: Missing data Irregular data (outliers) Unnecessary data \u2014 repetitive data, duplicates, and more Inconsistent data \u2014 capitalization, data types, typos, addresses Raw data is always messy, may suffer from various quality issues. We can't use it as it is. If you use such data for analysis, for example, feed into a machine learning model, you\u2019ll get useless insights most of the time. That\u2019s why data cleansing is a critical process for data analysts and data scientists. Useful Python Libraries pandas: a popular data analysis and manipulation tool, which will be used for most of our data cleaning techniques seaborn: statistical data visualization library Missingno Python library that provides a series of visualisations to understand the presence and distribution of missing data within a pandas dataframe. nltk: natural language toolkit Case Study --> Russian housing market dataset The goal of the project is to predict housing prices. import pandas as pd df = pd . read_csv ( \"sberbank-russian-housing-market/train/train.csv\" ) df . head () df . shape (30471, 292) There are 292 columns and 30471 rows in the data set # to check datatypes df . info () RangeIndex: 30471 entries, 0 to 30470 Columns: 292 entries, id to price_doc dtypes: float64(119), int64(157), object(16) memory usage: 67.9+ MB # to identify numeric and non-numeric attributes in the dataset numeric_cols = df . select_dtypes ( include = [ 'number' ]) . columns non_numeric_cols = df . select_dtypes ( exclude = [ 'number' ]) . columns numeric_cols Index(['id', 'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', ... 'cafe_count_5000_price_2500', 'cafe_count_5000_price_4000', 'cafe_count_5000_price_high', 'big_church_count_5000', 'church_count_5000', 'mosque_count_5000', 'leisure_count_5000', 'sport_count_5000', 'market_count_5000', 'price_doc'], dtype='object', length=276) non_numeric_cols Index(['timestamp', 'product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology'], dtype='object') # Missing Data When data is missing for a column in a row. Ways to handle it: How to find out? Method #1: missing data (by columns) count & percentage df [ non_numeric_cols ] . info () RangeIndex: 30471 entries, 0 to 30470 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 30471 non-null object 1 product_type 30471 non-null object 2 sub_area 30471 non-null object 3 culture_objects_top_25 30471 non-null object 4 thermal_power_plant_raion 30471 non-null object 5 incineration_raion 30471 non-null object 6 oil_chemistry_raion 30471 non-null object 7 radiation_raion 30471 non-null object 8 railroad_terminal_raion 30471 non-null object 9 big_market_raion 30471 non-null object 10 nuclear_reactor_raion 30471 non-null object 11 detention_facility_raion 30471 non-null object 12 water_1line 30471 non-null object 13 big_road1_1line 30471 non-null object 14 railroad_1line 30471 non-null object 15 ecology 30471 non-null object dtypes: object(16) memory usage: 3.7+ MB all counts are same for non-numeric columns, hence no missing data num_missing = df . isna () . sum () num_missing [: 10 ] id 0 timestamp 0 full_sq 0 life_sq 6383 floor 167 max_floor 9572 material 9572 build_year 13605 num_room 9572 kitch_sq 9572 dtype: int64 # to caclulate mean of missing values by columns num_missing = df . isna () . mean () num_missing [: 10 ] id 0.000000 timestamp 0.000000 full_sq 0.000000 life_sq 0.209478 floor 0.005481 max_floor 0.314135 material 0.314135 build_year 0.446490 num_room 0.314135 kitch_sq 0.314135 dtype: float64 Method #2: missing data (by columns) heatmap # to visualise missing values # heapmap can be created using \"seaborn\" and \"missingno\" libraries import seaborn as sns # since number of columns are very large, it would be difficult to visualise them at the same time. We can learn the patter # pattern of missing data for the first 30 columns cols = df . columns [: 30 ] colors = [ \"green\" , \"blue\" ] sns . heatmap ( df [ cols ] . isna (), cmap = sns . color_palette ( colors )) ![[Assets/DataVisualizationNotebook/output_19_1.png]] the column life_sq has missing values across different rows. While the column max_floor has most of its missing values # missingno The missingno library is a small toolset focused on missing data visualizations and utilities. So you can get the same missing data heatmap as above with shorter code. ! pip install missingno import missingno as msno msno . matrix ( df . iloc [:, : 30 ]) Requirement already satisfied: missingno in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (0.5.0) Requirement already satisfied: matplotlib in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (3.2.2) Requirement already satisfied: numpy in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (1.18.5) Requirement already satisfied: scipy in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (1.5.0) Requirement already satisfied: seaborn in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (0.10.1) Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.4.7) Requirement already satisfied: cycler>=0.10 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (0.10.0) Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.8.1) Requirement already satisfied: pandas>=0.22.0 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from seaborn->missingno) (1.0.5) Requirement already satisfied: six in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->missingno) (1.15.0) Requirement already satisfied: pytz>=2017.2 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->seaborn->missingno) (2020.1) ![output_22_2.png](../../../Assets/DataCleaningNotebook/output_22_2.png) Method #3: missing data (by rows) histogram summarize the missing data by rows. missing_by_row = df . isna () . sum ( axis = 'columns' ) missing_by_row . hist ( bins = 50 ) ![[Assets/DataVisualizationNotebook/output_24_1.png]] This histogram helps to identify the missing patterns among the 30,471 observations. For example, there are over 6,000 observations with no missing values, and close to 4,000 observations with 1 missing value. How to handle missing data Methods: Technique #1: drop columns / features: drop the entire column or features with large number of missing values. But, this will cause a loss of information. Let\u2019s consider the columns with a high percentage of missing. num_missing [ num_missing > 0.3 ] max_floor 0.314135 material 0.314135 build_year 0.446490 num_room 0.314135 kitch_sq 0.314135 state 0.444980 hospital_beds_raion 0.473926 cafe_sum_500_min_price_avg 0.435857 cafe_sum_500_max_price_avg 0.435857 cafe_avg_price_500 0.435857 dtype: float64 df . drop ([ 'max_floor' , 'material' , 'build_year' , 'num_room' , 'kitch_sq' , 'state' , 'hospital_beds_raion' , 'cafe_sum_500_min_price_avg' , 'cafe_sum_500_max_price_avg' , 'cafe_avg_price_500' ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 1 2011-08-20 43 27.0 4.0 NaN NaN NaN NaN NaN ... 9 4 0 13 22 1 0 52 4 5850000 1 2 2011-08-23 34 19.0 3.0 NaN NaN NaN NaN NaN ... 15 3 0 15 29 1 10 66 14 6000000 2 3 2011-08-27 43 29.0 2.0 NaN NaN NaN NaN NaN ... 10 3 0 11 27 0 4 67 10 5700000 3 4 2011-09-01 89 50.0 9.0 NaN NaN NaN NaN NaN ... 11 2 1 4 4 0 0 26 3 13100000 4 5 2011-09-05 77 77.0 4.0 NaN NaN NaN NaN NaN ... 319 108 17 135 236 2 91 195 14 16331452 5 rows \u00d7 292 columns Technique #2: drop rows / observations df . dropna ( axis = 0 , how = 'any' , thresh = 257 , subset = None , inplace = True ) # thresh argument that specifies the number of non-missing values that should be present for each row in order not to be dropped. df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 1 2011-08-20 43 27.0 4.0 NaN NaN NaN NaN NaN ... 9 4 0 13 22 1 0 52 4 5850000 1 2 2011-08-23 34 19.0 3.0 NaN NaN NaN NaN NaN ... 15 3 0 15 29 1 10 66 14 6000000 2 3 2011-08-27 43 29.0 2.0 NaN NaN NaN NaN NaN ... 10 3 0 11 27 0 4 67 10 5700000 3 4 2011-09-01 89 50.0 9.0 NaN NaN NaN NaN NaN ... 11 2 1 4 4 0 0 26 3 13100000 4 5 2011-09-05 77 77.0 4.0 NaN NaN NaN NaN NaN ... 319 108 17 135 236 2 91 195 14 16331452 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 30466 30469 2015-06-30 44 27.0 7.0 9.0 1.0 1975.0 2.0 6.0 ... 15 5 0 15 26 1 2 84 6 7400000 30467 30470 2015-06-30 86 59.0 3.0 9.0 2.0 1935.0 4.0 10.0 ... 313 128 24 98 182 1 82 171 15 25000000 30468 30471 2015-06-30 45 NaN 10.0 20.0 1.0 NaN 1.0 1.0 ... 1 1 0 2 12 0 1 11 1 6970959 30469 30472 2015-06-30 64 32.0 5.0 15.0 1.0 2003.0 2.0 11.0 ... 22 1 1 6 31 1 4 65 7 13500000 30470 30473 2015-06-30 43 28.0 1.0 9.0 1.0 1968.0 2.0 6.0 ... 5 2 0 7 16 0 9 54 10 5600000 29779 rows \u00d7 292 columns Technique #3: impute the missing with constant values. df_copy = df . copy () numeric_cols = df_copy . select_dtypes ( include = [ 'number' ]) . columns df_copy [ numeric_cols ] = df_copy [ numeric_cols ] . fillna ( - 999 ) df_copy [ non_numeric_cols ] = df_copy [ non_numeric_cols ] . fillna ( '_MISSING_' ) Technique #4: impute the missing with statistics #imputing numeric columns by median med = df_copy [ numeric_cols ] . median () df_copy [ numeric_cols ] = df_copy [ numeric_cols ] . fillna ( med ) # imputing non-numeric columns by mode most_freq = df_copy [ non_numeric_cols ] . mode () df_copy [ non_numeric_cols ] = df_copy [ non_numeric_cols ] . fillna ( most_freq ) most_freq .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp product_type sub_area culture_objects_top_25 thermal_power_plant_raion incineration_raion oil_chemistry_raion radiation_raion railroad_terminal_raion big_market_raion nuclear_reactor_raion detention_facility_raion water_1line big_road1_1line railroad_1line ecology 0 2014-12-16 Investment Poselenie Sosenskoe no no no no no no no no no no no no poor #to check for missing values df_copy . isna () . sum () id 0 timestamp 0 full_sq 0 life_sq 0 floor 0 .. mosque_count_5000 0 leisure_count_5000 0 sport_count_5000 0 market_count_5000 0 price_doc 0 Length: 292, dtype: int64 # Irregular data (outliers) Outliers could bias our data analysis results, providing a misleading representation of the data. Outliers could be real outliers or mistakes. How to detect? Method #1: descriptive statistics df_copy . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id full_sq life_sq floor max_floor material build_year num_room kitch_sq state ... big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc year month weekday count 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 ... 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 29779.000000 29779.000000 29779.000000 mean 3.927294e-17 1.133705e-15 -6.050975e-16 -3.694486e-15 -3.424010e-14 2.521536e-16 6.337155e-16 1.510536e-13 -2.153204e-13 1.240557e-13 ... 1.864436e-15 -1.152501e-16 -2.969346e-15 1.061662e-15 -3.434236e-16 3.142879e-17 3.015190e-15 2013.453105 6.744014 2.196783 std 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 ... 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 0.962059 3.523263 1.576159 min -1.733558e+00 -1.413996e+00 -2.004501e+00 -1.344496e+01 -1.484404e+00 -1.484503e+00 -1.975412e-02 -1.484507e+00 -1.482597e+00 -1.130673e+00 ... -5.234219e-01 -6.453354e-01 -7.320273e-01 -4.259748e-01 -1.165510e+00 -1.251928e+00 -1.477355e+00 2011.000000 1.000000 0.000000 25% -8.634904e-01 -4.234363e-01 4.506506e-01 1.039594e-02 -1.484404e+00 -1.484503e+00 -1.975412e-02 -1.484507e+00 -1.482597e+00 -1.130673e+00 ... -4.553321e-01 -4.568299e-01 -7.320273e-01 -4.259748e-01 -9.059867e-01 -1.046875e+00 -4.935775e-01 2013.000000 4.000000 1.000000 50% 1.914127e-03 -1.366954e-01 4.748155e-01 6.410995e-02 6.657718e-01 6.718609e-01 5.709977e-03 6.716512e-01 6.609854e-01 8.821739e-01 ... -2.851076e-01 -3.102146e-01 -7.320273e-01 -3.297071e-01 -1.057884e-01 -2.266611e-01 -1.863383e-01 2014.000000 6.000000 2.000000 75% 8.653282e-01 2.282476e-01 5.013968e-01 1.178240e-01 6.807036e-01 6.718609e-01 5.916512e-03 6.738074e-01 6.759905e-01 8.841867e-01 ... -1.148831e-01 -5.887390e-02 9.015939e-01 -8.903773e-02 4.781401e-01 1.003659e+00 2.398021e-01 2014.000000 10.000000 3.000000 max 1.732382e+00 1.374207e+02 1.848007e+01 1.004105e+00 8.961478e-01 6.826427e-01 1.725493e+02 7.104621e-01 4.976016e+00 9.465850e-01 ... 4.617358e+00 4.590928e+00 2.535215e+00 4.676216e+00 3.549171e+00 3.054193e+00 2.163833e+01 2015.000000 12.000000 6.000000 8 rows \u00d7 279 columns Method #2: histogram & box plot df_copy [ 'full_sq' ] . hist ( bins = 100 ) ![output_43_1.png](../../../Assets/DataCleaningNotebook/output_43_1.png) sns . boxplot ( df_copy [ 'full_sq' ]) ![output_44_1.png](../../../Assets/DataCleaningNotebook/output_44_1.png) Method #3: bar chart # to check outliers in categorical attributes df [ 'ecology' ] . value_counts () . plot ( kind = 'bar' ) # But if there is a category with only one value called \u2018extraordinary\u2019, that could be considered an \u2018outlier\u2019. ![output_46_1.png](../../../Assets/DataCleaningNotebook/output_46_1.png) How to handle missing data Unnecessary data Unnecessary type #1: repetitive & uninformative When an extremely high percentage of the column has a repetitive value, num_rows = len ( df_copy ) for col in df_copy . columns : cnts = df_copy [ col ] . value_counts ( dropna = False ) top_pct = ( cnts / num_rows ) . iloc [ 0 ] if top_pct > 0.999 : print ( ' {0} : {1:.2f} %' . format ( col , top_pct * 100 )) print ( cnts ) print () Unnecessary type #2: irrelevant If features are not related to the question we are trying to solve, they are irrelevant. Use \"drop()\" method to drop such features Unnecessary type #3: duplicates duplicate occurs when all the columns\u2019 values within the observations are the same. df_copy [ df_copy . duplicated ()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 rows \u00d7 292 columns #We first drop id, and then see if there are duplicated rows from the DataFrame df_copy [ df_copy . drop ( columns = [ 'id' ]) . duplicated ()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 4328 4331 2012-10-22 61 -999.0 18.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 11 2 1 5 4 0 1 32 5 8248500 6991 6994 2013-04-03 42 -999.0 2.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 3 2 0 2 16 1 0 20 4 3444000 8059 8062 2013-05-22 68 -999.0 2.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 3 2 0 2 16 1 0 20 4 5406690 8653 8656 2013-06-24 40 -999.0 12.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 1 0 0 4 6 0 0 4 1 4112800 14004 14007 2014-01-22 46 28.0 1.0 9.0 1.0 1968.0 2.0 5.0 ... 10 1 0 13 15 1 1 61 4 3000000 17404 17407 2014-04-15 134 134.0 1.0 1.0 1.0 0.0 3.0 0.0 ... 0 0 0 0 1 0 0 0 0 5798496 26675 26678 2014-12-17 62 -999.0 9.0 17.0 1.0 -999.0 2.0 1.0 ... 371 141 26 150 249 2 105 203 13 6552000 28361 28364 2015-03-14 62 -999.0 2.0 17.0 1.0 -999.0 2.0 1.0 ... 371 141 26 150 249 2 105 203 13 6520500 28712 28715 2015-03-30 41 41.0 11.0 17.0 1.0 2016.0 1.0 41.0 ... 2 2 0 2 9 0 0 7 2 4114580 9 rows \u00d7 292 columns df_dedupped = df . drop ( columns = [ 'id' ]) . drop_duplicates () print ( df . shape ) print ( df_dedupped . shape ) (29779, 292) (29770, 291) # Inconsistent data Inconsistent type #1: capitalization Inconsistent use of upper and lower cases in categorical values is typical. We need to clean it since Python is case-sensitive. # to print full numpy array without ... #import numpy as np #import sys #np.set_printoptions(threshold=sys.maxsize) # to display the entire list without ... with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): # more options can be specified also display ( df_copy [ 'sub_area' ] . value_counts ( dropna = False )) Poselenie Sosenskoe 1617 Nekrasovka 1611 Poselenie Vnukovskoe 1290 Poselenie Moskovskij 925 Poselenie Voskresenskoe 713 Mitino 679 Tverskoe 678 Krjukovo 518 Mar'ino 508 Juzhnoe Butovo 451 Poselenie Shherbinka 443 Solncevo 421 Zapadnoe Degunino 410 Poselenie Desjonovskoe 362 Otradnoe 353 Nagatinskij Zaton 327 Nagornoe 305 Bogorodskoe 305 Strogino 301 Izmajlovo 300 Tekstil'shhiki 298 Ljublino 297 Gol'janovo 295 Severnoe Tushino 282 Chertanovo Juzhnoe 273 Birjulevo Vostochnoe 268 Vyhino-Zhulebino 264 Horoshevo-Mnevniki 262 Zjuzino 259 Ochakovo-Matveevskoe 255 Perovo 247 Ramenki 241 Jasenevo 237 Kosino-Uhtomskoe 237 Bibirevo 230 Golovinskoe 224 Poselenie Filimonkovskoe 221 Caricyno 220 Kuz'minki 220 Kon'kovo 220 Veshnjaki 213 Akademicheskoe 211 Orehovo-Borisovo Juzhnoe 208 Koptevo 207 Orehovo-Borisovo Severnoe 206 Novogireevo 201 Chertanovo Severnoe 200 Danilovskoe 199 Ivanovskoe 197 Mozhajskoe 197 Chertanovo Central'noe 196 Pechatniki 192 Presnenskoe 190 Sokolinaja Gora 188 Obruchevskoe 185 Kuncevo 184 Brateevo 182 Severnoe Butovo 182 Rjazanskij 180 Hovrino 178 Losinoostrovskoe 177 Juzhnoe Tushino 175 Dmitrovskoe 174 Taganskoe 173 Severnoe Medvedkovo 167 Beskudnikovskoe 166 Teplyj Stan 165 Pokrovskoe Streshnevo 164 Severnoe Izmajlovo 163 Cheremushki 158 Nagatino-Sadovniki 158 Troickij okrug 158 Shhukino 155 Timirjazevskoe 154 Vostochnoe Izmajlovo 154 Preobrazhenskoe 152 Novo-Peredelkino 149 Filevskij Park 148 Lomonosovskoe 147 Kotlovka 147 Juzhnoe Medvedkovo 143 Poselenie Pervomajskoe 142 Novokosino 139 Fili Davydkovo 137 Horoshevskoe 136 Levoberezhnoe 135 Donskoe 135 Vojkovskoe 131 Sviblovo 131 Zjablikovo 127 Troparevo-Nikulino 126 Lianozovo 126 Juzhnoportovoe 126 Ajeroport 123 Babushkinskoe 123 Jaroslavskoe 121 Lefortovo 119 Vostochnoe Degunino 118 Mar'ina Roshha 116 Birjulevo Zapadnoe 115 Matushkino 111 Savelki 105 Krylatskoe 103 Butyrskoe 101 Silino 100 Prospekt Vernadskogo 100 Alekseevskoe 100 Moskvorech'e-Saburovo 99 Basmannoe 98 Meshhanskoe 94 Staroe Krjukovo 92 Hamovniki 90 Savelovskoe 85 Marfino 85 Jakimanka 81 Ostankinskoe 79 Gagarinskoe 79 Nizhegorodskoe 77 Sokol 72 Altuf'evskoe 68 Rostokino 64 Kurkino 62 Sokol'niki 60 Begovoe 60 Metrogorodok 58 Dorogomilovo 56 Zamoskvorech'e 50 Kapotnja 49 Vnukovo 44 Krasnosel'skoe 37 Severnoe 37 Poselenie Rogovskoe 30 Poselenie Rjazanovskoe 26 Poselenie Kokoshkino 20 Poselenie Mosrentgen 19 Poselenie Krasnopahorskoe 19 Arbat 15 Vostochnoe 7 Poselenie Marushkinskoe 6 Molzhaninovskoe 3 Poselenie Voronovskoe 2 Name: sub_area, dtype: int64 \u2018Poselenie Sosenskoe\u2019 and \u2018pOseleNie sosenskeo\u2019 could refer to the same district. How to handle? To avoid this, we can lowercase (or uppercase) all letters. df_copy [ 'sub_area_lower' ] = df_copy [ 'sub_area' ] . str . lower () df_copy [ 'sub_area_lower' ] . value_counts ( dropna = False ) poselenie sosenskoe 1617 nekrasovka 1611 poselenie vnukovskoe 1290 poselenie moskovskij 925 poselenie voskresenskoe 713 ... arbat 15 vostochnoe 7 poselenie marushkinskoe 6 molzhaninovskoe 3 poselenie voronovskoe 2 Name: sub_area_lower, Length: 141, dtype: int64 Inconsistent type #2: typos of categorical values A categorical column takes on a limited and usually fixed number of possible values. Sometimes it shows other values due to reasons like typos. How to find out? Let\u2019s see an example. Within the code below: We generate a new DataFrame, df_city_ex There is only one column that stores the city names. There are misspellings. For example, \u2018torontoo\u2019 and \u2018tronto\u2019 both refer to the city of \u2018toronto\u2019. The variable cities stores the 4 correct names of \u2018toronto\u2019, \u2018vancouver\u2019, \u2018montreal\u2019, and \u2018calgary\u2019. To identify typos, we use fuzzy logic matches. We use edit_distance from nltk, which measures the number of operations (e.g., substitution, insertion, deletion) needed to change from one string into another string. We calculate the distance between the actual values and the correct values. # Levenshtein Distance, Hamming distance df_city_ex = pd . DataFrame ( data = { 'city' : [ 'torontooo' , 'toronto' , 'turonto' , 'vancouver' , 'vancover' , 'vancouvr' , 'montreal' , 'calgary' ]}) df_city_ex [ 'city' ] . value_counts () vancouvr 1 montreal 1 vancover 1 turonto 1 calgary 1 vancouver 1 torontooo 1 toronto 1 Name: city, dtype: int64 #!pip install pyspellchecker from spellchecker import SpellChecker spell = SpellChecker () i = 0 for city in df_city_ex [ 'city' ]: # Get the one `most likely` answer df_city_ex . at [ i , 'city' ] = spell . correction ( city ) i = i + 1 #print(spell.correction(city)) df_city_ex [ 'city' ] 0 toronto 1 toronto 2 toronto 3 vancouver 4 vancouver 5 vancouver 6 montreal 7 calgary Name: city, dtype: object df_city_ex [ 'city' ] . value_counts () vancouver 3 toronto 3 montreal 1 calgary 1 Name: city, dtype: int64 --- Tags: [!AMLIndex](./!AMLIndex.md)","title":"Notebook for data cleaning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataCleaningNotebook.html#notebook-for-data-cleaning","text":"Reference: https://www.justintodata.com/data-cleaning-techniques-python-guide/#what-is-data-cleaning-and-why-is-it-important Data Cleaning : Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Methods & techniques in Python on how to find and clean: Missing data Irregular data (outliers) Unnecessary data \u2014 repetitive data, duplicates, and more Inconsistent data \u2014 capitalization, data types, typos, addresses Raw data is always messy, may suffer from various quality issues. We can't use it as it is. If you use such data for analysis, for example, feed into a machine learning model, you\u2019ll get useless insights most of the time. That\u2019s why data cleansing is a critical process for data analysts and data scientists. Useful Python Libraries pandas: a popular data analysis and manipulation tool, which will be used for most of our data cleaning techniques seaborn: statistical data visualization library Missingno Python library that provides a series of visualisations to understand the presence and distribution of missing data within a pandas dataframe. nltk: natural language toolkit Case Study --> Russian housing market dataset The goal of the project is to predict housing prices. import pandas as pd df = pd . read_csv ( \"sberbank-russian-housing-market/train/train.csv\" ) df . head () df . shape (30471, 292) There are 292 columns and 30471 rows in the data set # to check datatypes df . info () RangeIndex: 30471 entries, 0 to 30470 Columns: 292 entries, id to price_doc dtypes: float64(119), int64(157), object(16) memory usage: 67.9+ MB # to identify numeric and non-numeric attributes in the dataset numeric_cols = df . select_dtypes ( include = [ 'number' ]) . columns non_numeric_cols = df . select_dtypes ( exclude = [ 'number' ]) . columns numeric_cols Index(['id', 'full_sq', 'life_sq', 'floor', 'max_floor', 'material', 'build_year', 'num_room', 'kitch_sq', 'state', ... 'cafe_count_5000_price_2500', 'cafe_count_5000_price_4000', 'cafe_count_5000_price_high', 'big_church_count_5000', 'church_count_5000', 'mosque_count_5000', 'leisure_count_5000', 'sport_count_5000', 'market_count_5000', 'price_doc'], dtype='object', length=276) non_numeric_cols Index(['timestamp', 'product_type', 'sub_area', 'culture_objects_top_25', 'thermal_power_plant_raion', 'incineration_raion', 'oil_chemistry_raion', 'radiation_raion', 'railroad_terminal_raion', 'big_market_raion', 'nuclear_reactor_raion', 'detention_facility_raion', 'water_1line', 'big_road1_1line', 'railroad_1line', 'ecology'], dtype='object') # Missing Data When data is missing for a column in a row. Ways to handle it: How to find out? Method #1: missing data (by columns) count & percentage df [ non_numeric_cols ] . info () RangeIndex: 30471 entries, 0 to 30470 Data columns (total 16 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 timestamp 30471 non-null object 1 product_type 30471 non-null object 2 sub_area 30471 non-null object 3 culture_objects_top_25 30471 non-null object 4 thermal_power_plant_raion 30471 non-null object 5 incineration_raion 30471 non-null object 6 oil_chemistry_raion 30471 non-null object 7 radiation_raion 30471 non-null object 8 railroad_terminal_raion 30471 non-null object 9 big_market_raion 30471 non-null object 10 nuclear_reactor_raion 30471 non-null object 11 detention_facility_raion 30471 non-null object 12 water_1line 30471 non-null object 13 big_road1_1line 30471 non-null object 14 railroad_1line 30471 non-null object 15 ecology 30471 non-null object dtypes: object(16) memory usage: 3.7+ MB all counts are same for non-numeric columns, hence no missing data num_missing = df . isna () . sum () num_missing [: 10 ] id 0 timestamp 0 full_sq 0 life_sq 6383 floor 167 max_floor 9572 material 9572 build_year 13605 num_room 9572 kitch_sq 9572 dtype: int64 # to caclulate mean of missing values by columns num_missing = df . isna () . mean () num_missing [: 10 ] id 0.000000 timestamp 0.000000 full_sq 0.000000 life_sq 0.209478 floor 0.005481 max_floor 0.314135 material 0.314135 build_year 0.446490 num_room 0.314135 kitch_sq 0.314135 dtype: float64 Method #2: missing data (by columns) heatmap # to visualise missing values # heapmap can be created using \"seaborn\" and \"missingno\" libraries import seaborn as sns # since number of columns are very large, it would be difficult to visualise them at the same time. We can learn the patter # pattern of missing data for the first 30 columns cols = df . columns [: 30 ] colors = [ \"green\" , \"blue\" ] sns . heatmap ( df [ cols ] . isna (), cmap = sns . color_palette ( colors )) ![[Assets/DataVisualizationNotebook/output_19_1.png]] the column life_sq has missing values across different rows. While the column max_floor has most of its missing values # missingno The missingno library is a small toolset focused on missing data visualizations and utilities. So you can get the same missing data heatmap as above with shorter code. ! pip install missingno import missingno as msno msno . matrix ( df . iloc [:, : 30 ]) Requirement already satisfied: missingno in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (0.5.0) Requirement already satisfied: matplotlib in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (3.2.2) Requirement already satisfied: numpy in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (1.18.5) Requirement already satisfied: scipy in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (1.5.0) Requirement already satisfied: seaborn in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from missingno) (0.10.1) Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (1.2.0) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.4.7) Requirement already satisfied: cycler>=0.10 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (0.10.0) Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from matplotlib->missingno) (2.8.1) Requirement already satisfied: pandas>=0.22.0 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from seaborn->missingno) (1.0.5) Requirement already satisfied: six in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->missingno) (1.15.0) Requirement already satisfied: pytz>=2017.2 in c:\\users\\bits-wilp\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->seaborn->missingno) (2020.1) ![output_22_2.png](../../../Assets/DataCleaningNotebook/output_22_2.png) Method #3: missing data (by rows) histogram summarize the missing data by rows. missing_by_row = df . isna () . sum ( axis = 'columns' ) missing_by_row . hist ( bins = 50 ) ![[Assets/DataVisualizationNotebook/output_24_1.png]] This histogram helps to identify the missing patterns among the 30,471 observations. For example, there are over 6,000 observations with no missing values, and close to 4,000 observations with 1 missing value. How to handle missing data Methods: Technique #1: drop columns / features: drop the entire column or features with large number of missing values. But, this will cause a loss of information. Let\u2019s consider the columns with a high percentage of missing. num_missing [ num_missing > 0.3 ] max_floor 0.314135 material 0.314135 build_year 0.446490 num_room 0.314135 kitch_sq 0.314135 state 0.444980 hospital_beds_raion 0.473926 cafe_sum_500_min_price_avg 0.435857 cafe_sum_500_max_price_avg 0.435857 cafe_avg_price_500 0.435857 dtype: float64 df . drop ([ 'max_floor' , 'material' , 'build_year' , 'num_room' , 'kitch_sq' , 'state' , 'hospital_beds_raion' , 'cafe_sum_500_min_price_avg' , 'cafe_sum_500_max_price_avg' , 'cafe_avg_price_500' ], axis = 1 ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 1 2011-08-20 43 27.0 4.0 NaN NaN NaN NaN NaN ... 9 4 0 13 22 1 0 52 4 5850000 1 2 2011-08-23 34 19.0 3.0 NaN NaN NaN NaN NaN ... 15 3 0 15 29 1 10 66 14 6000000 2 3 2011-08-27 43 29.0 2.0 NaN NaN NaN NaN NaN ... 10 3 0 11 27 0 4 67 10 5700000 3 4 2011-09-01 89 50.0 9.0 NaN NaN NaN NaN NaN ... 11 2 1 4 4 0 0 26 3 13100000 4 5 2011-09-05 77 77.0 4.0 NaN NaN NaN NaN NaN ... 319 108 17 135 236 2 91 195 14 16331452 5 rows \u00d7 292 columns Technique #2: drop rows / observations df . dropna ( axis = 0 , how = 'any' , thresh = 257 , subset = None , inplace = True ) # thresh argument that specifies the number of non-missing values that should be present for each row in order not to be dropped. df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 1 2011-08-20 43 27.0 4.0 NaN NaN NaN NaN NaN ... 9 4 0 13 22 1 0 52 4 5850000 1 2 2011-08-23 34 19.0 3.0 NaN NaN NaN NaN NaN ... 15 3 0 15 29 1 10 66 14 6000000 2 3 2011-08-27 43 29.0 2.0 NaN NaN NaN NaN NaN ... 10 3 0 11 27 0 4 67 10 5700000 3 4 2011-09-01 89 50.0 9.0 NaN NaN NaN NaN NaN ... 11 2 1 4 4 0 0 26 3 13100000 4 5 2011-09-05 77 77.0 4.0 NaN NaN NaN NaN NaN ... 319 108 17 135 236 2 91 195 14 16331452 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 30466 30469 2015-06-30 44 27.0 7.0 9.0 1.0 1975.0 2.0 6.0 ... 15 5 0 15 26 1 2 84 6 7400000 30467 30470 2015-06-30 86 59.0 3.0 9.0 2.0 1935.0 4.0 10.0 ... 313 128 24 98 182 1 82 171 15 25000000 30468 30471 2015-06-30 45 NaN 10.0 20.0 1.0 NaN 1.0 1.0 ... 1 1 0 2 12 0 1 11 1 6970959 30469 30472 2015-06-30 64 32.0 5.0 15.0 1.0 2003.0 2.0 11.0 ... 22 1 1 6 31 1 4 65 7 13500000 30470 30473 2015-06-30 43 28.0 1.0 9.0 1.0 1968.0 2.0 6.0 ... 5 2 0 7 16 0 9 54 10 5600000 29779 rows \u00d7 292 columns Technique #3: impute the missing with constant values. df_copy = df . copy () numeric_cols = df_copy . select_dtypes ( include = [ 'number' ]) . columns df_copy [ numeric_cols ] = df_copy [ numeric_cols ] . fillna ( - 999 ) df_copy [ non_numeric_cols ] = df_copy [ non_numeric_cols ] . fillna ( '_MISSING_' ) Technique #4: impute the missing with statistics #imputing numeric columns by median med = df_copy [ numeric_cols ] . median () df_copy [ numeric_cols ] = df_copy [ numeric_cols ] . fillna ( med ) # imputing non-numeric columns by mode most_freq = df_copy [ non_numeric_cols ] . mode () df_copy [ non_numeric_cols ] = df_copy [ non_numeric_cols ] . fillna ( most_freq ) most_freq .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } timestamp product_type sub_area culture_objects_top_25 thermal_power_plant_raion incineration_raion oil_chemistry_raion radiation_raion railroad_terminal_raion big_market_raion nuclear_reactor_raion detention_facility_raion water_1line big_road1_1line railroad_1line ecology 0 2014-12-16 Investment Poselenie Sosenskoe no no no no no no no no no no no no poor #to check for missing values df_copy . isna () . sum () id 0 timestamp 0 full_sq 0 life_sq 0 floor 0 .. mosque_count_5000 0 leisure_count_5000 0 sport_count_5000 0 market_count_5000 0 price_doc 0 Length: 292, dtype: int64 # Irregular data (outliers) Outliers could bias our data analysis results, providing a misleading representation of the data. Outliers could be real outliers or mistakes. How to detect? Method #1: descriptive statistics df_copy . describe () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id full_sq life_sq floor max_floor material build_year num_room kitch_sq state ... big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc year month weekday count 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 ... 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 2.977900e+04 29779.000000 29779.000000 29779.000000 mean 3.927294e-17 1.133705e-15 -6.050975e-16 -3.694486e-15 -3.424010e-14 2.521536e-16 6.337155e-16 1.510536e-13 -2.153204e-13 1.240557e-13 ... 1.864436e-15 -1.152501e-16 -2.969346e-15 1.061662e-15 -3.434236e-16 3.142879e-17 3.015190e-15 2013.453105 6.744014 2.196783 std 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 ... 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 1.000017e+00 0.962059 3.523263 1.576159 min -1.733558e+00 -1.413996e+00 -2.004501e+00 -1.344496e+01 -1.484404e+00 -1.484503e+00 -1.975412e-02 -1.484507e+00 -1.482597e+00 -1.130673e+00 ... -5.234219e-01 -6.453354e-01 -7.320273e-01 -4.259748e-01 -1.165510e+00 -1.251928e+00 -1.477355e+00 2011.000000 1.000000 0.000000 25% -8.634904e-01 -4.234363e-01 4.506506e-01 1.039594e-02 -1.484404e+00 -1.484503e+00 -1.975412e-02 -1.484507e+00 -1.482597e+00 -1.130673e+00 ... -4.553321e-01 -4.568299e-01 -7.320273e-01 -4.259748e-01 -9.059867e-01 -1.046875e+00 -4.935775e-01 2013.000000 4.000000 1.000000 50% 1.914127e-03 -1.366954e-01 4.748155e-01 6.410995e-02 6.657718e-01 6.718609e-01 5.709977e-03 6.716512e-01 6.609854e-01 8.821739e-01 ... -2.851076e-01 -3.102146e-01 -7.320273e-01 -3.297071e-01 -1.057884e-01 -2.266611e-01 -1.863383e-01 2014.000000 6.000000 2.000000 75% 8.653282e-01 2.282476e-01 5.013968e-01 1.178240e-01 6.807036e-01 6.718609e-01 5.916512e-03 6.738074e-01 6.759905e-01 8.841867e-01 ... -1.148831e-01 -5.887390e-02 9.015939e-01 -8.903773e-02 4.781401e-01 1.003659e+00 2.398021e-01 2014.000000 10.000000 3.000000 max 1.732382e+00 1.374207e+02 1.848007e+01 1.004105e+00 8.961478e-01 6.826427e-01 1.725493e+02 7.104621e-01 4.976016e+00 9.465850e-01 ... 4.617358e+00 4.590928e+00 2.535215e+00 4.676216e+00 3.549171e+00 3.054193e+00 2.163833e+01 2015.000000 12.000000 6.000000 8 rows \u00d7 279 columns Method #2: histogram & box plot df_copy [ 'full_sq' ] . hist ( bins = 100 ) ![output_43_1.png](../../../Assets/DataCleaningNotebook/output_43_1.png) sns . boxplot ( df_copy [ 'full_sq' ]) ![output_44_1.png](../../../Assets/DataCleaningNotebook/output_44_1.png) Method #3: bar chart # to check outliers in categorical attributes df [ 'ecology' ] . value_counts () . plot ( kind = 'bar' ) # But if there is a category with only one value called \u2018extraordinary\u2019, that could be considered an \u2018outlier\u2019. ![output_46_1.png](../../../Assets/DataCleaningNotebook/output_46_1.png) How to handle missing data Unnecessary data Unnecessary type #1: repetitive & uninformative When an extremely high percentage of the column has a repetitive value, num_rows = len ( df_copy ) for col in df_copy . columns : cnts = df_copy [ col ] . value_counts ( dropna = False ) top_pct = ( cnts / num_rows ) . iloc [ 0 ] if top_pct > 0.999 : print ( ' {0} : {1:.2f} %' . format ( col , top_pct * 100 )) print ( cnts ) print () Unnecessary type #2: irrelevant If features are not related to the question we are trying to solve, they are irrelevant. Use \"drop()\" method to drop such features Unnecessary type #3: duplicates duplicate occurs when all the columns\u2019 values within the observations are the same. df_copy [ df_copy . duplicated ()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 0 rows \u00d7 292 columns #We first drop id, and then see if there are duplicated rows from the DataFrame df_copy [ df_copy . drop ( columns = [ 'id' ]) . duplicated ()] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id timestamp full_sq life_sq floor max_floor material build_year num_room kitch_sq ... cafe_count_5000_price_2500 cafe_count_5000_price_4000 cafe_count_5000_price_high big_church_count_5000 church_count_5000 mosque_count_5000 leisure_count_5000 sport_count_5000 market_count_5000 price_doc 4328 4331 2012-10-22 61 -999.0 18.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 11 2 1 5 4 0 1 32 5 8248500 6991 6994 2013-04-03 42 -999.0 2.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 3 2 0 2 16 1 0 20 4 3444000 8059 8062 2013-05-22 68 -999.0 2.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 3 2 0 2 16 1 0 20 4 5406690 8653 8656 2013-06-24 40 -999.0 12.0 -999.0 -999.0 -999.0 -999.0 -999.0 ... 1 0 0 4 6 0 0 4 1 4112800 14004 14007 2014-01-22 46 28.0 1.0 9.0 1.0 1968.0 2.0 5.0 ... 10 1 0 13 15 1 1 61 4 3000000 17404 17407 2014-04-15 134 134.0 1.0 1.0 1.0 0.0 3.0 0.0 ... 0 0 0 0 1 0 0 0 0 5798496 26675 26678 2014-12-17 62 -999.0 9.0 17.0 1.0 -999.0 2.0 1.0 ... 371 141 26 150 249 2 105 203 13 6552000 28361 28364 2015-03-14 62 -999.0 2.0 17.0 1.0 -999.0 2.0 1.0 ... 371 141 26 150 249 2 105 203 13 6520500 28712 28715 2015-03-30 41 41.0 11.0 17.0 1.0 2016.0 1.0 41.0 ... 2 2 0 2 9 0 0 7 2 4114580 9 rows \u00d7 292 columns df_dedupped = df . drop ( columns = [ 'id' ]) . drop_duplicates () print ( df . shape ) print ( df_dedupped . shape ) (29779, 292) (29770, 291) # Inconsistent data Inconsistent type #1: capitalization Inconsistent use of upper and lower cases in categorical values is typical. We need to clean it since Python is case-sensitive. # to print full numpy array without ... #import numpy as np #import sys #np.set_printoptions(threshold=sys.maxsize) # to display the entire list without ... with pd . option_context ( 'display.max_rows' , None , 'display.max_columns' , None ): # more options can be specified also display ( df_copy [ 'sub_area' ] . value_counts ( dropna = False )) Poselenie Sosenskoe 1617 Nekrasovka 1611 Poselenie Vnukovskoe 1290 Poselenie Moskovskij 925 Poselenie Voskresenskoe 713 Mitino 679 Tverskoe 678 Krjukovo 518 Mar'ino 508 Juzhnoe Butovo 451 Poselenie Shherbinka 443 Solncevo 421 Zapadnoe Degunino 410 Poselenie Desjonovskoe 362 Otradnoe 353 Nagatinskij Zaton 327 Nagornoe 305 Bogorodskoe 305 Strogino 301 Izmajlovo 300 Tekstil'shhiki 298 Ljublino 297 Gol'janovo 295 Severnoe Tushino 282 Chertanovo Juzhnoe 273 Birjulevo Vostochnoe 268 Vyhino-Zhulebino 264 Horoshevo-Mnevniki 262 Zjuzino 259 Ochakovo-Matveevskoe 255 Perovo 247 Ramenki 241 Jasenevo 237 Kosino-Uhtomskoe 237 Bibirevo 230 Golovinskoe 224 Poselenie Filimonkovskoe 221 Caricyno 220 Kuz'minki 220 Kon'kovo 220 Veshnjaki 213 Akademicheskoe 211 Orehovo-Borisovo Juzhnoe 208 Koptevo 207 Orehovo-Borisovo Severnoe 206 Novogireevo 201 Chertanovo Severnoe 200 Danilovskoe 199 Ivanovskoe 197 Mozhajskoe 197 Chertanovo Central'noe 196 Pechatniki 192 Presnenskoe 190 Sokolinaja Gora 188 Obruchevskoe 185 Kuncevo 184 Brateevo 182 Severnoe Butovo 182 Rjazanskij 180 Hovrino 178 Losinoostrovskoe 177 Juzhnoe Tushino 175 Dmitrovskoe 174 Taganskoe 173 Severnoe Medvedkovo 167 Beskudnikovskoe 166 Teplyj Stan 165 Pokrovskoe Streshnevo 164 Severnoe Izmajlovo 163 Cheremushki 158 Nagatino-Sadovniki 158 Troickij okrug 158 Shhukino 155 Timirjazevskoe 154 Vostochnoe Izmajlovo 154 Preobrazhenskoe 152 Novo-Peredelkino 149 Filevskij Park 148 Lomonosovskoe 147 Kotlovka 147 Juzhnoe Medvedkovo 143 Poselenie Pervomajskoe 142 Novokosino 139 Fili Davydkovo 137 Horoshevskoe 136 Levoberezhnoe 135 Donskoe 135 Vojkovskoe 131 Sviblovo 131 Zjablikovo 127 Troparevo-Nikulino 126 Lianozovo 126 Juzhnoportovoe 126 Ajeroport 123 Babushkinskoe 123 Jaroslavskoe 121 Lefortovo 119 Vostochnoe Degunino 118 Mar'ina Roshha 116 Birjulevo Zapadnoe 115 Matushkino 111 Savelki 105 Krylatskoe 103 Butyrskoe 101 Silino 100 Prospekt Vernadskogo 100 Alekseevskoe 100 Moskvorech'e-Saburovo 99 Basmannoe 98 Meshhanskoe 94 Staroe Krjukovo 92 Hamovniki 90 Savelovskoe 85 Marfino 85 Jakimanka 81 Ostankinskoe 79 Gagarinskoe 79 Nizhegorodskoe 77 Sokol 72 Altuf'evskoe 68 Rostokino 64 Kurkino 62 Sokol'niki 60 Begovoe 60 Metrogorodok 58 Dorogomilovo 56 Zamoskvorech'e 50 Kapotnja 49 Vnukovo 44 Krasnosel'skoe 37 Severnoe 37 Poselenie Rogovskoe 30 Poselenie Rjazanovskoe 26 Poselenie Kokoshkino 20 Poselenie Mosrentgen 19 Poselenie Krasnopahorskoe 19 Arbat 15 Vostochnoe 7 Poselenie Marushkinskoe 6 Molzhaninovskoe 3 Poselenie Voronovskoe 2 Name: sub_area, dtype: int64 \u2018Poselenie Sosenskoe\u2019 and \u2018pOseleNie sosenskeo\u2019 could refer to the same district. How to handle? To avoid this, we can lowercase (or uppercase) all letters. df_copy [ 'sub_area_lower' ] = df_copy [ 'sub_area' ] . str . lower () df_copy [ 'sub_area_lower' ] . value_counts ( dropna = False ) poselenie sosenskoe 1617 nekrasovka 1611 poselenie vnukovskoe 1290 poselenie moskovskij 925 poselenie voskresenskoe 713 ... arbat 15 vostochnoe 7 poselenie marushkinskoe 6 molzhaninovskoe 3 poselenie voronovskoe 2 Name: sub_area_lower, Length: 141, dtype: int64 Inconsistent type #2: typos of categorical values A categorical column takes on a limited and usually fixed number of possible values. Sometimes it shows other values due to reasons like typos. How to find out? Let\u2019s see an example. Within the code below: We generate a new DataFrame, df_city_ex There is only one column that stores the city names. There are misspellings. For example, \u2018torontoo\u2019 and \u2018tronto\u2019 both refer to the city of \u2018toronto\u2019. The variable cities stores the 4 correct names of \u2018toronto\u2019, \u2018vancouver\u2019, \u2018montreal\u2019, and \u2018calgary\u2019. To identify typos, we use fuzzy logic matches. We use edit_distance from nltk, which measures the number of operations (e.g., substitution, insertion, deletion) needed to change from one string into another string. We calculate the distance between the actual values and the correct values. # Levenshtein Distance, Hamming distance df_city_ex = pd . DataFrame ( data = { 'city' : [ 'torontooo' , 'toronto' , 'turonto' , 'vancouver' , 'vancover' , 'vancouvr' , 'montreal' , 'calgary' ]}) df_city_ex [ 'city' ] . value_counts () vancouvr 1 montreal 1 vancover 1 turonto 1 calgary 1 vancouver 1 torontooo 1 toronto 1 Name: city, dtype: int64 #!pip install pyspellchecker from spellchecker import SpellChecker spell = SpellChecker () i = 0 for city in df_city_ex [ 'city' ]: # Get the one `most likely` answer df_city_ex . at [ i , 'city' ] = spell . correction ( city ) i = i + 1 #print(spell.correction(city)) df_city_ex [ 'city' ] 0 toronto 1 toronto 2 toronto 3 vancouver 4 vancouver 5 vancouver 6 montreal 7 calgary Name: city, dtype: object df_city_ex [ 'city' ] . value_counts () vancouver 3 toronto 3 montreal 1 calgary 1 Name: city, dtype: int64 --- Tags: [!AMLIndex](./!AMLIndex.md)","title":"Notebook for data cleaning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataPreprocessingNotebook.html","text":"Notebook for data preprocessing # # Importing libraries import pandas as pd import numpy as np # Read csv file into a pandas dataframe df = pd . read_csv ( \"property_data.csv\" ) # Take a look at the first few rows df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3 1.0 1000 1 100002000.0 197.0 LEXINGTON N 3 1.5 -- 2 100003000.0 NaN LEXINGON N NaN 1.0 850 3 100004000.0 201.0 BERKELEY 12 1 NaN 700 4 NaN 203.0 BERKELEY Y 3 2.0 1600 5 100006000.0 207.0 BERKELEY Y NaN 1.0 800 6 100007000.0 NaN WASHINGTON NaN 2 NaN 950 7 100008000.0 213.0 tremont Y 1 1.0 NaN 8 100009000.0 215.0 TREMONT Y na 2.0 1800 9 100009000.0 215.0 TREMONT Y na 2.0 1800 # Looking at the ST_NUM column df [ 'ST_NUM' ] . isnull () 0 False 1 False 2 True 3 False 4 False 5 False 6 True 7 False 8 False 9 False Name: ST_NUM, dtype: bool df [ 'ST_NUM' ] . isnull () . sum () 2 # Looking at the NUM_BEDROOMS column df [ 'NUM_BEDROOMS' ] . isnull () . sum () 2 # Making a list of missing value types missing_values = [ \"n/a\" , \"na\" , \"--\" ] df = pd . read_csv ( \"property_data.csv\" , na_values = missing_values ) # Looking at the NUM_BEDROOMS column df [ 'NUM_BEDROOMS' ] . isnull () . sum () 4 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 NaN LEXINGON N NaN 1.0 850.0 3 100004000.0 201.0 BERKELEY 12 1.0 NaN 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y NaN 1.0 800.0 6 100007000.0 NaN WASHINGTON NaN 2.0 NaN 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y NaN 2.0 1800.0 9 100009000.0 215.0 TREMONT Y NaN 2.0 1800.0 df [ 'OWN_OCCUPIED' ] . isnull () 0 False 1 False 2 False 3 False 4 False 5 False 6 True 7 False 8 False 9 False Name: OWN_OCCUPIED, dtype: bool # Detecting numbers cnt = 0 for row in df [ 'OWN_OCCUPIED' ]: try : int ( row ) df . loc [ cnt , 'OWN_OCCUPIED' ] = np . nan except ValueError : pass cnt += 1 df [ 'OWN_OCCUPIED' ] . isnull () 0 False 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False 9 False Name: OWN_OCCUPIED, dtype: bool df . isnull () . sum () PID 1 ST_NUM 2 ST_NAME 0 OWN_OCCUPIED 2 NUM_BEDROOMS 4 NUM_BATH 2 SQ_FT 2 dtype: int64 # Total number of missing value df . isnull () . sum () . sum () 13 # Replace missing values with a number df [ 'ST_NUM' ] . fillna ( 125 , inplace = True ) # Location based replacement df . loc [ 2 , 'ST_NUM' ] = 125 # Replace using median median = df [ 'NUM_BEDROOMS' ] . median () df [ 'NUM_BEDROOMS' ] . fillna ( median , inplace = True ) df . isnull () . sum () PID 1 ST_NUM 0 ST_NAME 0 OWN_OCCUPIED 2 NUM_BEDROOMS 0 NUM_BATH 2 SQ_FT 2 dtype: int64 # Replace using mode df [ 'NUM_BATH' ] . fillna ( df [ 'NUM_BATH' ] . mode ()[ 0 ], inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY NaN 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON NaN 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df . dtypes PID float64 ST_NUM float64 ST_NAME object OWN_OCCUPIED object NUM_BEDROOMS float64 NUM_BATH float64 SQ_FT float64 dtype: object df [ 'OWN_OCCUPIED' ] . fillna ( 'N' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 # Replace using mean mean = df [ 'SQ_FT' ] . mean () df [ 'SQ_FT' ] . fillna ( mean , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 = df . dropna ( subset = [ 'PID' ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 . ST_NAME . unique () array(['PUTNAM', 'LEXINGTON', 'LEXINGON', 'BERKELEY', 'WASHINGTON', 'tremont', 'TREMONT'], dtype=object) df1 [ \"ST_NAME\" ] = df1 [ \"ST_NAME\" ] . apply ( lambda x : x . replace ( \"LEXINGON\" , \"LEXINGTON\" )) df1 [ \"ST_NAME\" ] = df1 [ \"ST_NAME\" ] . apply ( lambda x : x . upper ()) <ipython-input-27-3a1ae30bd232>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1[\"ST_NAME\"] = df1[\"ST_NAME\"].apply(lambda x: x.replace(\"LEXINGON\", \"LEXINGTON\")) <ipython-input-27-3a1ae30bd232>:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1[\"ST_NAME\"] = df1[\"ST_NAME\"].apply(lambda x: x.upper()) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGTON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 TREMONT Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 . drop_duplicates ( keep = 'last' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGTON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 TREMONT Y 1.0 1.0 1187.5 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 [ 'ST_NUM' ] = df1 [ 'ST_NUM' ] . astype ( str ) <ipython-input-30-09fc6084589a>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1['ST_NUM'] = df1['ST_NUM'].astype(str) df1 . dtypes PID float64 ST_NUM object ST_NAME object OWN_OCCUPIED object NUM_BEDROOMS float64 NUM_BATH float64 SQ_FT float64 dtype: object Tags: !AMLIndex","title":"Notebook for data preprocessing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataPreprocessingNotebook.html#notebook-for-data-preprocessing","text":"# Importing libraries import pandas as pd import numpy as np # Read csv file into a pandas dataframe df = pd . read_csv ( \"property_data.csv\" ) # Take a look at the first few rows df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3 1.0 1000 1 100002000.0 197.0 LEXINGTON N 3 1.5 -- 2 100003000.0 NaN LEXINGON N NaN 1.0 850 3 100004000.0 201.0 BERKELEY 12 1 NaN 700 4 NaN 203.0 BERKELEY Y 3 2.0 1600 5 100006000.0 207.0 BERKELEY Y NaN 1.0 800 6 100007000.0 NaN WASHINGTON NaN 2 NaN 950 7 100008000.0 213.0 tremont Y 1 1.0 NaN 8 100009000.0 215.0 TREMONT Y na 2.0 1800 9 100009000.0 215.0 TREMONT Y na 2.0 1800 # Looking at the ST_NUM column df [ 'ST_NUM' ] . isnull () 0 False 1 False 2 True 3 False 4 False 5 False 6 True 7 False 8 False 9 False Name: ST_NUM, dtype: bool df [ 'ST_NUM' ] . isnull () . sum () 2 # Looking at the NUM_BEDROOMS column df [ 'NUM_BEDROOMS' ] . isnull () . sum () 2 # Making a list of missing value types missing_values = [ \"n/a\" , \"na\" , \"--\" ] df = pd . read_csv ( \"property_data.csv\" , na_values = missing_values ) # Looking at the NUM_BEDROOMS column df [ 'NUM_BEDROOMS' ] . isnull () . sum () 4 df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 NaN LEXINGON N NaN 1.0 850.0 3 100004000.0 201.0 BERKELEY 12 1.0 NaN 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y NaN 1.0 800.0 6 100007000.0 NaN WASHINGTON NaN 2.0 NaN 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y NaN 2.0 1800.0 9 100009000.0 215.0 TREMONT Y NaN 2.0 1800.0 df [ 'OWN_OCCUPIED' ] . isnull () 0 False 1 False 2 False 3 False 4 False 5 False 6 True 7 False 8 False 9 False Name: OWN_OCCUPIED, dtype: bool # Detecting numbers cnt = 0 for row in df [ 'OWN_OCCUPIED' ]: try : int ( row ) df . loc [ cnt , 'OWN_OCCUPIED' ] = np . nan except ValueError : pass cnt += 1 df [ 'OWN_OCCUPIED' ] . isnull () 0 False 1 False 2 False 3 True 4 False 5 False 6 True 7 False 8 False 9 False Name: OWN_OCCUPIED, dtype: bool df . isnull () . sum () PID 1 ST_NUM 2 ST_NAME 0 OWN_OCCUPIED 2 NUM_BEDROOMS 4 NUM_BATH 2 SQ_FT 2 dtype: int64 # Total number of missing value df . isnull () . sum () . sum () 13 # Replace missing values with a number df [ 'ST_NUM' ] . fillna ( 125 , inplace = True ) # Location based replacement df . loc [ 2 , 'ST_NUM' ] = 125 # Replace using median median = df [ 'NUM_BEDROOMS' ] . median () df [ 'NUM_BEDROOMS' ] . fillna ( median , inplace = True ) df . isnull () . sum () PID 1 ST_NUM 0 ST_NAME 0 OWN_OCCUPIED 2 NUM_BEDROOMS 0 NUM_BATH 2 SQ_FT 2 dtype: int64 # Replace using mode df [ 'NUM_BATH' ] . fillna ( df [ 'NUM_BATH' ] . mode ()[ 0 ], inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY NaN 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON NaN 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df . dtypes PID float64 ST_NUM float64 ST_NAME object OWN_OCCUPIED object NUM_BEDROOMS float64 NUM_BATH float64 SQ_FT float64 dtype: object df [ 'OWN_OCCUPIED' ] . fillna ( 'N' , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 NaN 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 NaN 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 # Replace using mean mean = df [ 'SQ_FT' ] . mean () df [ 'SQ_FT' ] . fillna ( mean , inplace = True ) df .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 4 NaN 203.0 BERKELEY Y 3.0 2.0 1600.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 = df . dropna ( subset = [ 'PID' ]) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 tremont Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 . ST_NAME . unique () array(['PUTNAM', 'LEXINGTON', 'LEXINGON', 'BERKELEY', 'WASHINGTON', 'tremont', 'TREMONT'], dtype=object) df1 [ \"ST_NAME\" ] = df1 [ \"ST_NAME\" ] . apply ( lambda x : x . replace ( \"LEXINGON\" , \"LEXINGTON\" )) df1 [ \"ST_NAME\" ] = df1 [ \"ST_NAME\" ] . apply ( lambda x : x . upper ()) <ipython-input-27-3a1ae30bd232>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1[\"ST_NAME\"] = df1[\"ST_NAME\"].apply(lambda x: x.replace(\"LEXINGON\", \"LEXINGTON\")) <ipython-input-27-3a1ae30bd232>:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1[\"ST_NAME\"] = df1[\"ST_NAME\"].apply(lambda x: x.upper()) df1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGTON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 TREMONT Y 1.0 1.0 1187.5 8 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 . drop_duplicates ( keep = 'last' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } PID ST_NUM ST_NAME OWN_OCCUPIED NUM_BEDROOMS NUM_BATH SQ_FT 0 100001000.0 104.0 PUTNAM Y 3.0 1.0 1000.0 1 100002000.0 197.0 LEXINGTON N 3.0 1.5 1187.5 2 100003000.0 125.0 LEXINGTON N 2.5 1.0 850.0 3 100004000.0 201.0 BERKELEY N 1.0 1.0 700.0 5 100006000.0 207.0 BERKELEY Y 2.5 1.0 800.0 6 100007000.0 125.0 WASHINGTON N 2.0 1.0 950.0 7 100008000.0 213.0 TREMONT Y 1.0 1.0 1187.5 9 100009000.0 215.0 TREMONT Y 2.5 2.0 1800.0 df1 [ 'ST_NUM' ] = df1 [ 'ST_NUM' ] . astype ( str ) <ipython-input-30-09fc6084589a>:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy df1['ST_NUM'] = df1['ST_NUM'].astype(str) df1 . dtypes PID float64 ST_NUM object ST_NAME object OWN_OCCUPIED object NUM_BEDROOMS float64 NUM_BATH float64 SQ_FT float64 dtype: object Tags: !AMLIndex","title":"Notebook for data preprocessing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataVisualisationNotebook.html","text":"Notebook for data visualization # Different Python Libraries: Matplotlib: low level, provides lots of freedom Pandas Visualization: easy to use interface, built on Matplotlib Seaborn: high-level interface, great default styles Plotly: can create interactive plots Matplotlib: It is a low-level library with a Matlab like interface which offers lots of freedom at the cost of having to write more code. import matplotlib.pyplot as plt import matplotlib import pandas as pd names = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' , 'class' ] iris = pd . read_csv ( \"iris.csv\" , names = names ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa #Scatter Plot: to observe relationship between 2 variables #ax = plt.subplots() colors = [ 'red' , 'green' , 'blue' ] # scatter the sepal_length against the sepal_width plt . scatter ( iris [ 'sepal_length' ], iris [ 'sepal_width' ]) # set a title and labels plt . title ( 'Iris Dataset' ) plt . xlabel ( 'sepal_length' ) plt . ylabel ( 'sepal_width' ) Text(0, 0.5, 'sepal_width') # create color dictionary colors = { 'Iris-setosa' : 'r' , 'Iris-versicolor' : 'g' , 'Iris-virginica' : 'b' } # create a figure and axis fig , ax = plt . subplots () # plot each data-point for i in range ( len ( iris [ 'sepal_length' ])): ax . scatter ( iris [ 'sepal_length' ][ i ], iris [ 'sepal_width' ][ i ], color = colors [ iris [ 'class' ][ i ]]) # set a title and labels ax . set_title ( 'Iris Dataset' ) ax . set_xlabel ( 'sepal_length' ) ax . set_ylabel ( 'sepal_width' ) Text(0, 0.5, 'sepal_width') Line Chart In Matplotlib we can create a line chart by calling the plot method. We can also plot multiple columns in one graph, by looping through the columns we want and plotting each column on the same axis. # get columns to plot columns = iris . columns . drop ([ 'class' ]) # create x data x_data = range ( 0 , iris . shape [ 0 ]) # create figure and axis fig , ax = plt . subplots () # plot each column for column in columns : ax . plot ( x_data , iris [ column ], label = column ) # set title and legend ax . set_title ( 'Iris Dataset' ) ax . legend () <matplotlib.legend.Legend at 0x1ec4b3274c0> Histogram In Matplotlib we can create a Histogram using the hist method. If we pass it categorical data like the points column from the wine-review dataset it will automatically calculate how often each class occurs. # create figure and axis fig , ax = plt . subplots () # plot histogram ax . hist ( iris [ 'sepal_length' ]) # set title and labels ax . set_title ( 'Sepal_Length' ) ax . set_xlabel ( 'Sepal_Length' ) ax . set_ylabel ( 'Frequency' ) Text(0, 0.5, 'Frequency') Pandas Visualization Pandas Visualization makes it really easy to create plots out of a pandas dataframe and series. It also has a higher level API than Matplotlib and therefore we need less code for the same results. Scatter plot iris . plot . scatter ( 'sepal_length' , 'sepal_width' ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b78dac0> Line Chart iris . drop ([ 'class' ], axis = 1 ) . plot . line ( title = 'Iris Dataset' ) # \u201caxis 0\u201d represents rows and \u201caxis 1\u201d represents columns. <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b7cdeb0> Histogram iris [ 'sepal_length' ] . plot . hist () <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b857040> to create multiple histogram #The subplots argument specifies that we want a separate plot for each feature and the layout specifies the number of plots per row and column. iris . plot . hist ( subplots = True , layout = ( 2 , 2 ), figsize = ( 10 , 10 ), bins = 20 ) array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B8D3E20>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B31B280>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B6079A0>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B7CD520>]], dtype=object) ![[Assets/DataVisualizationNotebook/output_19_1.png]] iris . boxplot ( by = 'class' , column = [ 'sepal_length' ], grid = False ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d20fb80> Using Seaborn Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for creating attractive graphs. Seaborn has a lot to offer. You can create graphs in one line that would take you multiple tens of lines in Matplotlib. Its standard designs are awesome and it also has a nice interface for working with pandas dataframes. Seaborn has a scatter plot that shows relationship between x and y can be shown for different subsets of the data using the hue, size, and style parameters. import seaborn as sns sns . scatterplot ( x = 'sepal_length' , y = 'sepal_width' , data = iris ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b9b9d60> sns . scatterplot ( 'sepal_length' , 'sepal_width' , data = iris , hue = 'class' ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4bdc4910> sns . lineplot ( data = iris . drop ([ 'class' ], axis = 1 )) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4ce03220> ![[Assets/DataVisualizationNotebook/output_24_1.png]] sns . distplot ( iris [ 'sepal_length' ], bins = 10 , kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4ce9d760> sns . boxplot ( x = 'class' , y = 'sepal_width' , data = iris ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d37cdf0> A Heatmap is a graphical representation of data where the individual values contained in a matrix are represented as colors. Heatmaps are perfect for exploring the correlation of features in a dataset. sns . heatmap ( iris . corr (), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d432b50> Seaborns pairplot enable you to plot a grid of pairwise relationships in a datas sns . pairplot ( iris ) <seaborn.axisgrid.PairGrid at 0x1ec4d50fcd0> Tags: !AMLIndex","title":"Notebook for data visualization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/DataVisualisationNotebook.html#notebook-for-data-visualization","text":"Different Python Libraries: Matplotlib: low level, provides lots of freedom Pandas Visualization: easy to use interface, built on Matplotlib Seaborn: high-level interface, great default styles Plotly: can create interactive plots Matplotlib: It is a low-level library with a Matlab like interface which offers lots of freedom at the cost of having to write more code. import matplotlib.pyplot as plt import matplotlib import pandas as pd names = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' , 'class' ] iris = pd . read_csv ( \"iris.csv\" , names = names ) iris . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } sepal_length sepal_width petal_length petal_width class 0 5.1 3.5 1.4 0.2 Iris-setosa 1 4.9 3.0 1.4 0.2 Iris-setosa 2 4.7 3.2 1.3 0.2 Iris-setosa 3 4.6 3.1 1.5 0.2 Iris-setosa 4 5.0 3.6 1.4 0.2 Iris-setosa #Scatter Plot: to observe relationship between 2 variables #ax = plt.subplots() colors = [ 'red' , 'green' , 'blue' ] # scatter the sepal_length against the sepal_width plt . scatter ( iris [ 'sepal_length' ], iris [ 'sepal_width' ]) # set a title and labels plt . title ( 'Iris Dataset' ) plt . xlabel ( 'sepal_length' ) plt . ylabel ( 'sepal_width' ) Text(0, 0.5, 'sepal_width') # create color dictionary colors = { 'Iris-setosa' : 'r' , 'Iris-versicolor' : 'g' , 'Iris-virginica' : 'b' } # create a figure and axis fig , ax = plt . subplots () # plot each data-point for i in range ( len ( iris [ 'sepal_length' ])): ax . scatter ( iris [ 'sepal_length' ][ i ], iris [ 'sepal_width' ][ i ], color = colors [ iris [ 'class' ][ i ]]) # set a title and labels ax . set_title ( 'Iris Dataset' ) ax . set_xlabel ( 'sepal_length' ) ax . set_ylabel ( 'sepal_width' ) Text(0, 0.5, 'sepal_width') Line Chart In Matplotlib we can create a line chart by calling the plot method. We can also plot multiple columns in one graph, by looping through the columns we want and plotting each column on the same axis. # get columns to plot columns = iris . columns . drop ([ 'class' ]) # create x data x_data = range ( 0 , iris . shape [ 0 ]) # create figure and axis fig , ax = plt . subplots () # plot each column for column in columns : ax . plot ( x_data , iris [ column ], label = column ) # set title and legend ax . set_title ( 'Iris Dataset' ) ax . legend () <matplotlib.legend.Legend at 0x1ec4b3274c0> Histogram In Matplotlib we can create a Histogram using the hist method. If we pass it categorical data like the points column from the wine-review dataset it will automatically calculate how often each class occurs. # create figure and axis fig , ax = plt . subplots () # plot histogram ax . hist ( iris [ 'sepal_length' ]) # set title and labels ax . set_title ( 'Sepal_Length' ) ax . set_xlabel ( 'Sepal_Length' ) ax . set_ylabel ( 'Frequency' ) Text(0, 0.5, 'Frequency') Pandas Visualization Pandas Visualization makes it really easy to create plots out of a pandas dataframe and series. It also has a higher level API than Matplotlib and therefore we need less code for the same results. Scatter plot iris . plot . scatter ( 'sepal_length' , 'sepal_width' ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b78dac0> Line Chart iris . drop ([ 'class' ], axis = 1 ) . plot . line ( title = 'Iris Dataset' ) # \u201caxis 0\u201d represents rows and \u201caxis 1\u201d represents columns. <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b7cdeb0> Histogram iris [ 'sepal_length' ] . plot . hist () <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b857040> to create multiple histogram #The subplots argument specifies that we want a separate plot for each feature and the layout specifies the number of plots per row and column. iris . plot . hist ( subplots = True , layout = ( 2 , 2 ), figsize = ( 10 , 10 ), bins = 20 ) array([[<matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B8D3E20>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B31B280>], [<matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B6079A0>, <matplotlib.axes._subplots.AxesSubplot object at 0x000001EC4B7CD520>]], dtype=object) ![[Assets/DataVisualizationNotebook/output_19_1.png]] iris . boxplot ( by = 'class' , column = [ 'sepal_length' ], grid = False ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d20fb80> Using Seaborn Seaborn is a Python data visualization library based on Matplotlib. It provides a high-level interface for creating attractive graphs. Seaborn has a lot to offer. You can create graphs in one line that would take you multiple tens of lines in Matplotlib. Its standard designs are awesome and it also has a nice interface for working with pandas dataframes. Seaborn has a scatter plot that shows relationship between x and y can be shown for different subsets of the data using the hue, size, and style parameters. import seaborn as sns sns . scatterplot ( x = 'sepal_length' , y = 'sepal_width' , data = iris ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4b9b9d60> sns . scatterplot ( 'sepal_length' , 'sepal_width' , data = iris , hue = 'class' ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4bdc4910> sns . lineplot ( data = iris . drop ([ 'class' ], axis = 1 )) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4ce03220> ![[Assets/DataVisualizationNotebook/output_24_1.png]] sns . distplot ( iris [ 'sepal_length' ], bins = 10 , kde = False ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4ce9d760> sns . boxplot ( x = 'class' , y = 'sepal_width' , data = iris ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d37cdf0> A Heatmap is a graphical representation of data where the individual values contained in a matrix are represented as colors. Heatmaps are perfect for exploring the correlation of features in a dataset. sns . heatmap ( iris . corr (), annot = True ) <matplotlib.axes._subplots.AxesSubplot at 0x1ec4d432b50> Seaborns pairplot enable you to plot a grid of pairwise relationships in a datas sns . pairplot ( iris ) <seaborn.axisgrid.PairGrid at 0x1ec4d50fcd0> Tags: !AMLIndex","title":"Notebook for data visualization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html","text":"End Sem AML Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # tags: !AMLIndex QuestionPapers","title":"End Sem AML Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#end-sem-aml-paper","text":"","title":"End Sem AML Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/End%20Sem%20Paper%20AML.html#question-6","text":"tags: !AMLIndex QuestionPapers","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html","text":"Mid Sem AML Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !AMLIndex QuestionPapers","title":"Mid Sem AML Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html#mid-sem-aml-paper","text":"","title":"Mid Sem AML Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Mid%20Sem%20Paper%20AML.html#question-4","text":"tags: !AMLIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html","text":"Module 1 # Machine Learning # A scientific discipline that explores the construction and study of algos that can learn from data Such algos operate by building a model based on inputs and using that to make predictions or decisions rather than following only explicitly programmed instructions without human intervention Traditional Programming VS Machine Learning # We give the computer some data and a program and we expect an output from the same in the case of traditional programming In case of ML we give the computer some training data and some expected output and we end up getting a procedure that solves the problem for new input data Definition by Tom Mitchell (1998) # A computer program is said to learn from experience \\(E\\) with respect to some class of tasks T and performance measure \\(P\\) , if its performance at task in \\(T\\) , as measured by \\(P\\) , improves with experience \\(E\\) Where does ML fit in? # Steps of ML # Define Business Problem Gathering Data Prepare said data (Takes most of the time, around 80%) Choosing a model Training Evaluation Hyperparameter Tuning Prediction Types of Learning # Supervised/Inductive Learning Unsupervised Learning Semi-supervised Learning Reinforcement Learning Supervised Learning # This learning type uses data that is labeled. Data that have categorized data into columns are considered labeled data. In the above image we are trying to predict a genralized function \\(f(x)\\) that will predict the potential value of y given a new value of x. Regression # \\[ y = f(x_1, x_2, x_3,..., x_n) \\] \\(y = Output\\) \\(f() = Prediction Function\\) \\(x_1, x_2,.., x_n = Features Used\\) Training: Given a training set of labeled examples, estimate the prediction function f by minimizing the prediction error on the training set Testing: Apply f to a never before seen test example x and output the predicted value y f(x) Classification # Unsupervised Learning # Unsupervised learning helps in getting to know not so obvious attributes that categorize data sets into smaller groups in case of clustering Reinforcement Learning # No pre-defined data Semi supervised learning model in ML Allow an agent to take actions and interact with an environment so as to maximize the total rewards Examples Autonomous Cars Game playing Robot in a maze Difference between the three types of learning # Other Categories of Learning # - In batch learning there is a potential for relearning when an updation is needed. It is used ideally done once before deployment - In case of changing data sets we use online/incremental learning - Instance based learning: also called as lazy learning. Learning techniques do not build a model but stores all training instance in memory and when they undergo classification they use proximity measures k closely related members to categorize them. - Model Based learning where we detect patterns in the training data and build a predictive model Challenges of machine learning # Testing and Validation # Cross Validation # Choice of Hyperparameters # Modern ML modesl often use a lot of model params Model performance depends on chouce of params Each parm can assume a number of values Expensive to perform Grid Search CV method Open Source ML Programming Tools # Tags: !AMLIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#module-1","text":"","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#machine-learning","text":"A scientific discipline that explores the construction and study of algos that can learn from data Such algos operate by building a model based on inputs and using that to make predictions or decisions rather than following only explicitly programmed instructions without human intervention","title":"Machine Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#traditional-programming-vs-machine-learning","text":"We give the computer some data and a program and we expect an output from the same in the case of traditional programming In case of ML we give the computer some training data and some expected output and we end up getting a procedure that solves the problem for new input data","title":"Traditional Programming VS Machine Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#definition-by-tom-mitchell-1998","text":"A computer program is said to learn from experience \\(E\\) with respect to some class of tasks T and performance measure \\(P\\) , if its performance at task in \\(T\\) , as measured by \\(P\\) , improves with experience \\(E\\)","title":"Definition by Tom Mitchell (1998)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#where-does-ml-fit-in","text":"","title":"Where does ML fit in?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#steps-of-ml","text":"Define Business Problem Gathering Data Prepare said data (Takes most of the time, around 80%) Choosing a model Training Evaluation Hyperparameter Tuning Prediction","title":"Steps of ML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#types-of-learning","text":"Supervised/Inductive Learning Unsupervised Learning Semi-supervised Learning Reinforcement Learning","title":"Types of Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#supervised-learning","text":"This learning type uses data that is labeled. Data that have categorized data into columns are considered labeled data. In the above image we are trying to predict a genralized function \\(f(x)\\) that will predict the potential value of y given a new value of x.","title":"Supervised Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#regression","text":"\\[ y = f(x_1, x_2, x_3,..., x_n) \\] \\(y = Output\\) \\(f() = Prediction Function\\) \\(x_1, x_2,.., x_n = Features Used\\) Training: Given a training set of labeled examples, estimate the prediction function f by minimizing the prediction error on the training set Testing: Apply f to a never before seen test example x and output the predicted value y f(x)","title":"Regression"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#classification","text":"","title":"Classification"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#unsupervised-learning","text":"Unsupervised learning helps in getting to know not so obvious attributes that categorize data sets into smaller groups in case of clustering","title":"Unsupervised Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#reinforcement-learning","text":"No pre-defined data Semi supervised learning model in ML Allow an agent to take actions and interact with an environment so as to maximize the total rewards Examples Autonomous Cars Game playing Robot in a maze","title":"Reinforcement Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#difference-between-the-three-types-of-learning","text":"","title":"Difference between the three types of learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#other-categories-of-learning","text":"- In batch learning there is a potential for relearning when an updation is needed. It is used ideally done once before deployment - In case of changing data sets we use online/incremental learning - Instance based learning: also called as lazy learning. Learning techniques do not build a model but stores all training instance in memory and when they undergo classification they use proximity measures k closely related members to categorize them. - Model Based learning where we detect patterns in the training data and build a predictive model","title":"Other Categories of Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#challenges-of-machine-learning","text":"","title":"Challenges of machine learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#testing-and-validation","text":"","title":"Testing and Validation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#cross-validation","text":"","title":"Cross Validation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#choice-of-hyperparameters","text":"Modern ML modesl often use a lot of model params Model performance depends on chouce of params Each parm can assume a number of values Expensive to perform Grid Search CV method","title":"Choice of Hyperparameters"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module1AML.html#open-source-ml-programming-tools","text":"Tags: !AMLIndex","title":"Open Source ML Programming Tools"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html","text":"Module 2 # Machine Learning Pipeline # Framing a machine learning problem Get the data Data Pre processing Data Visualization and Analysis Feature Engineering Model Building and Evaluation Fine tune the model Present the solution Launch, monitor and maintain Case Study # Bank is loosing too much money due to bad loans and wants to reduce losses. The following things are looked at to frame the problem Frame the problem # Define the objectives in business terms How will your solution be used What are the current solutions How should you frame this problem (Supervised or unsupervised) How should the performance be measured Is the performance measure aligned with the business objective What are comparable problems Is Human expertise available How would you solve the problem? List the assumptions you have made What would be the minimum performance needed to reach the business objective? An example problem statement # Build a model of housing prices using census data Data attributes and so on for each district Districts are the smallest geographical unit (population ~600-3000) The model to predict the median housing price in an district, given all the other metrics Goodness of the model is determined by how close the model output is wrt actual price for unseen data. Framing the problem # What is the expected usage and benefit? Impacts the choice of algorithms, goodness measure, and effort in lifecycle management of the model What is the baseline method and its performance? Analyze the dataset Each instance comes with the expected output Hence we are going with supervised Goal is to predict a real valued price based on multiple variables line population, income etc Regression is chosen for this reason Output is based on input data at rest, not rapidly changing data rapidly. Dataset small enough to fit in memory Batch Since we are predicting a single value we are calling it a univariate problem Choice of Performance metrics Root Mean Square Mean Absolute Error (MAE) Types and properties of attributes Data types of attributes # job, marital -> nominal education -> Ordinal credit default -> Binary (symmetric) housing loan -> Binary Categorical Binary Symmetric (Simple matching coefficient proximity measure) Asymmetric (Jaccart coefficient proximity measure) Nominal Numerical/continuous (Minkowsky distance for proximity measure) to find outliers Some pandas functions # df . types () # Get the datatypes of all columns in dataframe df . info () # Get more info on non null values in data frame df . describe () # Get statistical summary of each attribute and 5 point summary of numberic attributes Data Types # relational/Object data Transactional data Document data Web and social network data Spatial data time series data Note: Topics such as data types, pre processing, visualization and analysis are explained well in Jupyter Notebooks --------------------------------- # Sampling # Binarization # One Hot Encoding # Attribute Transformation # - This is done to reduce skewness in data. - Transformation is done to bring data to the same scale that helps in feature scaling Normalization # Robust scaler normalizes withing the IQR (Q3 - Q1) Curse of Dimensionality # Dimensionality Reduction # - PCA: - Other ways to reduce dimensionality: - Correlation Decision: - Chi Square Stats (Categorical attributes) - Pearson Coefficient (Numeric attributes) - Annova Test (Mixed attributes) - Wrapper Method is to train models with several subsets of data and choose the one that was most fruitful do then do further improvement on that. Feature Creation # Tags: !AMLIndex Tags: !AMLIndex","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#module-2","text":"","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#machine-learning-pipeline","text":"Framing a machine learning problem Get the data Data Pre processing Data Visualization and Analysis Feature Engineering Model Building and Evaluation Fine tune the model Present the solution Launch, monitor and maintain","title":"Machine Learning Pipeline"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#case-study","text":"Bank is loosing too much money due to bad loans and wants to reduce losses. The following things are looked at to frame the problem","title":"Case Study"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#frame-the-problem","text":"Define the objectives in business terms How will your solution be used What are the current solutions How should you frame this problem (Supervised or unsupervised) How should the performance be measured Is the performance measure aligned with the business objective What are comparable problems Is Human expertise available How would you solve the problem? List the assumptions you have made What would be the minimum performance needed to reach the business objective?","title":"Frame the problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#an-example-problem-statement","text":"Build a model of housing prices using census data Data attributes and so on for each district Districts are the smallest geographical unit (population ~600-3000) The model to predict the median housing price in an district, given all the other metrics Goodness of the model is determined by how close the model output is wrt actual price for unseen data.","title":"An example problem statement"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#framing-the-problem","text":"What is the expected usage and benefit? Impacts the choice of algorithms, goodness measure, and effort in lifecycle management of the model What is the baseline method and its performance? Analyze the dataset Each instance comes with the expected output Hence we are going with supervised Goal is to predict a real valued price based on multiple variables line population, income etc Regression is chosen for this reason Output is based on input data at rest, not rapidly changing data rapidly. Dataset small enough to fit in memory Batch Since we are predicting a single value we are calling it a univariate problem Choice of Performance metrics Root Mean Square Mean Absolute Error (MAE) Types and properties of attributes","title":"Framing the problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#data-types-of-attributes","text":"job, marital -> nominal education -> Ordinal credit default -> Binary (symmetric) housing loan -> Binary Categorical Binary Symmetric (Simple matching coefficient proximity measure) Asymmetric (Jaccart coefficient proximity measure) Nominal Numerical/continuous (Minkowsky distance for proximity measure) to find outliers","title":"Data types of attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#some-pandas-functions","text":"df . types () # Get the datatypes of all columns in dataframe df . info () # Get more info on non null values in data frame df . describe () # Get statistical summary of each attribute and 5 point summary of numberic attributes","title":"Some pandas functions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#data-types","text":"relational/Object data Transactional data Document data Web and social network data Spatial data time series data Note: Topics such as data types, pre processing, visualization and analysis are explained well in Jupyter Notebooks","title":"Data Types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#---------------------------------","text":"","title":"---------------------------------"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#sampling","text":"","title":"Sampling"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#binarization","text":"","title":"Binarization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#one-hot-encoding","text":"","title":"One Hot Encoding"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#attribute-transformation","text":"- This is done to reduce skewness in data. - Transformation is done to bring data to the same scale that helps in feature scaling","title":"Attribute Transformation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#normalization","text":"Robust scaler normalizes withing the IQR (Q3 - Q1)","title":"Normalization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#curse-of-dimensionality","text":"","title":"Curse of Dimensionality"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#dimensionality-reduction","text":"- PCA: - Other ways to reduce dimensionality: - Correlation Decision: - Chi Square Stats (Categorical attributes) - Pearson Coefficient (Numeric attributes) - Annova Test (Mixed attributes) - Wrapper Method is to train models with several subsets of data and choose the one that was most fruitful do then do further improvement on that.","title":"Dimensionality Reduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module2AML.html#feature-creation","text":"Tags: !AMLIndex Tags: !AMLIndex","title":"Feature Creation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html","text":"Module 3 # Intuition behind cost function # Linear Regression # Stochastic Gradient Decent # Logistic vs Linear Regression # Linear is used to predict continuous numbers (Numerical) whereas the Logistic regression is used to predict a categorical value. Logistic regression # A logistic function Sigmoid Function # - It takes a real value and outputs a value between 0 and 1 - Maps input horizon to a bounded symmetrical output range - Examples - Logistic Function - Hyperbolic Tangent - Conditional probability of a random variable can be a sigmoid function Tags: !AMLIndex","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#module-3","text":"","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#intuition-behind-cost-function","text":"","title":"Intuition behind cost function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#linear-regression","text":"","title":"Linear Regression"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#stochastic-gradient-decent","text":"","title":"Stochastic Gradient Decent"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#logistic-vs-linear-regression","text":"Linear is used to predict continuous numbers (Numerical) whereas the Logistic regression is used to predict a categorical value.","title":"Logistic vs Linear Regression"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#logistic-regression","text":"A logistic function","title":"Logistic regression"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module3AML.html#sigmoid-function","text":"- It takes a real value and outputs a value between 0 and 1 - Maps input horizon to a bounded symmetrical output range - Examples - Logistic Function - Hyperbolic Tangent - Conditional probability of a random variable can be a sigmoid function Tags: !AMLIndex","title":"Sigmoid Function"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html","text":"Module 5 # Model Evaluation Parameters # Precision # What percentage of tuples that are positive are actually positive \\(Precision = \\frac{True Positives}{True Positives + False Positives}\\) Recall # What percentage of positive tuples did the classifier label as positive \\(Recall = \\frac{True Positives}{True Positives + False Negatives}\\) F Measure # Harmonic mean of precision and recall \\(F = 2 \\times \\frac{precision \\times recall}{precision + recall}\\) High values of \\(F_1\\) score indicates that both the precision and recall both are high \\(F_\\beta\\) is the wighted measure of precision and recall. Here we assign \\(\\beta\\) times as much to weight to recall as to precision. This is done if either recall or precision is more important than the other. \\(F_\\beta = \\frac{(1 + \\beta^2) \\times precision \\times recall}{\\beta^2 \\times precision + recall}\\) Things to note # Accuracy is useful when data is balanced and all tuples are almost equally occurring If the data is imbalanced (When the tuples of a particular category are very little) then we lean on precision and accuracy as a better metric for measuring the model performance To artificially create instances for the minority class to balance the data sklearn has a smoat library that helps us in achieving the same. Holdout and Cross validation # Hypermarameters are those that controls the learning process. To know this we need to use cross validation We train the model k number of times with different subsetting of train and test samples The above is trained with taking a param as a hyperparam. We calculate the avg accuracy of the k models and assign the score for that hyperparam and repeat all these steps for other params as hyperparams and see which one has the best score. This helps us getting to know what the hyper parameter is. ROC AUC Curve # ROC is a probability curve between TPR and FPR to show their tradeoff \\(TPR = \\frac{TP}{TP + FN}\\) \\(FPR = \\frac{FP}{TN + FP}\\) AUC shows degree of separateability. It shows how good a model is capable of distinguishing between classes Higher AUC means better the model is at predicting AUC of ROC evaluates model performance on average. For model comparison, AUC of ROC should be larger for the model to be superior or better performing. In python we can use sklearn 's predictProba to calculate the probabilities that are required for plotting the ROC curve. Checking for correlation between features # We check how a given feature is correlated to the result. If they are not then we can drop them entirely Similarly we check how correlated the features are to other features Random Variables # Continuous Random Variable # Gaussian Distribution # It is a symmetric distribution that is bell shaped It is symmetric around the mean \\(f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\\cdot e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\\) The mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) decides the shape of the curve Note: Topics such as Conditional probability, Independence, Law of total probability and Bayes theorem are extensively discussed in the Advanced statistics course Naive Bayesian Classifier # In this classifier we use conditional probability to predict a class \\(y\\) given the values of other attributes \\(X\\) . This is given as : \\(P(y | X) = \\frac{P(X | y) \\times P(y)}{P(X)}\\) For the sake of simplicity we can ignore the denominator and choose \\(y\\) based on maximizing Tags: !AMLIndex","title":"Module 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#module-5","text":"","title":"Module 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#model-evaluation-parameters","text":"","title":"Model Evaluation Parameters"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#precision","text":"What percentage of tuples that are positive are actually positive \\(Precision = \\frac{True Positives}{True Positives + False Positives}\\)","title":"Precision"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#recall","text":"What percentage of positive tuples did the classifier label as positive \\(Recall = \\frac{True Positives}{True Positives + False Negatives}\\)","title":"Recall"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#f-measure","text":"Harmonic mean of precision and recall \\(F = 2 \\times \\frac{precision \\times recall}{precision + recall}\\) High values of \\(F_1\\) score indicates that both the precision and recall both are high \\(F_\\beta\\) is the wighted measure of precision and recall. Here we assign \\(\\beta\\) times as much to weight to recall as to precision. This is done if either recall or precision is more important than the other. \\(F_\\beta = \\frac{(1 + \\beta^2) \\times precision \\times recall}{\\beta^2 \\times precision + recall}\\)","title":"F Measure"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#things-to-note","text":"Accuracy is useful when data is balanced and all tuples are almost equally occurring If the data is imbalanced (When the tuples of a particular category are very little) then we lean on precision and accuracy as a better metric for measuring the model performance To artificially create instances for the minority class to balance the data sklearn has a smoat library that helps us in achieving the same.","title":"Things to note"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#holdout-and-cross-validation","text":"Hypermarameters are those that controls the learning process. To know this we need to use cross validation We train the model k number of times with different subsetting of train and test samples The above is trained with taking a param as a hyperparam. We calculate the avg accuracy of the k models and assign the score for that hyperparam and repeat all these steps for other params as hyperparams and see which one has the best score. This helps us getting to know what the hyper parameter is.","title":"Holdout and Cross validation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#roc-auc-curve","text":"ROC is a probability curve between TPR and FPR to show their tradeoff \\(TPR = \\frac{TP}{TP + FN}\\) \\(FPR = \\frac{FP}{TN + FP}\\) AUC shows degree of separateability. It shows how good a model is capable of distinguishing between classes Higher AUC means better the model is at predicting AUC of ROC evaluates model performance on average. For model comparison, AUC of ROC should be larger for the model to be superior or better performing. In python we can use sklearn 's predictProba to calculate the probabilities that are required for plotting the ROC curve.","title":"ROC AUC Curve"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#checking-for-correlation-between-features","text":"We check how a given feature is correlated to the result. If they are not then we can drop them entirely Similarly we check how correlated the features are to other features","title":"Checking for correlation between features"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#random-variables","text":"","title":"Random Variables"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#continuous-random-variable","text":"","title":"Continuous Random Variable"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#gaussian-distribution","text":"It is a symmetric distribution that is bell shaped It is symmetric around the mean \\(f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}\\cdot e^{-\\frac{1}{2}(\\frac{x - \\mu}{\\sigma})^2}\\) The mean \\(\\mu\\) and the standard deviation \\(\\sigma\\) decides the shape of the curve Note: Topics such as Conditional probability, Independence, Law of total probability and Bayes theorem are extensively discussed in the Advanced statistics course","title":"Gaussian Distribution"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module5AML.html#naive-bayesian-classifier","text":"In this classifier we use conditional probability to predict a class \\(y\\) given the values of other attributes \\(X\\) . This is given as : \\(P(y | X) = \\frac{P(X | y) \\times P(y)}{P(X)}\\) For the sake of simplicity we can ignore the denominator and choose \\(y\\) based on maximizing Tags: !AMLIndex","title":"Naive Bayesian Classifier"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html","text":"Module 6 # Information Gain # Gini Index # Inexpensive to construct Extremely fast at Inductive bias in decision tree learning # Inductive bias is the assumption made by the model to learn the target function and to generalize beyond training data What is the inductive bias of DT Learning Shorter trees are preferred over longer trees Prefer trees that place high information gain attributes close to the root Limitations of decision tree learning # Overfitting Building trees that \"adapt too much\" to the training example may lead to \"overfitting\" May therefore fail to fit additional data or predict future observations reliably Training data accuracy increases but the test data accuracy decreases when the size of the tree increases Pre pruning: Stop the algorithm before it becomes a fully grown tree General stopping conditions for a node Stop if all instances belong to the same class Stop if all the attribute values are the same More restrictive conditions: Stop if number of instances is less than some user specified threshold Stop if class distribution of instances are independent of the available features Stop if expanding the current node foes not improve impurity measures. Post pruning: Grow decision tree to its entirity Trim the nodes of the decision tree ina bottom up fashion If generalization error improves after trimming, replace sub tree by a leaf node Majority class of instances in the sub tree is used as the class label of leaf node. Ensemble methods # Use multiple learning algorithms to obtain better predictive performance than could be obtained from any one of the constituent learning algorithm By combining individual models we can get less bias and less variance. This method will perform worse if the error rate is \\(\\varepsilon \\gt 0.5\\) Each base class has to be independent of each other. Methods for constructing ensemble classifier # Using different algorithms Using different hyper parameters Using different training sets By manipulating input features By manipulating the class labels Types of ensemble methods # Simple methods: Max voting Averaging Weighted averaging Advanced methods: Bagging: Homogeneous weak learners, learning done independently from each other. Boosting: Homogeneous weak learners, learning done sequentially in a adaptive way. Stacking: Heterogeneous weak learners, learning done independently and combines them by training a meta model to output a prediction based on the different weak models predictions. Bootstrap Sampling # Bagging # Tags: !AMLIndex","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#module-6","text":"","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#information-gain","text":"","title":"Information Gain"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#gini-index","text":"Inexpensive to construct Extremely fast at","title":"Gini Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#inductive-bias-in-decision-tree-learning","text":"Inductive bias is the assumption made by the model to learn the target function and to generalize beyond training data What is the inductive bias of DT Learning Shorter trees are preferred over longer trees Prefer trees that place high information gain attributes close to the root","title":"Inductive bias in decision tree learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#limitations-of-decision-tree-learning","text":"Overfitting Building trees that \"adapt too much\" to the training example may lead to \"overfitting\" May therefore fail to fit additional data or predict future observations reliably Training data accuracy increases but the test data accuracy decreases when the size of the tree increases Pre pruning: Stop the algorithm before it becomes a fully grown tree General stopping conditions for a node Stop if all instances belong to the same class Stop if all the attribute values are the same More restrictive conditions: Stop if number of instances is less than some user specified threshold Stop if class distribution of instances are independent of the available features Stop if expanding the current node foes not improve impurity measures. Post pruning: Grow decision tree to its entirity Trim the nodes of the decision tree ina bottom up fashion If generalization error improves after trimming, replace sub tree by a leaf node Majority class of instances in the sub tree is used as the class label of leaf node.","title":"Limitations of decision tree learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#ensemble-methods","text":"Use multiple learning algorithms to obtain better predictive performance than could be obtained from any one of the constituent learning algorithm By combining individual models we can get less bias and less variance. This method will perform worse if the error rate is \\(\\varepsilon \\gt 0.5\\) Each base class has to be independent of each other.","title":"Ensemble methods"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#methods-for-constructing-ensemble-classifier","text":"Using different algorithms Using different hyper parameters Using different training sets By manipulating input features By manipulating the class labels","title":"Methods for constructing ensemble classifier"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#types-of-ensemble-methods","text":"Simple methods: Max voting Averaging Weighted averaging Advanced methods: Bagging: Homogeneous weak learners, learning done independently from each other. Boosting: Homogeneous weak learners, learning done sequentially in a adaptive way. Stacking: Heterogeneous weak learners, learning done independently and combines them by training a meta model to output a prediction based on the different weak models predictions.","title":"Types of ensemble methods"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#bootstrap-sampling","text":"","title":"Bootstrap Sampling"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module6AML.html#bagging","text":"Tags: !AMLIndex","title":"Bagging"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html","text":"Module 7 # Unsupervised Learning # In this kind of learning we deal with unlabelled data. Unspuervised Learning Forms: 1. Clustering 2. Association RA 3. Outlier Analysis 4. Dimensionality Reduction (PCA) Clustering # Clustering aims to find grouping in data - Given a \\(X\\) find \\(K\\) clusters using data similarity - Minimize intra cluster distance and maximize inter cluster distance . We use proximity measures to know the distances. - The quality of a clustering result depends on both the similarity measure used by the method and its implementation Clustering is used in applications where dcouments need to categorized It is also used in recommendation system Types # Partitional Clustering: K Means Hierarchical Density Distribution based: Guassian Mixture Proximity Measure For Binary Attributes # Dissimilarity Between Binary Variables # - Manhattan: Sum of Absolute differences - Euclidean distance: Straight line distance between two data points - Supremum: Maximum absolute difference sum We need to normalize data to avoid some attributes do not contribute more than others Distance for mixed attribute types Gower Distance # Example # Cosine Similarity # K Means Algorithm # Clustering tendencies # All clustering algos find some clusters, even if the data does not have natural clusters or not Alternativuelt we can directly check the data for clustering tendency. A common approach is to use statistical tests for spatial randomness among data points We use Hopkins statistics helps in checking if the data is nearly uniformly distributed and applying the algo is not worth it. Silhouette score # Limitations # When clusters are of different sizes there is a chance that a big cluster can be split because K means assumes that the clusters are of the same size Different density within clusters can also cause the above mentioned issue Gaussian Mixture Models # We use multiple simple distributions and Tags: !AMLIndex","title":"Module 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#module-7","text":"","title":"Module 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#unsupervised-learning","text":"In this kind of learning we deal with unlabelled data. Unspuervised Learning Forms: 1. Clustering 2. Association RA 3. Outlier Analysis 4. Dimensionality Reduction (PCA)","title":"Unsupervised Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#clustering","text":"Clustering aims to find grouping in data - Given a \\(X\\) find \\(K\\) clusters using data similarity - Minimize intra cluster distance and maximize inter cluster distance . We use proximity measures to know the distances. - The quality of a clustering result depends on both the similarity measure used by the method and its implementation Clustering is used in applications where dcouments need to categorized It is also used in recommendation system","title":"Clustering"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#types","text":"Partitional Clustering: K Means Hierarchical Density Distribution based: Guassian Mixture","title":"Types"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#proximity-measure-for-binary-attributes","text":"","title":"Proximity Measure For Binary Attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#dissimilarity-between-binary-variables","text":"- Manhattan: Sum of Absolute differences - Euclidean distance: Straight line distance between two data points - Supremum: Maximum absolute difference sum We need to normalize data to avoid some attributes do not contribute more than others","title":"Dissimilarity Between Binary Variables"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#distance-for-mixed-attribute-types-gower-distance","text":"","title":"Distance for mixed attribute types Gower Distance"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#example","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#cosine-similarity","text":"","title":"Cosine Similarity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#k-means-algorithm","text":"","title":"K Means Algorithm"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#clustering-tendencies","text":"All clustering algos find some clusters, even if the data does not have natural clusters or not Alternativuelt we can directly check the data for clustering tendency. A common approach is to use statistical tests for spatial randomness among data points We use Hopkins statistics helps in checking if the data is nearly uniformly distributed and applying the algo is not worth it.","title":"Clustering tendencies"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#silhouette-score","text":"","title":"Silhouette score"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#limitations","text":"When clusters are of different sizes there is a chance that a big cluster can be split because K means assumes that the clusters are of the same size Different density within clusters can also cause the above mentioned issue","title":"Limitations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Applied%20ML/Module7AML.html#gaussian-mixture-models","text":"We use multiple simple distributions and Tags: !AMLIndex","title":"Gaussian Mixture Models"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/%21OOADIndex.html","text":"Object Oriented Analysis And Design # Assignments # OOADAssignment Question Papers # Mid Sem Paper OOAD End Sem Paper OOAD Course Content # Module 1 Module1OOAD Module 3 Module3OOAD Module 4 Module4OOAD Pre Recorded Module 1 PreRecordedModule1OOAD Pre Recorded Module 2 PreRecordedModule2OOAD Pre Recorded Module 3 PreRecordedModule3OOAD Pre Recorded Module 4 PreRecordedModule4OOAD Tags: !Semester2Index","title":"Object Oriented Analysis And Design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/%21OOADIndex.html#object-oriented-analysis-and-design","text":"","title":"Object Oriented Analysis And Design"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/%21OOADIndex.html#assignments","text":"OOADAssignment","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/%21OOADIndex.html#question-papers","text":"Mid Sem Paper OOAD End Sem Paper OOAD","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/%21OOADIndex.html#course-content","text":"Module 1 Module1OOAD Module 3 Module3OOAD Module 4 Module4OOAD Pre Recorded Module 1 PreRecordedModule1OOAD Pre Recorded Module 2 PreRecordedModule2OOAD Pre Recorded Module 3 PreRecordedModule3OOAD Pre Recorded Module 4 PreRecordedModule4OOAD Tags: !Semester2Index","title":"Course Content"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html","text":"End Sem OOAD Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !OOADIndex QuestionPapers","title":"End Sem OOAD Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html#end-sem-ooad-paper","text":"","title":"End Sem OOAD Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/End%20Sem%20Paper%20OOAD.html#question-4","text":"tags: !OOADIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Mid%20Sem%20Paper%20OOAD.html","text":"Mid Sem OOAD Paper # Question 1 # Question 2 # Question 3 # tags: !OOADIndex QuestionPapers","title":"Mid Sem OOAD Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Mid%20Sem%20Paper%20OOAD.html#mid-sem-ooad-paper","text":"","title":"Mid Sem OOAD Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Mid%20Sem%20Paper%20OOAD.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Mid%20Sem%20Paper%20OOAD.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Mid%20Sem%20Paper%20OOAD.html#question-3","text":"tags: !OOADIndex QuestionPapers","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html","text":"Module 1 # Terminologies # Model # Modeling Language # A system is a collection of subsystems organized by models which are inturn depicted by views. An Example # System: Aircraft Models: Flight Simulator, Scale Model Views: All blueprints, electrical wiring, fuel system The ':' used to differentiate the name and the type of the object Object Oriented Modeling # Why Models? # To abstract reality and show essential details and filter out the rest. To deal with complexity. To allow us to focus on the big picture. To understand requirements, design cleanly, more maintainable systems. Why Objects? # To more accurately reflect reality Reduce the semantic gap To localize changes Analysis Mode - models related to an investigation of the domain and problem space (Example is a use case model) Design Mode - models related to the solution (Example is a class diagram) Elements of modeling language # UML is strictly just notations UML is not a methodology UML is not a process UML is not proprietary It is a collection of diagrams for system visualization of Software architecture Behavior Physical system Views in UML # Use-case View: A view showing the functionality of the system as perceived by the external actors Logical View: A view showing the static structure and dynamic view Component View: A view showing the organization of the system Concurrency View Deployment View Intro to UML # Model ELements # Class Object State Use Case UML Diagrams # Structural Diagrams Behavioral Diagrams Use case diagram: External interaction with actors - # Class/Object Diagram: Captures static structural aspects, objects and relationships Class vs Object Diagram: State Diagram: Sequence Diagram: Collaboration Diagram: Activity Diagrams: Deployment Diagrams A process for Making Models # Iterative Development and the Unified Process # Unified Process # The critical idea is iterative development Iterative development is successively enlarging and refining a system through multiple iterations, using feedback and adaptation Each iteration will include requirements, analysis, design The requirements of a project are completely frozen before the design and development process commences. As this approach is not always feasible, there is also a need for flexible and adaptable agile methods that allow late changes in specifications RUP is a complete software development process framework developed by Rational Corporation It is an iterative development methodolofy based upon six industry proven best practices Phases in RUP # Inception Elaboration Construction Transition Iterations # Resource Histogram # Unified Process Best Practices # Inception # Formulate the scope of the project Plan and prepare the use case Synthasize candidate architecture Prepare the environment Inception Exit Criteria # Elaboration # An analysis is done to determine the risk, stability of ision of what the product is going to become - Products and artifacts described in the exit criteria of the previous phase - The plan apporved by the project Management Elaboration Exit Criteria # Construction # Construction Exit Criteria # Transition # - Test product in customer side - Fine tune the product based on customer - Deliver the product to customer Transition Exit Criteria # Advantages of RUP # Fully Dressed Format Tags: !OOADIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#module-1","text":"","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#terminologies","text":"","title":"Terminologies"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#model","text":"","title":"Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#modeling-language","text":"A system is a collection of subsystems organized by models which are inturn depicted by views.","title":"Modeling Language"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#an-example","text":"System: Aircraft Models: Flight Simulator, Scale Model Views: All blueprints, electrical wiring, fuel system The ':' used to differentiate the name and the type of the object","title":"An Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#object-oriented-modeling","text":"","title":"Object Oriented Modeling"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#why-models","text":"To abstract reality and show essential details and filter out the rest. To deal with complexity. To allow us to focus on the big picture. To understand requirements, design cleanly, more maintainable systems.","title":"Why Models?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#why-objects","text":"To more accurately reflect reality Reduce the semantic gap To localize changes Analysis Mode - models related to an investigation of the domain and problem space (Example is a use case model) Design Mode - models related to the solution (Example is a class diagram)","title":"Why Objects?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#elements-of-modeling-language","text":"UML is strictly just notations UML is not a methodology UML is not a process UML is not proprietary It is a collection of diagrams for system visualization of Software architecture Behavior Physical system","title":"Elements of modeling language"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#views-in-uml","text":"Use-case View: A view showing the functionality of the system as perceived by the external actors Logical View: A view showing the static structure and dynamic view Component View: A view showing the organization of the system Concurrency View Deployment View","title":"Views in UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#intro-to-uml","text":"","title":"Intro to UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#model-elements","text":"Class Object State Use Case","title":"Model ELements"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#uml-diagrams","text":"Structural Diagrams Behavioral Diagrams Use case diagram: External interaction with actors","title":"UML Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#-","text":"Class/Object Diagram: Captures static structural aspects, objects and relationships Class vs Object Diagram: State Diagram: Sequence Diagram: Collaboration Diagram: Activity Diagrams: Deployment Diagrams","title":"-"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#a-process-for-making-models","text":"","title":"A process for Making Models"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#iterative-development-and-the-unified-process","text":"","title":"Iterative Development and the Unified Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#unified-process","text":"The critical idea is iterative development Iterative development is successively enlarging and refining a system through multiple iterations, using feedback and adaptation Each iteration will include requirements, analysis, design The requirements of a project are completely frozen before the design and development process commences. As this approach is not always feasible, there is also a need for flexible and adaptable agile methods that allow late changes in specifications RUP is a complete software development process framework developed by Rational Corporation It is an iterative development methodolofy based upon six industry proven best practices","title":"Unified Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#phases-in-rup","text":"Inception Elaboration Construction Transition","title":"Phases in RUP"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#iterations","text":"","title":"Iterations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#resource-histogram","text":"","title":"Resource Histogram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#unified-process-best-practices","text":"","title":"Unified Process Best Practices"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#inception","text":"Formulate the scope of the project Plan and prepare the use case Synthasize candidate architecture Prepare the environment","title":"Inception"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#inception-exit-criteria","text":"","title":"Inception Exit Criteria"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#elaboration","text":"An analysis is done to determine the risk, stability of ision of what the product is going to become - Products and artifacts described in the exit criteria of the previous phase - The plan apporved by the project Management","title":"Elaboration"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#elaboration-exit-criteria","text":"","title":"Elaboration Exit Criteria"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#construction","text":"","title":"Construction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#construction-exit-criteria","text":"","title":"Construction Exit Criteria"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#transition","text":"- Test product in customer side - Fine tune the product based on customer - Deliver the product to customer","title":"Transition"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#transition-exit-criteria","text":"","title":"Transition Exit Criteria"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module1OOAD.html#advantages-of-rup","text":"Fully Dressed Format Tags: !OOADIndex","title":"Advantages of RUP"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html","text":"Module 3 # Operation Contract # Operation contract identifies system state changes when an operation happens. Define what each system operation does An operation is a single event from the system sequence diagram A domain model is used to help generate an operation contract When making an operation contract, think of the state of the system before the action and the state of the system after the action. The conditions both before and after the action should be described in the operation contract. The pre and post conditions describe state, not actions. Sections of an Operation Contract # Operation: Name of the operation and parameters Cross references: Use cases and scenarios this operation can occur within Preconditions: Noteworthy assumptions about the state of the system or objects in the domain model before execution of the operation Postconditions: This is the most important section. The state of objects in the domain model after completion of the operation Example of operation contracts # Main Success Scenario # Customer arrives at POS checkout with goods and/or services to purchase. Cashier starts a new sale. Cashier enters item identifier. System records sale line item and presents item description, price, and running total. Price calculated from a set of price rules. Cashier repeats steps 3-4 until indicates done. System presents total with taxes calculated. Cashier tells Customer the total, and asks for payment. Customer pays and System handles payment. POS Domain Model # Operation Contract for makeNewSale operation # Operation: makeNewSale() Cross References: Use case: Process sale Scenario: Process sale Preconditions: none Postconditions: a sale instance \"s\" was created (instance creation) s was associated with the register (association formed) attributes of s were initialized Operation Contract for enterItem operation # Operation: enterItem(itemID,quantity) Cross References: Use Case: Process Sale Scenario: Process Sale Preconditions: There is a sale underway. Postconditions: A salesLineItem instance sli was created (instance creation) sli was associated with the current Sale (association formed) sli.quantity became quantity (attribute modification) sli was associated with a ProductDescription, based on itemId match (association formed) Operation Contract for endSale operation # Operation: endSale() Cross references: Use case: Process sale Scenario: Process sale Preconditions: There is a sale underway Postconditions: s.isComplete became true (attribute modification) Operation Contract for makePayment operation # Operation: makePayment(amount:Money) Cross References: Use Case: Process Sale Scenario: Process Sale Preconditions: There is a sale underway Postconditions a payment instance \u201cp\u201d was created (instance creation) p.amountTendered became amount (attribute modification) p was associated with s:Sale (association former) s:Sale was associated with the Store (association formed) Tags: !OOADIndex","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#module-3","text":"","title":"Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#operation-contract","text":"Operation contract identifies system state changes when an operation happens. Define what each system operation does An operation is a single event from the system sequence diagram A domain model is used to help generate an operation contract When making an operation contract, think of the state of the system before the action and the state of the system after the action. The conditions both before and after the action should be described in the operation contract. The pre and post conditions describe state, not actions.","title":"Operation Contract"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#sections-of-an-operation-contract","text":"Operation: Name of the operation and parameters Cross references: Use cases and scenarios this operation can occur within Preconditions: Noteworthy assumptions about the state of the system or objects in the domain model before execution of the operation Postconditions: This is the most important section. The state of objects in the domain model after completion of the operation","title":"Sections of an Operation Contract"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#example-of-operation-contracts","text":"","title":"Example of operation contracts"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#main-success-scenario","text":"Customer arrives at POS checkout with goods and/or services to purchase. Cashier starts a new sale. Cashier enters item identifier. System records sale line item and presents item description, price, and running total. Price calculated from a set of price rules. Cashier repeats steps 3-4 until indicates done. System presents total with taxes calculated. Cashier tells Customer the total, and asks for payment. Customer pays and System handles payment.","title":"Main Success Scenario"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#pos-domain-model","text":"","title":"POS Domain Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#operation-contract-for-makenewsale-operation","text":"Operation: makeNewSale() Cross References: Use case: Process sale Scenario: Process sale Preconditions: none Postconditions: a sale instance \"s\" was created (instance creation) s was associated with the register (association formed) attributes of s were initialized","title":"Operation Contract for makeNewSale operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#operation-contract-for-enteritem-operation","text":"Operation: enterItem(itemID,quantity) Cross References: Use Case: Process Sale Scenario: Process Sale Preconditions: There is a sale underway. Postconditions: A salesLineItem instance sli was created (instance creation) sli was associated with the current Sale (association formed) sli.quantity became quantity (attribute modification) sli was associated with a ProductDescription, based on itemId match (association formed)","title":"Operation Contract for enterItem operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#operation-contract-for-endsale-operation","text":"Operation: endSale() Cross references: Use case: Process sale Scenario: Process sale Preconditions: There is a sale underway Postconditions: s.isComplete became true (attribute modification)","title":"Operation Contract for endSale operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module3OOAD.html#operation-contract-for-makepayment-operation","text":"Operation: makePayment(amount:Money) Cross References: Use Case: Process Sale Scenario: Process Sale Preconditions: There is a sale underway Postconditions a payment instance \u201cp\u201d was created (instance creation) p.amountTendered became amount (attribute modification) p was associated with s:Sale (association former) s:Sale was associated with the Store (association formed) Tags: !OOADIndex","title":"Operation Contract for makePayment operation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html","text":"Module 4 # Sequence Diagrams # Example Code and its sequence diagram # Public class A { Private BB myB = new BB (); Public void doOne () { myB . doTwo (); myB . doThree (); } } Sequence diagram for payment system # Sequence vs Communication # UML tools usually emphasize sequence Easier to see call flow sequence\u2014read from top to bottom Communication diagrams are more space-efficient Communication Diagrams expand top to bottom; Sequence Diagrams expand left to right Common Notation # Singleton Objects # Only one instance of the class instantiated at any time Lifeline Boxes # Lifeline is a dashed line below the object that creates a shortlived object Messages # Reply or Returns # Use the message syntax returnVar=Message(parameter) Using a reply or return message at the end of an activation bar Messages to self # We use a nested activation bar to satisfy this Condition and looping # Other frame operators # Relating interaction diagrams # Interaction occurances is a reference to an interaction within another interaction Polymorphic Messages # Communication Diagrams # Connection path between objects. Navigation and visibility. Instance of an association. Multiple messages and messages both ways Creation of instances # Sequencing # Conditional Messages # Mutually Exclusive Conditions # Iterations # Design Class Diagrams # During analysis emphasize domain concepts During design shift to software artifacts UML has no explicit notation for DCDs Uniform UML notation supports smoother development from analysis to design Domain model vs design class diagram # Developing DCD for POS DCD # Adding dependencies: # An instance of product description is used by a method in the sale class making sale dependent on the product description.: Composition (Whole part) relationship # an aggregation is shown with a hollow diamond instead of a fully coloured one. Association Classes # Interfaces and template classes # Tags: !OOADIndex","title":"Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#module-4","text":"","title":"Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#sequence-diagrams","text":"","title":"Sequence Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#example-code-and-its-sequence-diagram","text":"Public class A { Private BB myB = new BB (); Public void doOne () { myB . doTwo (); myB . doThree (); } }","title":"Example Code and its sequence diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#sequence-diagram-for-payment-system","text":"","title":"Sequence diagram for payment system"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#sequence-vs-communication","text":"UML tools usually emphasize sequence Easier to see call flow sequence\u2014read from top to bottom Communication diagrams are more space-efficient Communication Diagrams expand top to bottom; Sequence Diagrams expand left to right","title":"Sequence vs Communication"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#common-notation","text":"","title":"Common Notation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#singleton-objects","text":"Only one instance of the class instantiated at any time","title":"Singleton Objects"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#lifeline-boxes","text":"Lifeline is a dashed line below the object that creates a shortlived object","title":"Lifeline Boxes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#messages","text":"","title":"Messages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#reply-or-returns","text":"Use the message syntax returnVar=Message(parameter) Using a reply or return message at the end of an activation bar","title":"Reply or Returns"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#messages-to-self","text":"We use a nested activation bar to satisfy this","title":"Messages to self"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#condition-and-looping","text":"","title":"Condition and looping"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#other-frame-operators","text":"","title":"Other frame operators"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#relating-interaction-diagrams","text":"Interaction occurances is a reference to an interaction within another interaction","title":"Relating interaction diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#polymorphic-messages","text":"","title":"Polymorphic Messages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#communication-diagrams","text":"Connection path between objects. Navigation and visibility. Instance of an association. Multiple messages and messages both ways","title":"Communication Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#creation-of-instances","text":"","title":"Creation of instances"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#sequencing","text":"","title":"Sequencing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#conditional-messages","text":"","title":"Conditional Messages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#mutually-exclusive-conditions","text":"","title":"Mutually Exclusive Conditions"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#iterations","text":"","title":"Iterations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#design-class-diagrams","text":"During analysis emphasize domain concepts During design shift to software artifacts UML has no explicit notation for DCDs Uniform UML notation supports smoother development from analysis to design","title":"Design Class Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#domain-model-vs-design-class-diagram","text":"","title":"Domain model vs design class diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#developing-dcd-for-pos-dcd","text":"","title":"Developing DCD for POS DCD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#adding-dependencies","text":"An instance of product description is used by a method in the sale class making sale dependent on the product description.:","title":"Adding dependencies:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#composition-whole-part-relationship","text":"an aggregation is shown with a hollow diamond instead of a fully coloured one.","title":"Composition (Whole part) relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#association-classes","text":"","title":"Association Classes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/Module4OOAD.html#interfaces-and-template-classes","text":"Tags: !OOADIndex","title":"Interfaces and template classes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/OOADAssignment.html","text":"OBJECT ORIENTED ANALYSIS AND DESIGN ASSIGNMENT # Instructions: 1. One can use any open source and/or proprietary software for the construction of UML state-chart-diagram. Hand drawn diagrams on a A4 sized paper are also allowed. 2. Use the concepts covered in the class for construction of an optimized diagram. 3. Copy cases direct/indirect (identified if any) will be awarded zero marks. Draw a state chart diagram for the following application description: Consider an online shopping website which sells books on different subjects. The user interface of their web application has three different modes. In the first mode, that is, search-browse-choose (sbc) mode the user can perform the following: - View a precompiled list of books like bestsellers, food, stories, etc. - Search for books by author name, or keywords in the title of the book. - User can retrieve another list while viewing one list. - The user can click on a book to get its description (ISBN, Author, Title, Year, Pages, Price etc.). - The user can add the item they are viewing in the shopping cart or move back to the list they were browsing. - The user can switch the sbc mode to decide-and-order (do) mode. In the do mode, the user is requested to verify his/her cart. In this mode the user can leave the cart as it is or they can remove the added items. Once the users confirm the cart items they switch from do mode to personal information entry (pie) mode. In the pie mode, the user provides his/her personal information for payment and delivery. The application is very flexible and allows the user to switch to previous mode from the current mode or exit. My Submission # My Submission can be found here tags: !OOADIndex Assignments","title":"OBJECT ORIENTED ANALYSIS AND DESIGN ASSIGNMENT"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/OOADAssignment.html#object-oriented-analysis-and-design-assignment","text":"Instructions: 1. One can use any open source and/or proprietary software for the construction of UML state-chart-diagram. Hand drawn diagrams on a A4 sized paper are also allowed. 2. Use the concepts covered in the class for construction of an optimized diagram. 3. Copy cases direct/indirect (identified if any) will be awarded zero marks. Draw a state chart diagram for the following application description: Consider an online shopping website which sells books on different subjects. The user interface of their web application has three different modes. In the first mode, that is, search-browse-choose (sbc) mode the user can perform the following: - View a precompiled list of books like bestsellers, food, stories, etc. - Search for books by author name, or keywords in the title of the book. - User can retrieve another list while viewing one list. - The user can click on a book to get its description (ISBN, Author, Title, Year, Pages, Price etc.). - The user can add the item they are viewing in the shopping cart or move back to the list they were browsing. - The user can switch the sbc mode to decide-and-order (do) mode. In the do mode, the user is requested to verify his/her cart. In this mode the user can leave the cart as it is or they can remove the added items. Once the users confirm the cart items they switch from do mode to personal information entry (pie) mode. In the pie mode, the user provides his/her personal information for payment and delivery. The application is very flexible and allows the user to switch to previous mode from the current mode or exit.","title":"OBJECT ORIENTED ANALYSIS AND DESIGN ASSIGNMENT"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/OOADAssignment.html#my-submission","text":"My Submission can be found here tags: !OOADIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html","text":"Pre Recorded Lecture Module 1 # Programming paradigms # OO programming attempts to model interacting components to build a solution. This breaking down of final solution to interacting components makes it easy to maintain software. System Development Life Cycle SDLC # Analyze: Design Implement Test Deployment Maintenance SDLC Models # Waterfall Model # All the stages are done one after the other from top to bottom. This means that all requirements are gathered first then it is all analyzed and then the design has started. Now it is difficult to add another requirement from the customer till the acceptance phase. Advantages # Simple in approach Disadvantages # Sequential in design no space for concurrency It is not change friendly Poor resource utilization, since any one stage is busy the others are on standstill. Unified Model # For complex systems the linear approach of waterfall model does not suffice The UP is a process for object oriented UML is compulsory to use for Modeling Development is organized into a series of iterations The outcome of these iterations is a tested integrated and executable system An iteration represents a complete dev cycle with its own analysis, design, implementation and testing. Iterations are short and rapid feedback and adaptation is done Iterations are timeboxed and the recommended iteration duration is about two to six weeks Unified Process # The iterative lifecycle is based on refinement of a system through multiple iterations with feedback and adaptation The system grows incrementally over time, iteration by iteration The system may not be eligible or production deployment until many iterations The ouput of an iteration is not an experimental prototype but a production subset of the final system Each iteration tackles new requirements and incrementally extends the system An iteration may occasionally revisit existing software and improve it. Embracing change # UP is change friendly Periodic testing and design based on rapid feedback leads to better product Gives developers a better understanding of the customers requirements Phases in UP # Inception Elaboration Construction Transition Agile Principles # Effective response to change Effective communication among all stakeholders Drawing the customer onto the team Organizing a team so that it is in control of the work performed Rapid incremental delivery of software Is driven by customer descriptions called user stories Recognizes that plans are short lived Develops software iteratively with a heavy emphasis on construction activities Delivers multiple software increments Adapts as changes occur Welcomes changing requirements even in late development Promotes face to face interactions between stakeholders Agile Manifesto # Test Driven Development # TDD is a technique whereby you write you test cases before you write any implementation code Tests drive or dictate the code that is developed Leads to better design Better code documentation More productive Promotes good design UML # Unified Modeling language Modeling language to draw diagrams in a uniform way Class and Object in UML # Attributes in a class and object are shown as: Operations are the things an object can do. In the programming domain these are basically methods that are implemented in a class Relationships in UML # IS-A Inheritance: Superclass and subclass # A class may be the subset of another class A saving account is also an account making it a subclass of account class But at the same time not all accounts are savings account HAS-A Aggregation # An object may be part of another object Tags: !OOADIndex","title":"Pre Recorded Lecture Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#pre-recorded-lecture-module-1","text":"","title":"Pre Recorded Lecture Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#programming-paradigms","text":"OO programming attempts to model interacting components to build a solution. This breaking down of final solution to interacting components makes it easy to maintain software.","title":"Programming paradigms"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#system-development-life-cycle-sdlc","text":"Analyze: Design Implement Test Deployment Maintenance","title":"System Development Life Cycle SDLC"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#sdlc-models","text":"","title":"SDLC Models"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#waterfall-model","text":"All the stages are done one after the other from top to bottom. This means that all requirements are gathered first then it is all analyzed and then the design has started. Now it is difficult to add another requirement from the customer till the acceptance phase.","title":"Waterfall Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#advantages","text":"Simple in approach","title":"Advantages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#disadvantages","text":"Sequential in design no space for concurrency It is not change friendly Poor resource utilization, since any one stage is busy the others are on standstill.","title":"Disadvantages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#unified-model","text":"For complex systems the linear approach of waterfall model does not suffice The UP is a process for object oriented UML is compulsory to use for Modeling Development is organized into a series of iterations The outcome of these iterations is a tested integrated and executable system An iteration represents a complete dev cycle with its own analysis, design, implementation and testing. Iterations are short and rapid feedback and adaptation is done Iterations are timeboxed and the recommended iteration duration is about two to six weeks","title":"Unified Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#unified-process","text":"The iterative lifecycle is based on refinement of a system through multiple iterations with feedback and adaptation The system grows incrementally over time, iteration by iteration The system may not be eligible or production deployment until many iterations The ouput of an iteration is not an experimental prototype but a production subset of the final system Each iteration tackles new requirements and incrementally extends the system An iteration may occasionally revisit existing software and improve it.","title":"Unified Process"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#embracing-change","text":"UP is change friendly Periodic testing and design based on rapid feedback leads to better product Gives developers a better understanding of the customers requirements","title":"Embracing change"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#phases-in-up","text":"Inception Elaboration Construction Transition","title":"Phases in UP"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#agile-principles","text":"Effective response to change Effective communication among all stakeholders Drawing the customer onto the team Organizing a team so that it is in control of the work performed Rapid incremental delivery of software Is driven by customer descriptions called user stories Recognizes that plans are short lived Develops software iteratively with a heavy emphasis on construction activities Delivers multiple software increments Adapts as changes occur Welcomes changing requirements even in late development Promotes face to face interactions between stakeholders","title":"Agile Principles"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#agile-manifesto","text":"","title":"Agile Manifesto"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#test-driven-development","text":"TDD is a technique whereby you write you test cases before you write any implementation code Tests drive or dictate the code that is developed Leads to better design Better code documentation More productive Promotes good design","title":"Test Driven Development"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#uml","text":"Unified Modeling language Modeling language to draw diagrams in a uniform way","title":"UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#class-and-object-in-uml","text":"Attributes in a class and object are shown as: Operations are the things an object can do. In the programming domain these are basically methods that are implemented in a class","title":"Class and Object in UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#relationships-in-uml","text":"","title":"Relationships in UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#is-a-inheritance-superclass-and-subclass","text":"A class may be the subset of another class A saving account is also an account making it a subclass of account class But at the same time not all accounts are savings account","title":"IS-A Inheritance: Superclass and subclass"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule1OOAD.html#has-a-aggregation","text":"An object may be part of another object Tags: !OOADIndex","title":"HAS-A Aggregation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html","text":"Pre Recorded Lecture Module 2 # Requirement Categorization # Functional Requirements are the features and capabilities Non functional requirements Usability Reliability Performance Supportability What is a Use case diagram and Use cases? # It shows interaction among actors and use cases of the system It is not data flow diagram Significance of use case diagram Use casees is detailing of the use case diagram Use cases are textual artifacts Use case depicts functional requirements Use case diagram for a Point of Sale Case Study # Use case types and formats # Use cases may be written in three formality types Brief Casual Fully dressed Styles of use cases # Essential : Focus is on intend Avoid making UI decisions Basic functionality is given importance in this style Concrete : UI decisions are embedded in the use case text. This style is not suitable for early requirement analysis. Tags: !OOADIndex","title":"Pre Recorded Lecture Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#pre-recorded-lecture-module-2","text":"","title":"Pre Recorded Lecture Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#requirement-categorization","text":"Functional Requirements are the features and capabilities Non functional requirements Usability Reliability Performance Supportability","title":"Requirement Categorization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#what-is-a-use-case-diagram-and-use-cases","text":"It shows interaction among actors and use cases of the system It is not data flow diagram Significance of use case diagram Use casees is detailing of the use case diagram Use cases are textual artifacts Use case depicts functional requirements","title":"What is a Use case diagram and Use cases?"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#use-case-diagram-for-a-point-of-sale-case-study","text":"","title":"Use case diagram for a Point of Sale Case Study"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#use-case-types-and-formats","text":"Use cases may be written in three formality types Brief Casual Fully dressed","title":"Use case types and formats"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule2OOAD.html#styles-of-use-cases","text":"Essential : Focus is on intend Avoid making UI decisions Basic functionality is given importance in this style Concrete : UI decisions are embedded in the use case text. This style is not suitable for early requirement analysis. Tags: !OOADIndex","title":"Styles of use cases"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html","text":"Pre Recorded Lecture Module 3 # Domain Model # Illustrates meaningful conc3ets in a problem domain It is a representation of real world things, not software components It is a set of static structure diagrams; no operation are defined. It may show: Concepts Associations between concepts Attribute of concepts Every domain concept has a name and a set of attributes Domain Model in UML # Reading direction by default is either top -> down or left -> right Domain Concepts in POS # Strategy to identify conceptual classes # Use noun phrase identification In textual descriptions of the problem domain and consider them as concepts or attributes Use cases are excellent descriptions for this analysis Consider the following scenario: Customer, POS checkout, items, cashier, quantity, system, item price, item info, sale all these are nouns item info, item price and quantity are all nouns that are attributes for the domain concept. Adding Associations # Multiplicity # Adding Attributes # Sample Domain Model # System Sequence Diagram # Investigate the behaviour of the ysstem as a blackbox System behaviour is a description of what the system does Use cases describe how external actors interact with the system. During this interaction an actor generates events A request even initiates an operation upon the system SSD for Process Sale Scenario # Significance of SSD # Shows interaction of the system with the outside world It is used to depict how system responds to external events It plays key role in GUI design of the system Operation Contract # Writing an OC for a sale line item # Tags: !OOADIndex","title":"Pre Recorded Lecture Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#pre-recorded-lecture-module-3","text":"","title":"Pre Recorded Lecture Module 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#domain-model","text":"Illustrates meaningful conc3ets in a problem domain It is a representation of real world things, not software components It is a set of static structure diagrams; no operation are defined. It may show: Concepts Associations between concepts Attribute of concepts Every domain concept has a name and a set of attributes","title":"Domain Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#domain-model-in-uml","text":"Reading direction by default is either top -> down or left -> right","title":"Domain Model in UML"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#domain-concepts-in-pos","text":"","title":"Domain Concepts in POS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#strategy-to-identify-conceptual-classes","text":"Use noun phrase identification In textual descriptions of the problem domain and consider them as concepts or attributes Use cases are excellent descriptions for this analysis Consider the following scenario: Customer, POS checkout, items, cashier, quantity, system, item price, item info, sale all these are nouns item info, item price and quantity are all nouns that are attributes for the domain concept.","title":"Strategy to identify conceptual classes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#adding-associations","text":"","title":"Adding Associations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#multiplicity","text":"","title":"Multiplicity"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#adding-attributes","text":"","title":"Adding Attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#sample-domain-model","text":"","title":"Sample Domain Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#system-sequence-diagram","text":"Investigate the behaviour of the ysstem as a blackbox System behaviour is a description of what the system does Use cases describe how external actors interact with the system. During this interaction an actor generates events A request even initiates an operation upon the system","title":"System Sequence Diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#ssd-for-process-sale-scenario","text":"","title":"SSD for Process Sale Scenario"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#significance-of-ssd","text":"Shows interaction of the system with the outside world It is used to depict how system responds to external events It plays key role in GUI design of the system","title":"Significance of SSD"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#operation-contract","text":"","title":"Operation Contract"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule3OOAD.html#writing-an-oc-for-a-sale-line-item","text":"Tags: !OOADIndex","title":"Writing an OC for a sale line item"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html","text":"Pre Recorded Lecture Module 4 # Relating use cases # Includes invokes a new use case Extends tries to relate an exception In the above example we can see that Rent items extends Sale Items Derived Attributes # During analysis step, analyst only records very obvious attributes During design phase there are more refined attributes that can be derived from another attribute Interaction Diagram # Interaction diagrams show relationship between objects, hence we see only objects interacting in the diagram (Anonymous or named objects) and no classes. Collaboration Diagram # - When few messages are involved then we can use a collaboration diagram - Having many objects then collaboration diagram is a better way to go Sequence Diagram # - When the number of messages are a lot and there is a sequence of messages then it is better to use sequence diagrams. - Only limitation is that a lot of objects cannot be placed horizontally. Self or this relation in messages # Creation of instances # In a collaboration diagram In a sequence diagram Condition Messages # Iterations # Make Payment Scenario in PoS # State Transition Diagrams # Tags: !OOADIndex","title":"Pre Recorded Lecture Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#pre-recorded-lecture-module-4","text":"","title":"Pre Recorded Lecture Module 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#relating-use-cases","text":"Includes invokes a new use case Extends tries to relate an exception In the above example we can see that Rent items extends Sale Items","title":"Relating use cases"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#derived-attributes","text":"During analysis step, analyst only records very obvious attributes During design phase there are more refined attributes that can be derived from another attribute","title":"Derived Attributes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#interaction-diagram","text":"Interaction diagrams show relationship between objects, hence we see only objects interacting in the diagram (Anonymous or named objects) and no classes.","title":"Interaction Diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#collaboration-diagram","text":"- When few messages are involved then we can use a collaboration diagram - Having many objects then collaboration diagram is a better way to go","title":"Collaboration Diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#sequence-diagram","text":"- When the number of messages are a lot and there is a sequence of messages then it is better to use sequence diagrams. - Only limitation is that a lot of objects cannot be placed horizontally.","title":"Sequence Diagram"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#self-or-this-relation-in-messages","text":"","title":"Self or this relation in messages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#creation-of-instances","text":"In a collaboration diagram In a sequence diagram","title":"Creation of instances"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#condition-messages","text":"","title":"Condition Messages"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#iterations","text":"","title":"Iterations"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#make-payment-scenario-in-pos","text":"","title":"Make Payment Scenario in PoS"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Object%20Oriented%20Analysis%20And%20Design/PreRecordedModule4OOAD.html#state-transition-diagrams","text":"Tags: !OOADIndex","title":"State Transition Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/%21SoftwareArchitecturesIndex.html","text":"Software Architectures # Assignments # Assignment 1 SAAssignment1 Assignment 2 SAAssignment2 Question Papers # Mid Sem Paper SA End Sem Paper SA Course Content # Module 1 Module1SA Module 2 Module2SA Module 6 Module6SA Module 8 Module8SA Pre Recorded Module 1 PreRecordedModule1SA Tags: !Semester2Index","title":"Software Architectures"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/%21SoftwareArchitecturesIndex.html#software-architectures","text":"","title":"Software Architectures"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/%21SoftwareArchitecturesIndex.html#assignments","text":"Assignment 1 SAAssignment1 Assignment 2 SAAssignment2","title":"Assignments"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/%21SoftwareArchitecturesIndex.html#question-papers","text":"Mid Sem Paper SA End Sem Paper SA","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/%21SoftwareArchitecturesIndex.html#course-content","text":"Module 1 Module1SA Module 2 Module2SA Module 6 Module6SA Module 8 Module8SA Pre Recorded Module 1 PreRecordedModule1SA Tags: !Semester2Index","title":"Course Content"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html","text":"End Sem SA Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !SoftwareArchitecturesIndex QuestionPapers","title":"End Sem SA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html#end-sem-sa-paper","text":"","title":"End Sem SA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/End%20Sem%20Paper%20SA.html#question-4","text":"tags: !SoftwareArchitecturesIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html","text":"Mid Sem SA Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !SoftwareArchitecturesIndex QuestionPapers","title":"Mid Sem SA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html#mid-sem-sa-paper","text":"","title":"Mid Sem SA Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Mid%20Sem%20Paper%20SA.html#question-4","text":"tags: !SoftwareArchitecturesIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html","text":"Module 1 # What is Software Architecture # The architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both. Many architectural decisions are made early Many decisions are made early that are not architectural Its hard to look at a decision and tell whether or not its \"major\" Structures on the other had are fairly easy to identify in software, and they form a powerful tool for system design. A structure is a set of lements held together by a relation Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures Module Component and Connector Allocation Modules # Structures can be partitioned into implementation units called modules Modules are assigned specific computational responsibilities, and are the basis of work assignments Component and Connector # Other structures focus on the way the elements interact with each other at runtime to carry out the system's functions Allocation # Allocation structures describe the mapping from software structures to the system's environments Tags: !SoftwareArchitecturesIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html#module-1","text":"","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html#what-is-software-architecture","text":"The architecture of a system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both. Many architectural decisions are made early Many decisions are made early that are not architectural Its hard to look at a decision and tell whether or not its \"major\" Structures on the other had are fairly easy to identify in software, and they form a powerful tool for system design. A structure is a set of lements held together by a relation Software systems are composed of many structures, and no single structure holds claim to being the architecture. There are three important categories of architectural structures Module Component and Connector Allocation","title":"What is Software Architecture"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html#modules","text":"Structures can be partitioned into implementation units called modules Modules are assigned specific computational responsibilities, and are the basis of work assignments","title":"Modules"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html#component-and-connector","text":"Other structures focus on the way the elements interact with each other at runtime to carry out the system's functions","title":"Component and Connector"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module1SA.html#allocation","text":"Allocation structures describe the mapping from software structures to the system's environments Tags: !SoftwareArchitecturesIndex","title":"Allocation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html","text":"Module 2 # Availability # Availability General Scenario # Sample Concrete Availability Scenario # The heartbeat monitor determines that the server is non responsive during normal operations. The system informs the admin when the heartbeat stops. Check the following for more info Module2SA#Module 2#Availability Tactics#Detect Faults Goal of Availability tactics # Availability Tactics # Detect Faults # Ping/echo: asynchronous req/res message pair exchanged between nodes, used to determine reachability and the round trip delay through the associated network path Monitor: A component used to monitor the state of health of other parts of the system. A system monitor can detect failure or congestion in the network or other shared resources such as from a denial of service attack Heartbeat: a periodic message exchange between a system monitor and a process being monitored Timestamp: Used to detect incorrect sequences of events, in distributed message passing systems Sanity Checking: Checks the validity or reasonableness of a component's operations or outputs; typically based on a knowledge of the internal design. Condition Monitoring: Checking conditions in a process or device, or validating assumptions made during the design. Voting: to check that replicated components are producing the same results. Comes in various flavors: replication, functional redundancy, analytic redundancy. Example is height calculation in an aircraft. There are several ways we can determine this, we now vote all these results and check against a particular tolerance and when the majority of the values are within that then we can choose that value and determine the component that is failing. Exception Detection: detection of a system condition that alters the normal flow of execution, eg system exception parameter fence, parameter typing, timeout. Self test: procedure for a component to test itself for correct operation. Recover From Faults # Active redundancy (Hot spare): A spare processes inputs just like the active one so that the spares are synchronous in case of failure Passive redundancy (Warm spare): Only active nodes process inputs and the spares are brought up when the active ones fail Spare (Cold spare): Redundant spares are OOS until a failure happens at which point a power on reset procedure is initiated on the spare prior to its being placed in service Exception handling: dealing with the exception by masking it by correcting it Rollback: Revert to a previous known good state. Software Upgrade: in service upgrades to executable code images in a non service affecting manner Retry Ignore Faulty Behavior: For example ignoring spurious messages that can cause failure Degradation Reconfigure Shadow: operating a previously gfailed or in service upgraded component in a shoadow mode for a predefined time prior to reverting the component back o an active role State resync Prevent Faults # Escalating restart: recover from faults by varying the granularity of the components restarted and minimizing the level of service affected Non stop forwarding: Functionality is split into supervisory and data. Removal from service: Temporarily placing a system in an OOS state for the purpose of mitigating potential system failures Transactions: bundling state updates so that async messages exchanged between distributed components are atomic, consistent isolated and durable Predictive model: monitor the state of health of a process Exception Prevention: preventing system exceptions Design Checklist for Availability # Usability # Usability is concerned with how east it is to accomplish a desired task by the user A focus on usability is the cheapest and easiest ways to improve a systems quality. Usability comprises of: Learning system features Using a system efficiently Minimizing impact of errors Adapting to failures Usability General Scenario # POS Terminal # Actors: Cashier Customer Supervisor High level vs low level goals # log out: Secondary goal Handle payment: Primary goal Negotiate contract with supplier: Very high level goal These use cases are at different levels, and are the all valid Extends relationship # Specialization relationship # Drawing System Sequence Diagrams # Example POS terminal Process Sale scenario Iterations are enclosed in one box Domain Model # Finding concepts # Tags: !SoftwareArchitecturesIndex","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#module-2","text":"","title":"Module 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#availability","text":"","title":"Availability"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#availability-general-scenario","text":"","title":"Availability General Scenario"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#sample-concrete-availability-scenario","text":"The heartbeat monitor determines that the server is non responsive during normal operations. The system informs the admin when the heartbeat stops. Check the following for more info Module2SA#Module 2#Availability Tactics#Detect Faults","title":"Sample Concrete Availability Scenario"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#goal-of-availability-tactics","text":"","title":"Goal of Availability tactics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#availability-tactics","text":"","title":"Availability Tactics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#detect-faults","text":"Ping/echo: asynchronous req/res message pair exchanged between nodes, used to determine reachability and the round trip delay through the associated network path Monitor: A component used to monitor the state of health of other parts of the system. A system monitor can detect failure or congestion in the network or other shared resources such as from a denial of service attack Heartbeat: a periodic message exchange between a system monitor and a process being monitored Timestamp: Used to detect incorrect sequences of events, in distributed message passing systems Sanity Checking: Checks the validity or reasonableness of a component's operations or outputs; typically based on a knowledge of the internal design. Condition Monitoring: Checking conditions in a process or device, or validating assumptions made during the design. Voting: to check that replicated components are producing the same results. Comes in various flavors: replication, functional redundancy, analytic redundancy. Example is height calculation in an aircraft. There are several ways we can determine this, we now vote all these results and check against a particular tolerance and when the majority of the values are within that then we can choose that value and determine the component that is failing. Exception Detection: detection of a system condition that alters the normal flow of execution, eg system exception parameter fence, parameter typing, timeout. Self test: procedure for a component to test itself for correct operation.","title":"Detect Faults"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#recover-from-faults","text":"Active redundancy (Hot spare): A spare processes inputs just like the active one so that the spares are synchronous in case of failure Passive redundancy (Warm spare): Only active nodes process inputs and the spares are brought up when the active ones fail Spare (Cold spare): Redundant spares are OOS until a failure happens at which point a power on reset procedure is initiated on the spare prior to its being placed in service Exception handling: dealing with the exception by masking it by correcting it Rollback: Revert to a previous known good state. Software Upgrade: in service upgrades to executable code images in a non service affecting manner Retry Ignore Faulty Behavior: For example ignoring spurious messages that can cause failure Degradation Reconfigure Shadow: operating a previously gfailed or in service upgraded component in a shoadow mode for a predefined time prior to reverting the component back o an active role State resync","title":"Recover From Faults"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#prevent-faults","text":"Escalating restart: recover from faults by varying the granularity of the components restarted and minimizing the level of service affected Non stop forwarding: Functionality is split into supervisory and data. Removal from service: Temporarily placing a system in an OOS state for the purpose of mitigating potential system failures Transactions: bundling state updates so that async messages exchanged between distributed components are atomic, consistent isolated and durable Predictive model: monitor the state of health of a process Exception Prevention: preventing system exceptions","title":"Prevent Faults"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#design-checklist-for-availability","text":"","title":"Design Checklist for Availability"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#usability","text":"Usability is concerned with how east it is to accomplish a desired task by the user A focus on usability is the cheapest and easiest ways to improve a systems quality. Usability comprises of: Learning system features Using a system efficiently Minimizing impact of errors Adapting to failures","title":"Usability"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#usability-general-scenario","text":"","title":"Usability General Scenario"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#pos-terminal","text":"Actors: Cashier Customer Supervisor","title":"POS Terminal"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#high-level-vs-low-level-goals","text":"log out: Secondary goal Handle payment: Primary goal Negotiate contract with supplier: Very high level goal These use cases are at different levels, and are the all valid","title":"High level vs low level goals"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#extends-relationship","text":"","title":"Extends relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#specialization-relationship","text":"","title":"Specialization relationship"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#drawing-system-sequence-diagrams","text":"Example POS terminal Process Sale scenario Iterations are enclosed in one box","title":"Drawing System Sequence Diagrams"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#domain-model","text":"","title":"Domain Model"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module2SA.html#finding-concepts","text":"Tags: !SoftwareArchitecturesIndex","title":"Finding concepts"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html","text":"Module 6 # Architectural Patterns # Properties of patterns # Addresses a recurring design problem that arises in specific design situations and presents a solution to it Document existing well proven design experience Identify and specify abstractions at the high level Helps to build complex systems Manage software complexity A set of guidelines that help to get a good design are: 1. Avoid rigidity (Hard to change) 2. Avoid Fragility (Whenever the design changes, it should not break) 3. Avoid Immobility (Cannot be reused) OO Design Principles # Open Close: Open to extension and close for modification Template and strategy pattern Dependency Inversion: Decouple two module dependencies Adapter pattern Liskov's Substitution: Superclass can be replaced by subclass Interface Segregation: Interfaces are made only for a particular purpose Single responsibilies: One class for only one task Three part schema for a pattern # Context # A scenario or situation where design problem arises Ideally the scenario should be generic but it might not always be possible Example Developing messaging solution for mobile applications Developing software for a man machine interface Problem # Start with a problem statement that captures the theme Completed by forces A requirement A constraint A desirable property The forces can complement or contradict. This can be used to choose what force needs to be taken into consideration and what should not be Solution # It is a configuration to balance the forces mentioned earlier Like structure with components and relationships Run time behavior Structure: Addresses static part of the solution Run time: Behavior while running Problem Categories # Layering Pattern # Example # Layers # Services of Layer J are only used by Layer J + 1 No further direct dependencies between layers Each layer can have many components The following is used to define the layer abstraction criteria - Most generic components are in the lowest layer and domain specific components are in the top layer - More stable components are in lower layers. - Distance from hardware is also used in determining the layers Construction of layer # Use black box approach to specify layer interface Structure each layer Identify components in each layer Bridge or strategy pattern can help Examples # Summary # Pipe and Filter Pattern # Filter # Has interfaces from which a set of inputs flow in and set of outputs flow out Processing step is in a filter component Independent entities Does not share state with other filters Pipes # Data is passed through pipes between adjacent filters Stateless data stream Source end feeds filter input and sink receives output 3 Part schema # A simple case # Passive Filters Active Filters Buffered Pipe for synchronizing Components of Pipe and Filter # Implementation Steps # Pros and Cons # No intermediate files necessary Filter addition replacement and reuse Rapid prototyping Concurrent execution Certain analysis possible like deadlock Throughput and latency checks Data transformation overhead Error handling is a challenge Sharing state is expensive Lowest common denominator on data transmission determines the overall throughput Examples # Most PaaS providers have message oriented service orchestration Azure has queing services Amazon SQS is used to communicate between EC2 instances Blackboard Pattern # Title Details Context A set of heterogeneous specialized modules which dynamically change their strategies as a response to unpredictable events Problem When there is no deterministic solutions to process Solutions to partial problems require representation No predetermined strat is present to solve a problem Dealing with uncertain knowledge Forces A complete search of solution is not possible Different algos used for partial solutions One algorithm uses results of another algo No strict sequence between algos Examples # Speech recognition Vehicle Identification Robot Control Modern Machine Learning algos for complex task OCR based text recognition Components # Central data structures Control plan encapsulates information necessary to run the system Domain KS are concerned with the solving of domain specific problems Control KS adapt the current control plan to the current situation Pros and cons # Distributed System Patterns # Title Description Context Complex environment of systems to take advantage of computing power via clusters A software may be available only on a specific computer Due security we might have to run different parts in different systems Some services are provided by business partners over internet Problem To build a sw system as a set of decoupled interoperating components rather than a monolith Require a flexible means of interprocess communication Forces It should be possible to distribute components during deployment Need to exchange add or remove components at run time Architecture should hide system specific and implementation specific details from users of components and services Broker Pattern # A broker component helps in achieving better decoupling of clients and servers - Servers: Register themselves with the broker and make their services available - Clients: access the functionality of servers by sending requests via the broker The broker: - Locating the appropriate server and forwarding a request to that server - Transmitting results and exceptions back to the client Implementation # Define an object model or use an existing one Decide which kind of component interoperability the system should offer Specify the APIs the broker component provides for collaborating Use proxy objects to hide implementation details from clients and servers Design the broker component in parallel with steps 3 and Develop IDL compilers CORBA # CORBA is the oldest amongst the middleware technologies used in today's IT world CORBA stands for Common Object Request Broker Architecture CORBA supports Dynamic Invocation Interface (DII) Remote Method Invocation # Sun's Java RMI is based on the transparent Broker variant pattern Client side proxy and server side invoker have to be created manually by am additional compilation step The service interface is written in Java RMI is limited to usage in Java To establish interoperability RMI IIOP is provided Benefits # Location independence Type system transparency (Marshaling and unmarshaling are done) Isolation Separation of concern Resource management Portability Liability # Error handling Overhead since developers do not know about the location of objects Performance Lower fault tolerance (server / broker failures) Model View Controller # MVC Dynamics # MVC Initialization # AJAX application as MVC # Benefits # Multiple views can be done for the same model Synchronized views Pluggable views and controllers Exchangeability of look and feel Framework potential Liabilities # Increased complexity Potential execssive number of updates Inefficiency of data access in view Difficulty of using MVC with modern user interface tools Micro Kernel # Context # The development of several applications that use similar programming interfaces that build on the same core functionality Problem # Developing software for an application domain that needs to cope with a broad spectrum of similar standards and technologies Continuous evolution, platform must be extensible, portable and adaptable Tags: !SoftwareArchitecturesIndex","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#module-6","text":"","title":"Module 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#architectural-patterns","text":"","title":"Architectural Patterns"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#properties-of-patterns","text":"Addresses a recurring design problem that arises in specific design situations and presents a solution to it Document existing well proven design experience Identify and specify abstractions at the high level Helps to build complex systems Manage software complexity A set of guidelines that help to get a good design are: 1. Avoid rigidity (Hard to change) 2. Avoid Fragility (Whenever the design changes, it should not break) 3. Avoid Immobility (Cannot be reused)","title":"Properties of patterns"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#oo-design-principles","text":"Open Close: Open to extension and close for modification Template and strategy pattern Dependency Inversion: Decouple two module dependencies Adapter pattern Liskov's Substitution: Superclass can be replaced by subclass Interface Segregation: Interfaces are made only for a particular purpose Single responsibilies: One class for only one task","title":"OO Design Principles"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#three-part-schema-for-a-pattern","text":"","title":"Three part schema for a pattern"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#context","text":"A scenario or situation where design problem arises Ideally the scenario should be generic but it might not always be possible Example Developing messaging solution for mobile applications Developing software for a man machine interface","title":"Context"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#problem","text":"Start with a problem statement that captures the theme Completed by forces A requirement A constraint A desirable property The forces can complement or contradict. This can be used to choose what force needs to be taken into consideration and what should not be","title":"Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#solution","text":"It is a configuration to balance the forces mentioned earlier Like structure with components and relationships Run time behavior Structure: Addresses static part of the solution Run time: Behavior while running","title":"Solution"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#problem-categories","text":"","title":"Problem Categories"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#layering-pattern","text":"","title":"Layering Pattern"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#example","text":"","title":"Example"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#layers","text":"Services of Layer J are only used by Layer J + 1 No further direct dependencies between layers Each layer can have many components The following is used to define the layer abstraction criteria - Most generic components are in the lowest layer and domain specific components are in the top layer - More stable components are in lower layers. - Distance from hardware is also used in determining the layers","title":"Layers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#construction-of-layer","text":"Use black box approach to specify layer interface Structure each layer Identify components in each layer Bridge or strategy pattern can help","title":"Construction of layer"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#examples","text":"","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#summary","text":"","title":"Summary"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#pipe-and-filter-pattern","text":"","title":"Pipe and Filter Pattern"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#filter","text":"Has interfaces from which a set of inputs flow in and set of outputs flow out Processing step is in a filter component Independent entities Does not share state with other filters","title":"Filter"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#pipes","text":"Data is passed through pipes between adjacent filters Stateless data stream Source end feeds filter input and sink receives output","title":"Pipes"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#3-part-schema","text":"","title":"3 Part schema"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#a-simple-case","text":"Passive Filters Active Filters Buffered Pipe for synchronizing","title":"A simple case"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#components-of-pipe-and-filter","text":"","title":"Components of Pipe and Filter"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#implementation-steps","text":"","title":"Implementation Steps"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#pros-and-cons","text":"No intermediate files necessary Filter addition replacement and reuse Rapid prototyping Concurrent execution Certain analysis possible like deadlock Throughput and latency checks Data transformation overhead Error handling is a challenge Sharing state is expensive Lowest common denominator on data transmission determines the overall throughput","title":"Pros and Cons"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#examples_1","text":"Most PaaS providers have message oriented service orchestration Azure has queing services Amazon SQS is used to communicate between EC2 instances","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#blackboard-pattern","text":"Title Details Context A set of heterogeneous specialized modules which dynamically change their strategies as a response to unpredictable events Problem When there is no deterministic solutions to process Solutions to partial problems require representation No predetermined strat is present to solve a problem Dealing with uncertain knowledge Forces A complete search of solution is not possible Different algos used for partial solutions One algorithm uses results of another algo No strict sequence between algos","title":"Blackboard Pattern"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#examples_2","text":"Speech recognition Vehicle Identification Robot Control Modern Machine Learning algos for complex task OCR based text recognition","title":"Examples"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#components","text":"Central data structures Control plan encapsulates information necessary to run the system Domain KS are concerned with the solving of domain specific problems Control KS adapt the current control plan to the current situation","title":"Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#pros-and-cons_1","text":"","title":"Pros and cons"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#distributed-system-patterns","text":"Title Description Context Complex environment of systems to take advantage of computing power via clusters A software may be available only on a specific computer Due security we might have to run different parts in different systems Some services are provided by business partners over internet Problem To build a sw system as a set of decoupled interoperating components rather than a monolith Require a flexible means of interprocess communication Forces It should be possible to distribute components during deployment Need to exchange add or remove components at run time Architecture should hide system specific and implementation specific details from users of components and services","title":"Distributed System Patterns"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#broker-pattern","text":"A broker component helps in achieving better decoupling of clients and servers - Servers: Register themselves with the broker and make their services available - Clients: access the functionality of servers by sending requests via the broker The broker: - Locating the appropriate server and forwarding a request to that server - Transmitting results and exceptions back to the client","title":"Broker Pattern"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#implementation","text":"Define an object model or use an existing one Decide which kind of component interoperability the system should offer Specify the APIs the broker component provides for collaborating Use proxy objects to hide implementation details from clients and servers Design the broker component in parallel with steps 3 and Develop IDL compilers","title":"Implementation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#corba","text":"CORBA is the oldest amongst the middleware technologies used in today's IT world CORBA stands for Common Object Request Broker Architecture CORBA supports Dynamic Invocation Interface (DII)","title":"CORBA"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#remote-method-invocation","text":"Sun's Java RMI is based on the transparent Broker variant pattern Client side proxy and server side invoker have to be created manually by am additional compilation step The service interface is written in Java RMI is limited to usage in Java To establish interoperability RMI IIOP is provided","title":"Remote Method Invocation"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#benefits","text":"Location independence Type system transparency (Marshaling and unmarshaling are done) Isolation Separation of concern Resource management Portability","title":"Benefits"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#liability","text":"Error handling Overhead since developers do not know about the location of objects Performance Lower fault tolerance (server / broker failures)","title":"Liability"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#model-view-controller","text":"","title":"Model View Controller"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#mvc-dynamics","text":"","title":"MVC Dynamics"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#mvc-initialization","text":"","title":"MVC Initialization"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#ajax-application-as-mvc","text":"","title":"AJAX application as MVC"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#benefits_1","text":"Multiple views can be done for the same model Synchronized views Pluggable views and controllers Exchangeability of look and feel Framework potential","title":"Benefits"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#liabilities","text":"Increased complexity Potential execssive number of updates Inefficiency of data access in view Difficulty of using MVC with modern user interface tools","title":"Liabilities"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#micro-kernel","text":"","title":"Micro Kernel"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#context_1","text":"The development of several applications that use similar programming interfaces that build on the same core functionality","title":"Context"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module6SA.html#problem_1","text":"Developing software for an application domain that needs to cope with a broad spectrum of similar standards and technologies Continuous evolution, platform must be extensible, portable and adaptable Tags: !SoftwareArchitecturesIndex","title":"Problem"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html","text":"Module 8 # Cloud Computing # Properties of CC # On demand services Ubiquitous access thorough network on heterogeneous platforms and location independent Resource Pooling Elasticity in resource scaling Multitenancy can be supported (Having support for different users/class of users) Four Key Infrastructure Components # Traditional versus cloud computing # Hypervisor # VMM should primarily have Equivalence Efficiency Resource Control Additionally it can have Mediation between VM and hardware Encapsulation of virtual machine Failure and recovery Check pointing Live migration Tags: !SoftwareArchitecturesIndex","title":"Module 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#module-8","text":"","title":"Module 8"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#cloud-computing","text":"","title":"Cloud Computing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#properties-of-cc","text":"On demand services Ubiquitous access thorough network on heterogeneous platforms and location independent Resource Pooling Elasticity in resource scaling Multitenancy can be supported (Having support for different users/class of users)","title":"Properties of CC"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#four-key-infrastructure-components","text":"","title":"Four Key Infrastructure Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#traditional-versus-cloud-computing","text":"","title":"Traditional versus cloud computing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/Module8SA.html#hypervisor","text":"VMM should primarily have Equivalence Efficiency Resource Control Additionally it can have Mediation between VM and hardware Encapsulation of virtual machine Failure and recovery Check pointing Live migration Tags: !SoftwareArchitecturesIndex","title":"Hypervisor"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/PreRecordedModule1SA.html","text":"Module 1 # Tags: !SoftwareArchitecturesIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/PreRecordedModule1SA.html#module-1","text":"Tags: !SoftwareArchitecturesIndex","title":"Module 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html","text":"Assignment #1 (5% weight) # Objective: To get familiar with the software architecture basics. # Activity: # Choose an existing system from your workplace Understand the purpose (goal) of the system & its key requirements Study architecture and understand the tactics used Document your work in the following format in PPT: # Purpose of the system (Goal) Key requirements of the system \u2013 functional & non-functional Utility tree of Architecturally Significant Requirements (ASR) Tactics used to achieve the top 5 ASRs Software Architecture diagram \u2013 Context diagram, Module decomposition, Component & Connection diagram, Deployment diagram Description of how the system works Key learnings (one slide per participant) My Submission # My submission can be found here tags: !SoftwareArchitecturesIndex Assignments","title":"Assignment #1 (5% weight)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html#assignment-1-5-weight","text":"","title":"Assignment #1 (5% weight)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html#objective-to-get-familiar-with-the-software-architecture-basics","text":"","title":"Objective: To get familiar with the software architecture basics."},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html#activity","text":"Choose an existing system from your workplace Understand the purpose (goal) of the system & its key requirements Study architecture and understand the tactics used","title":"Activity:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html#document-your-work-in-the-following-format-in-ppt","text":"Purpose of the system (Goal) Key requirements of the system \u2013 functional & non-functional Utility tree of Architecturally Significant Requirements (ASR) Tactics used to achieve the top 5 ASRs Software Architecture diagram \u2013 Context diagram, Module decomposition, Component & Connection diagram, Deployment diagram Description of how the system works Key learnings (one slide per participant)","title":"Document your work in the following format in PPT:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment1.html#my-submission","text":"My submission can be found here tags: !SoftwareArchitecturesIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment2.html","text":"Assignment #2 (10% weight, 5% for Submission 5% for Discussion) # Objective: To gain experience in architecting real-life applications in domains such as Retail, Transportation, Healthcare, Hospitality, etc. Example systems: Swiggy, Uber, an IoT system to monitor the health of industrial air conditioners. # Activity: # Identify top 3 Architecturally Significant Requirements (ASRs) and write them in the form of a Utility tree. Why are these architecturally significant? Describe in detail, the tactics you recommend for each ASR. For example, if caching is a tactic you recommend, please mention what you will cache, what tool you would use, how it will work, etc. Draw 2 software architecture diagrams \u2013 component & connection view and deployment view \u2013 to understand how the system works. Indicate important messages between components by labelling the connections in the C&C view. Also indicate the communication method used. Draw a sequence diagram for one major scenario (use case). Mention the scenario. State the architecture patterns used. Explain, wherein the architecture, these patterns have been used. What did you learn by doing this assignment? Mention 3 key learnings. One slide per person. My Submission # My submission can be found here tags: !SoftwareArchitecturesIndex Assignments","title":"Assignment #2 (10% weight, 5% for Submission 5% for Discussion)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment2.html#assignment-2-10-weight-5-for-submission-5-for-discussion","text":"","title":"Assignment #2 (10% weight, 5% for Submission 5% for Discussion)"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment2.html#objective-to-gain-experience-in-architecting-real-life-applications-in-domains-such-as-retail-transportation-healthcare-hospitality-etc-example-systems-swiggy-uber-an-iot-system-to-monitor-the-health-of-industrial-air-conditioners","text":"","title":"Objective: To gain experience in architecting real-life applications in domains such as Retail, Transportation, Healthcare, Hospitality, etc. Example systems: Swiggy, Uber, an IoT system to monitor the health of industrial air conditioners."},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment2.html#activity","text":"Identify top 3 Architecturally Significant Requirements (ASRs) and write them in the form of a Utility tree. Why are these architecturally significant? Describe in detail, the tactics you recommend for each ASR. For example, if caching is a tactic you recommend, please mention what you will cache, what tool you would use, how it will work, etc. Draw 2 software architecture diagrams \u2013 component & connection view and deployment view \u2013 to understand how the system works. Indicate important messages between components by labelling the connections in the C&C view. Also indicate the communication method used. Draw a sequence diagram for one major scenario (use case). Mention the scenario. State the architecture patterns used. Explain, wherein the architecture, these patterns have been used. What did you learn by doing this assignment? Mention 3 key learnings. One slide per person.","title":"Activity:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%202/Software%20Architectures/SAAssignment2.html#my-submission","text":"My submission can be found here tags: !SoftwareArchitecturesIndex Assignments","title":"My Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/%21Semester3Index.html","text":"Semester 3 Index # Linked pages to subjects # Data Mining: !DMIndex Natural Language Processing: !NLPIndex Deep Learning: !DLIndex Lecturer Contact Details # Data Mining : Manik Gupta Natural Language Processing : Sudarshan Deshmukh Deep Learning : Tanmay Tulsidas Verlekar Tags: !AllSemestersIndex","title":"Semester 3 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/%21Semester3Index.html#semester-3-index","text":"","title":"Semester 3 Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/%21Semester3Index.html#linked-pages-to-subjects","text":"Data Mining: !DMIndex Natural Language Processing: !NLPIndex Deep Learning: !DLIndex","title":"Linked pages to subjects"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/%21Semester3Index.html#lecturer-contact-details","text":"Data Mining : Manik Gupta Natural Language Processing : Sudarshan Deshmukh Deep Learning : Tanmay Tulsidas Verlekar Tags: !AllSemestersIndex","title":"Lecturer Contact Details"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html","text":"Data Mining # Evaluative Components # No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Lab 20% EC2 Mid Sem Test 30% EC3 Comprehensive Exam 40% Course Objectives # Design and use of data mining algorithms. Will cover algorithmic and application perspectives of data mining At the end of the course the student should be able to: Choose an appropriate technique based on given data Identify and design an appropriate data mining analysis technique given a problem Gain practical hands on experience in implementing data mining algorithms Index # Assignment # DMAssignment Question Papers # Mid Sem Paper DM End Sem Paper DM Tags: !Semester3Index","title":"Data Mining"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#data-mining","text":"","title":"Data Mining"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#evaluative-components","text":"No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Lab 20% EC2 Mid Sem Test 30% EC3 Comprehensive Exam 40%","title":"Evaluative Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#course-objectives","text":"Design and use of data mining algorithms. Will cover algorithmic and application perspectives of data mining At the end of the course the student should be able to: Choose an appropriate technique based on given data Identify and design an appropriate data mining analysis technique given a problem Gain practical hands on experience in implementing data mining algorithms","title":"Course Objectives"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#index","text":"","title":"Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#assignment","text":"DMAssignment","title":"Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/%21DMIndex.html#question-papers","text":"Mid Sem Paper DM End Sem Paper DM Tags: !Semester3Index","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html","text":"Data Mining Assignment # Introduction # Data mining is the process of working with large data sets to identify patterns and establish relationships to solve problems through data analysis. As a part of the assignment, you will be learning to design and implement the complete DM processing pipeline and gain an understanding how to perform preprocessing to analysis to draw insights for a given dataset. Different tasks involved in the assignment 1. Dataset Selection: You can select one of the datasets from the below: 1. https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset 2. https://www.kaggle.com/uciml/mushroom-classification 3. https://www.kaggle.com/fedesoriano/stroke-prediction-dataset Once you decide on the dataset, you would need to study and understand the dataset in detail and identify the key questions or insights that you would like to draw from the dataset. 2. Data Preprocessing and Visualization: You would need to perform exploratory data analysis along with suitable visualizations and identify/employ different preprocessing techniques suitable for the dataset. You should implement atleast 2 data preprocessing techniques studied in the class in addition to any data cleaning (if required) and show the results. 3. Data Analysis: Based on the insights that you wish to draw from the dataset, you would need to identify the key DM tasks like association analysis, clustering, classification or outlier analysis that are applicable for the dataset. You should implement atleast 3 data analysis techniques and corresponding algorithms studied in the class and show the results. Weightage of Individual Components # Total Assignment Weightage 20% 1. Data Preprocessing and Visualization 5% 2. Data Analysis (at least 3 Data mining tasks learnt during the class) 15% Main Deliverable and Deadline # Main deliverable is executable code in the form of Jupyter Ipython Notebooks with detailed markdown text explaining tasks performed/results obtained, comments and plots. Notebooks should be able to run smoothly and be self contained with all necessary information specified in it. Submissions need to be made on CMS ( https://elearn.bits-pilani.ac.in ) by November 15 th , midnight. No late submissions will be accepted. Only one team member can make submission and team no/member information should be clearly mentioned at the beginning of the notebook. Please update submission status clearly in the spreadsheet shared. Few Important Things to Remember # Teams once decided cannot be changed, else submission will not be accepted/marked. You need to use Python for executing the project. Use of standard libraries is not allowed for data processing/analysis (provide implementations from scratch) other than for data handling/visualization purposes. We will run and execute the notebooks for marking purposes and those which fail to produce desired results/plots will be penalized. (path settings etc. should be done accordingly) No email submissions will be accepted. Emails related to team formation will not be entertained. Assignment Submission # The assignment submission can be found here ( https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Data%20Mining ). The readme for this assignment: _____ _ | __ \\ | | | |__) |___ __ _ __| |_ __ ___ ___ | _ // _ \\/ _` |/ _` | '_ ` _ \\ / _ \\ | | \\ \\ __/ (_| | (_| | | | | | | __/ |_| \\_\\___|\\__,_|\\__,_|_| |_| |_|\\___| Datamining Assignment by group 13: | Roll No. | Name | | ----------- | ----------------- | | 2021MT<xxx> | dolor sit amet | | 2021MT<xxx> | Akhil S | | 2021MT<xxx> | commodo consequat | | 2021MT<xxx> | totam rem aperiam | Make sure all the python files submitted along with the notebook is placed in the same working directory. These python files hold the custom implementations for all the algorithms that are used in this assignment. The dependent python files are: 1. GaussianNaiveBayes.py 2. DecisionTree.py 3. KMeansClustering.py 4. dbscan.py 5. lof.py and the dataset found from kaggle is the healthcare-dataset-stroke-data.csv file Tags: !DMIndex Assignments","title":"Data Mining Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#data-mining-assignment","text":"","title":"Data Mining Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#introduction","text":"Data mining is the process of working with large data sets to identify patterns and establish relationships to solve problems through data analysis. As a part of the assignment, you will be learning to design and implement the complete DM processing pipeline and gain an understanding how to perform preprocessing to analysis to draw insights for a given dataset. Different tasks involved in the assignment 1. Dataset Selection: You can select one of the datasets from the below: 1. https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset 2. https://www.kaggle.com/uciml/mushroom-classification 3. https://www.kaggle.com/fedesoriano/stroke-prediction-dataset Once you decide on the dataset, you would need to study and understand the dataset in detail and identify the key questions or insights that you would like to draw from the dataset. 2. Data Preprocessing and Visualization: You would need to perform exploratory data analysis along with suitable visualizations and identify/employ different preprocessing techniques suitable for the dataset. You should implement atleast 2 data preprocessing techniques studied in the class in addition to any data cleaning (if required) and show the results. 3. Data Analysis: Based on the insights that you wish to draw from the dataset, you would need to identify the key DM tasks like association analysis, clustering, classification or outlier analysis that are applicable for the dataset. You should implement atleast 3 data analysis techniques and corresponding algorithms studied in the class and show the results.","title":"Introduction"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#weightage-of-individual-components","text":"Total Assignment Weightage 20% 1. Data Preprocessing and Visualization 5% 2. Data Analysis (at least 3 Data mining tasks learnt during the class) 15%","title":"Weightage of Individual Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#main-deliverable-and-deadline","text":"Main deliverable is executable code in the form of Jupyter Ipython Notebooks with detailed markdown text explaining tasks performed/results obtained, comments and plots. Notebooks should be able to run smoothly and be self contained with all necessary information specified in it. Submissions need to be made on CMS ( https://elearn.bits-pilani.ac.in ) by November 15 th , midnight. No late submissions will be accepted. Only one team member can make submission and team no/member information should be clearly mentioned at the beginning of the notebook. Please update submission status clearly in the spreadsheet shared.","title":"Main Deliverable and Deadline"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#few-important-things-to-remember","text":"Teams once decided cannot be changed, else submission will not be accepted/marked. You need to use Python for executing the project. Use of standard libraries is not allowed for data processing/analysis (provide implementations from scratch) other than for data handling/visualization purposes. We will run and execute the notebooks for marking purposes and those which fail to produce desired results/plots will be penalized. (path settings etc. should be done accordingly) No email submissions will be accepted. Emails related to team formation will not be entertained.","title":"Few Important Things to Remember"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/DMAssignment.html#assignment-submission","text":"The assignment submission can be found here ( https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Data%20Mining ). The readme for this assignment: _____ _ | __ \\ | | | |__) |___ __ _ __| |_ __ ___ ___ | _ // _ \\/ _` |/ _` | '_ ` _ \\ / _ \\ | | \\ \\ __/ (_| | (_| | | | | | | __/ |_| \\_\\___|\\__,_|\\__,_|_| |_| |_|\\___| Datamining Assignment by group 13: | Roll No. | Name | | ----------- | ----------------- | | 2021MT<xxx> | dolor sit amet | | 2021MT<xxx> | Akhil S | | 2021MT<xxx> | commodo consequat | | 2021MT<xxx> | totam rem aperiam | Make sure all the python files submitted along with the notebook is placed in the same working directory. These python files hold the custom implementations for all the algorithms that are used in this assignment. The dependent python files are: 1. GaussianNaiveBayes.py 2. DecisionTree.py 3. KMeansClustering.py 4. dbscan.py 5. lof.py and the dataset found from kaggle is the healthcare-dataset-stroke-data.csv file Tags: !DMIndex Assignments","title":"Assignment Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html","text":"End Sem DM Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DMIndex QuestionPapers","title":"End Sem DM Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#end-sem-dm-paper","text":"","title":"End Sem DM Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/End%20Sem%20Paper%20DM.html#question-7","text":"tags: !DMIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html","text":"Mid Sem DM Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DMIndex QuestionPapers","title":"Mid Sem DM Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#mid-sem-dm-paper","text":"","title":"Mid Sem DM Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Data%20Mining/Mid%20Sem%20Paper%20DM.html#question-7","text":"tags: !DMIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html","text":"Deep Learning # Evaluative Components # No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Assignment 15% EC2 Mid Sem Test 20% EC3 Comprehensive Exam 45% Resources # Dive Into Deep Learning Interactive Text Book Dive Into Deep Learning pdf Course Overview # Supervised Learning: Neural Networks Back Propagation DFF Neural Networks Regularization CNN RNN Unsupervised Learning Auto Encoders GAN Applications: Computer Vision, Speech Recognition etc. Index # Assignment # DLAssignment Question Papers # Mid Sem Paper DL End Sem Paper DL Tags: !Semester3Index","title":"Deep Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#deep-learning","text":"","title":"Deep Learning"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#evaluative-components","text":"No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Assignment 15% EC2 Mid Sem Test 20% EC3 Comprehensive Exam 45%","title":"Evaluative Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#resources","text":"Dive Into Deep Learning Interactive Text Book Dive Into Deep Learning pdf","title":"Resources"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#course-overview","text":"Supervised Learning: Neural Networks Back Propagation DFF Neural Networks Regularization CNN RNN Unsupervised Learning Auto Encoders GAN Applications: Computer Vision, Speech Recognition etc.","title":"Course Overview"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#index","text":"","title":"Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#assignment","text":"DLAssignment","title":"Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/%21DLIndex.html#question-papers","text":"Mid Sem Paper DL End Sem Paper DL Tags: !Semester3Index","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html","text":"Deep Learning Assignment # This DL assignment submission is by Akhil S - 2021MT12054 # Dataset # The dataset required for this can be found here: https://www.kaggle.com/competitions/deep-learning-sszg529-bits-pilani/overview Sample pictures for this data set: Assignment # This is a micro-facial emotion recognition task based on micro facial expressions dataset. Predict one of 5 emotions which are defined here: 1 -> Happiness 2 -> Other 3 -> Anger 4 -> Contempt 5 -> Surprise (Strictly follow the above encoding for your output.) Submission should be as per the format given. Any other format would lead to an invalid solution and won't be evaluated. The final solution should have 2 columns: 'Id' and 'Class'. Submission # The submission for the same can be found here ( https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Deep%20Learning ) Tags: !DLIndex Assignments","title":"Deep Learning Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html#deep-learning-assignment","text":"","title":"Deep Learning Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html#this-dl-assignment-submission-is-by-akhil-s---2021mt12054","text":"","title":"This DL assignment submission is by Akhil S - 2021MT12054"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html#dataset","text":"The dataset required for this can be found here: https://www.kaggle.com/competitions/deep-learning-sszg529-bits-pilani/overview Sample pictures for this data set:","title":"Dataset"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html#assignment","text":"This is a micro-facial emotion recognition task based on micro facial expressions dataset. Predict one of 5 emotions which are defined here: 1 -> Happiness 2 -> Other 3 -> Anger 4 -> Contempt 5 -> Surprise (Strictly follow the above encoding for your output.) Submission should be as per the format given. Any other format would lead to an invalid solution and won't be evaluated. The final solution should have 2 columns: 'Id' and 'Class'.","title":"Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/DLAssignment.html#submission","text":"The submission for the same can be found here ( https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Deep%20Learning ) Tags: !DLIndex Assignments","title":"Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html","text":"End Sem DL Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # Question 7 # tags: !DLIndex QuestionPapers","title":"End Sem DL Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#end-sem-dl-paper","text":"","title":"End Sem DL Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-6","text":"","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/End%20Sem%20Paper%20DL.html#question-7","text":"tags: !DLIndex QuestionPapers","title":"Question 7"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html","text":"Mid Sem DL Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !DLIndex QuestionPapers","title":"Mid Sem DL Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html#mid-sem-dl-paper","text":"","title":"Mid Sem DL Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Deep%20Learning/Mid%20Sem%20Paper%20DL.html#question-4","text":"tags: !DLIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html","text":"Natural Language Processing # Evaluative Components # No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Lab 20% EC2 Mid Sem Test 30% EC3 Comprehensive Exam 40% Index # [[ Spam Detection Jupyter Notebook ]] Assignment # NLPAssignment Question Papers # Mid Sem Paper NLP End Sem Paper NLP Tags: !Semester3Index","title":"Natural Language Processing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html#natural-language-processing","text":"","title":"Natural Language Processing"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html#evaluative-components","text":"No. Name Weight EC1 Quiz 1 5% Quiz 2 5% Lab 20% EC2 Mid Sem Test 30% EC3 Comprehensive Exam 40%","title":"Evaluative Components"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html#index","text":"[[ Spam Detection Jupyter Notebook ]]","title":"Index"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html#assignment","text":"NLPAssignment","title":"Assignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/%21NLPIndex.html#question-papers","text":"Mid Sem Paper NLP End Sem Paper NLP Tags: !Semester3Index","title":"Question Papers"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html","text":"End Sem NLP Paper # Question 1 # Question 2 # Question 3 # Question 4 # tags: !NLPIndex QuestionPapers","title":"End Sem NLP Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html#end-sem-nlp-paper","text":"","title":"End Sem NLP Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/End%20Sem%20Paper%20NLP.html#question-4","text":"tags: !NLPIndex QuestionPapers","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html","text":"Mid Sem NLP Paper # Question 1 # Question 2 # Question 3 # Question 4 # Question 5 # Question 6 # tags: !NLPIndex QuestionPapers","title":"Mid Sem NLP Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#mid-sem-nlp-paper","text":"","title":"Mid Sem NLP Paper"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-1","text":"","title":"Question 1"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-2","text":"","title":"Question 2"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-3","text":"","title":"Question 3"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-4","text":"","title":"Question 4"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-5","text":"","title":"Question 5"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Mid%20Sem%20Paper%20NLP.html#question-6","text":"tags: !NLPIndex QuestionPapers","title":"Question 6"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html","text":"Profession News Recommender App # This NLP assignment submission is by Akhil S - 2021MT12054 # Assignment: # Proposed solution will recommend real time news based on user professional profile. Steps: 1. Enter your Educational details(Degree, University) 2. Enter your most recent experience(Organization Name, Experience description) 3. Enter your skills 4. Implement a content based recommendation system which will recommend news based on above entered data in real time. [6 marks] 5. Convert the solution in FastAPI endpoint.[4 marks] Prerequisites: # Make sure that the requirements mentioned in requirements.txt is installed, this can be done by running the following: pip install -r requirements.txt Make sure that tfidf.py file is in the same path as the app.py file Running the app: # After the prerequisites are met, we need to run the app.py python app.py NOTE: This app can take more time than usually when run for the first time since code tries to download stop words from nltk. Go to http://127.0.0.1:8000/docs , the end point for the assignment is \"/assignment\". You can try this api out by clicking \"Try it out\" button Provide the Education, Experience and Skills and hit execute This API will return a list of objects where each object has the Title, PublishedDate and URL for the news article Screenshots and samples: # Swagger UI for API: API endpoint result with sample data on UI Sample output from data provided to API [ { \"Title\" : \"Are SaaS And Software-Defined Operating Platforms Compatible? - Forbes\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 15:47:51 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMid2h0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvcGV0ZXJiZW5kb3JzYW11ZWwvMjAyMi8xMS8xNi9hcmUtc2Fhcy1hbmQtc29mdHdhcmUtZGVmaW5lZC1vcGVyYXRpbmctcGxhdGZvcm1zLWNvbXBhdGlibGUv0gEA?oc=5\" }, { \"Title\" : \"Spatial Leverages Years of Immersive Audio Experience Design to Develop Spatial Space Kit, a Simple, Scalable Hardware and Software Solution - Business Wire\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 14:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiwwFodHRwczovL3d3dy5idXNpbmVzc3dpcmUuY29tL25ld3MvaG9tZS8yMDIyMTExNjAwNTM5My9lbi9TcGF0aWFsLUxldmVyYWdlcy1ZZWFycy1vZi1JbW1lcnNpdmUtQXVkaW8tRXhwZXJpZW5jZS1EZXNpZ24tdG8tRGV2ZWxvcC1TcGF0aWFsLVNwYWNlLUtpdC1hLVNpbXBsZS1TY2FsYWJsZS1IYXJkd2FyZS1hbmQtU29mdHdhcmUtU29sdXRpb27SAQA?oc=5\" }, { \"Title\" : \"Full-Stack Developers: Surge In Demand and The Reason Behind It - Youth Incorporated\" , \"PublishedDate\" : \"Tue, 08 Nov 2022 08:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiVmh0dHBzOi8veW91dGhpbmNtYWcuY29tL2Z1bGwtc3RhY2stZGV2ZWxvcGVycy1zdXJnZS1pbi1kZW1hbmQtYW5kLXRoZS1yZWFzb24tYmVoaW5kLWl00gFaaHR0cHM6Ly95b3V0aGluY21hZy5jb20vZnVsbC1zdGFjay1kZXZlbG9wZXJzLXN1cmdlLWluLWRlbWFuZC1hbmQtdGhlLXJlYXNvbi1iZWhpbmQtaXQvYW1w?oc=5\" }, { \"Title\" : \"Why Millennials are Crazy about Full-stack Web Developer Jobs? - Analytics Insight\" , \"PublishedDate\" : \"Fri, 28 Oct 2022 07:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiX2h0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L3doeS1taWxsZW5uaWFscy1hcmUtY3JhenktYWJvdXQtZnVsbC1zdGFjay13ZWItZGV2ZWxvcGVyLWpvYnMv0gEA?oc=5\" }, { \"Title\" : \"Top 10 Google Colab Alternatives for Machine Learning Engineers in 2023 - Analytics Insight\" , \"PublishedDate\" : \"Mon, 14 Nov 2022 10:34:44 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiaWh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L3RvcC0xMC1nb29nbGUtY29sYWItYWx0ZXJuYXRpdmVzLWZvci1tYWNoaW5lLWxlYXJuaW5nLWVuZ2luZWVycy1pbi0yMDIzL9IBAA?oc=5\" }, { \"Title\" : \"Putting chiral perturbation theory to the test - Advanced Science News\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 08:29:21 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiU2h0dHBzOi8vd3d3LmFkdmFuY2Vkc2NpZW5jZW5ld3MuY29tL3B1dHRpbmctY2hpcmFsLXBlcnR1cmJhdGlvbi10aGVvcnktdG8tdGhlLXRlc3Qv0gEA?oc=5\" }, { \"Title\" : \"IIT Kanpur Invites Applications for Two Free Online Courses on Data Science - DATAQUEST\" , \"PublishedDate\" : \"Thu, 17 Nov 2022 06:00:29 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiZGh0dHBzOi8vd3d3LmRxaW5kaWEuY29tL2lpdC1rYW5wdXItaW52aXRlcy1hcHBsaWNhdGlvbnMtZm9yLXR3by1mcmVlLW9ubGluZS1jb3Vyc2VzLW9uLWRhdGEtc2NpZW5jZS_SAWhodHRwczovL3d3dy5kcWluZGlhLmNvbS9paXQta2FucHVyLWludml0ZXMtYXBwbGljYXRpb25zLWZvci10d28tZnJlZS1vbmxpbmUtY291cnNlcy1vbi1kYXRhLXNjaWVuY2UvYW1wLw?oc=5\" }, { \"Title\" : \"IIT Madras to offer course in advanced quantum computing - The Hindu\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 10:53:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMieWh0dHBzOi8vd3d3LnRoZWhpbmR1LmNvbS9uZXdzL2NpdGllcy9jaGVubmFpL2lpdC1tYWRyYXMtdG8tb2ZmZXItY291cnNlLWluLWFkdmFuY2VkLXF1YW50dW0tY29tcHV0aW5nL2FydGljbGU2NjE0NDIxOS5lY2XSAQA?oc=5\" }, { \"Title\" : \"Chennai Startup\u2019s Electric Two-Wheelers Zip Across Africa, Raise $50 Million Funding - The Better India\" , \"PublishedDate\" : \"Mon, 07 Nov 2022 08:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMifGh0dHBzOi8vd3d3LnRoZWJldHRlcmluZGlhLmNvbS8zMDIxODcvY2hlbm5haS1zdGFydHVwLWVsZWN0cmljLXZlaGljbGUtdHdvLXdoZWVsZXJzLWluLWFmcmljYS1lbGVjdHJpYy1hdXRvcy1hbmQtZS1tb2JpbGl0eS_SAQA?oc=5\" } ] Submission # This project can be found in https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Natural%20Language%20Processing Tags: !NLPIndex Assignments","title":"NLPAssignment"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#profession-news-recommender-app","text":"","title":"Profession News Recommender App"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#this-nlp-assignment-submission-is-by-akhil-s---2021mt12054","text":"","title":"This NLP assignment submission is by Akhil S - 2021MT12054"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#assignment","text":"Proposed solution will recommend real time news based on user professional profile. Steps: 1. Enter your Educational details(Degree, University) 2. Enter your most recent experience(Organization Name, Experience description) 3. Enter your skills 4. Implement a content based recommendation system which will recommend news based on above entered data in real time. [6 marks] 5. Convert the solution in FastAPI endpoint.[4 marks]","title":"Assignment:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#prerequisites","text":"Make sure that the requirements mentioned in requirements.txt is installed, this can be done by running the following: pip install -r requirements.txt Make sure that tfidf.py file is in the same path as the app.py file","title":"Prerequisites:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#running-the-app","text":"After the prerequisites are met, we need to run the app.py python app.py NOTE: This app can take more time than usually when run for the first time since code tries to download stop words from nltk. Go to http://127.0.0.1:8000/docs , the end point for the assignment is \"/assignment\". You can try this api out by clicking \"Try it out\" button Provide the Education, Experience and Skills and hit execute This API will return a list of objects where each object has the Title, PublishedDate and URL for the news article","title":"Running the app:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#screenshots-and-samples","text":"Swagger UI for API: API endpoint result with sample data on UI Sample output from data provided to API [ { \"Title\" : \"Are SaaS And Software-Defined Operating Platforms Compatible? - Forbes\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 15:47:51 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMid2h0dHBzOi8vd3d3LmZvcmJlcy5jb20vc2l0ZXMvcGV0ZXJiZW5kb3JzYW11ZWwvMjAyMi8xMS8xNi9hcmUtc2Fhcy1hbmQtc29mdHdhcmUtZGVmaW5lZC1vcGVyYXRpbmctcGxhdGZvcm1zLWNvbXBhdGlibGUv0gEA?oc=5\" }, { \"Title\" : \"Spatial Leverages Years of Immersive Audio Experience Design to Develop Spatial Space Kit, a Simple, Scalable Hardware and Software Solution - Business Wire\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 14:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiwwFodHRwczovL3d3dy5idXNpbmVzc3dpcmUuY29tL25ld3MvaG9tZS8yMDIyMTExNjAwNTM5My9lbi9TcGF0aWFsLUxldmVyYWdlcy1ZZWFycy1vZi1JbW1lcnNpdmUtQXVkaW8tRXhwZXJpZW5jZS1EZXNpZ24tdG8tRGV2ZWxvcC1TcGF0aWFsLVNwYWNlLUtpdC1hLVNpbXBsZS1TY2FsYWJsZS1IYXJkd2FyZS1hbmQtU29mdHdhcmUtU29sdXRpb27SAQA?oc=5\" }, { \"Title\" : \"Full-Stack Developers: Surge In Demand and The Reason Behind It - Youth Incorporated\" , \"PublishedDate\" : \"Tue, 08 Nov 2022 08:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiVmh0dHBzOi8veW91dGhpbmNtYWcuY29tL2Z1bGwtc3RhY2stZGV2ZWxvcGVycy1zdXJnZS1pbi1kZW1hbmQtYW5kLXRoZS1yZWFzb24tYmVoaW5kLWl00gFaaHR0cHM6Ly95b3V0aGluY21hZy5jb20vZnVsbC1zdGFjay1kZXZlbG9wZXJzLXN1cmdlLWluLWRlbWFuZC1hbmQtdGhlLXJlYXNvbi1iZWhpbmQtaXQvYW1w?oc=5\" }, { \"Title\" : \"Why Millennials are Crazy about Full-stack Web Developer Jobs? - Analytics Insight\" , \"PublishedDate\" : \"Fri, 28 Oct 2022 07:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiX2h0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L3doeS1taWxsZW5uaWFscy1hcmUtY3JhenktYWJvdXQtZnVsbC1zdGFjay13ZWItZGV2ZWxvcGVyLWpvYnMv0gEA?oc=5\" }, { \"Title\" : \"Top 10 Google Colab Alternatives for Machine Learning Engineers in 2023 - Analytics Insight\" , \"PublishedDate\" : \"Mon, 14 Nov 2022 10:34:44 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiaWh0dHBzOi8vd3d3LmFuYWx5dGljc2luc2lnaHQubmV0L3RvcC0xMC1nb29nbGUtY29sYWItYWx0ZXJuYXRpdmVzLWZvci1tYWNoaW5lLWxlYXJuaW5nLWVuZ2luZWVycy1pbi0yMDIzL9IBAA?oc=5\" }, { \"Title\" : \"Putting chiral perturbation theory to the test - Advanced Science News\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 08:29:21 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiU2h0dHBzOi8vd3d3LmFkdmFuY2Vkc2NpZW5jZW5ld3MuY29tL3B1dHRpbmctY2hpcmFsLXBlcnR1cmJhdGlvbi10aGVvcnktdG8tdGhlLXRlc3Qv0gEA?oc=5\" }, { \"Title\" : \"IIT Kanpur Invites Applications for Two Free Online Courses on Data Science - DATAQUEST\" , \"PublishedDate\" : \"Thu, 17 Nov 2022 06:00:29 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMiZGh0dHBzOi8vd3d3LmRxaW5kaWEuY29tL2lpdC1rYW5wdXItaW52aXRlcy1hcHBsaWNhdGlvbnMtZm9yLXR3by1mcmVlLW9ubGluZS1jb3Vyc2VzLW9uLWRhdGEtc2NpZW5jZS_SAWhodHRwczovL3d3dy5kcWluZGlhLmNvbS9paXQta2FucHVyLWludml0ZXMtYXBwbGljYXRpb25zLWZvci10d28tZnJlZS1vbmxpbmUtY291cnNlcy1vbi1kYXRhLXNjaWVuY2UvYW1wLw?oc=5\" }, { \"Title\" : \"IIT Madras to offer course in advanced quantum computing - The Hindu\" , \"PublishedDate\" : \"Wed, 16 Nov 2022 10:53:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMieWh0dHBzOi8vd3d3LnRoZWhpbmR1LmNvbS9uZXdzL2NpdGllcy9jaGVubmFpL2lpdC1tYWRyYXMtdG8tb2ZmZXItY291cnNlLWluLWFkdmFuY2VkLXF1YW50dW0tY29tcHV0aW5nL2FydGljbGU2NjE0NDIxOS5lY2XSAQA?oc=5\" }, { \"Title\" : \"Chennai Startup\u2019s Electric Two-Wheelers Zip Across Africa, Raise $50 Million Funding - The Better India\" , \"PublishedDate\" : \"Mon, 07 Nov 2022 08:00:00 GMT\" , \"URL\" : \"https://news.google.com/__i/rss/rd/articles/CBMifGh0dHBzOi8vd3d3LnRoZWJldHRlcmluZGlhLmNvbS8zMDIxODcvY2hlbm5haS1zdGFydHVwLWVsZWN0cmljLXZlaGljbGUtdHdvLXdoZWVsZXJzLWluLWFmcmljYS1lbGVjdHJpYy1hdXRvcy1hbmQtZS1tb2JpbGl0eS_SAQA?oc=5\" } ]","title":"Screenshots and samples:"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/NLPAssignment.html#submission","text":"This project can be found in https://github.com/Akhilsudh/BITS-Assignment/tree/master/Semester%203/Natural%20Language%20Processing Tags: !NLPIndex Assignments","title":"Submission"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Spam%20Detection%20Jupyer%20Notebook.html","text":"Spam Detection Jupyter Notebook # import pandas as pd sms_spam = pd . read_csv ( 'SMSSpamCollection' , sep = ' \\t ' , header = None , names = [ 'Label' , 'SMS' ]) print ( sms_spam . shape ) sms_spam . head () (5572, 2) Label SMS 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... sms_spam [ 'Label' ] . value_counts ( normalize = True ) ham 0.865937 spam 0.134063 Name: Label, dtype: float64 # Randomize the dataset data_randomized = sms_spam . sample ( frac = 1 , random_state = 1 ) # Calculate index for split training_test_index = round ( len ( data_randomized ) * 0.8 ) # Split into training and test sets training_set = data_randomized [: training_test_index ] . reset_index ( drop = True ) test_set = data_randomized [ training_test_index :] . reset_index ( drop = True ) print ( training_set . shape ) print ( test_set . shape ) (4458, 2) (1114, 2) training_set [ 'Label' ] . value_counts ( normalize = True ) ham 0.86541 spam 0.13459 Name: Label, dtype: float64 test_set [ 'Label' ] . value_counts ( normalize = True ) ham 0.868043 spam 0.131957 Name: Label, dtype: float64 # Before cleaning training_set . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS 0 ham Yep, by the pretty sculpture 1 ham Yes, princess. Are you going to make me moan? 2 ham Welp apparently he retired # After cleaning training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . replace ( '\\W' , ' ' ) # Removes punctuation training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . lower () training_set . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS 0 ham yep by the pretty sculpture 1 ham yes princess are you going to make me moan 2 ham welp apparently he retired training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . split () vocabulary = [] for sms in training_set [ 'SMS' ]: for word in sms : vocabulary . append ( word ) vocabulary = list ( set ( vocabulary )) len ( vocabulary ) 7783 word_counts_per_sms = { 'secret' : [ 2 , 1 , 1 ], 'prize' : [ 2 , 0 , 1 ], 'claim' : [ 1 , 0 , 1 ], 'now' : [ 1 , 0 , 1 ], 'coming' : [ 0 , 1 , 0 ], 'to' : [ 0 , 1 , 0 ], 'my' : [ 0 , 1 , 0 ], 'party' : [ 0 , 1 , 0 ], 'winner' : [ 0 , 0 , 1 ] } word_counts = pd . DataFrame ( word_counts_per_sms ) word_counts . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } secret prize claim now coming to my party winner 0 2 2 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 2 1 1 1 1 0 0 0 0 1 word_counts_per_sms = { unique_word : [ 0 ] * len ( training_set [ 'SMS' ]) for unique_word in vocabulary } for index , sms in enumerate ( training_set [ 'SMS' ]): for word in sms : word_counts_per_sms [ word ][ index ] += 1 word_counts = pd . DataFrame ( word_counts_per_sms ) word_counts . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } indyarocks port eg surly trained voda organise vu leg 06 ... imposter gee 4882 reduce sexual nange conducts noworriesloans taxes book 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows \u00d7 7783 columns training_set_clean = pd . concat ([ training_set , word_counts ], axis = 1 ) training_set_clean . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS indyarocks port eg surly trained voda organise vu ... imposter gee 4882 reduce sexual nange conducts noworriesloans taxes book 0 ham [yep, by, the, pretty, sculpture] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 ham [yes, princess, are, you, going, to, make, me,... 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 ham [welp, apparently, he, retired] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 ham [havent] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 ham [i, forgot, 2, ask, \u00fc, all, smth, there, s, a,... 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows \u00d7 7785 columns # Isolating spam and ham messages first spam_messages = training_set_clean [ training_set_clean [ 'Label' ] == 'spam' ] ham_messages = training_set_clean [ training_set_clean [ 'Label' ] == 'ham' ] # P(Spam) and P(Ham) p_spam = len ( spam_messages ) / len ( training_set_clean ) p_ham = len ( ham_messages ) / len ( training_set_clean ) # N_Spam n_words_per_spam_message = spam_messages [ 'SMS' ] . apply ( len ) n_spam = n_words_per_spam_message . sum () # N_Ham n_words_per_ham_message = ham_messages [ 'SMS' ] . apply ( len ) n_ham = n_words_per_ham_message . sum () # N_Vocabulary n_vocabulary = len ( vocabulary ) # Laplace smoothing alpha = 1 # Initiate parameters parameters_spam = { unique_word : 0 for unique_word in vocabulary } parameters_ham = { unique_word : 0 for unique_word in vocabulary } # Calculate parameters for word in vocabulary : n_word_given_spam = spam_messages [ word ] . sum () # spam_messages already defined p_word_given_spam = ( n_word_given_spam + alpha ) / ( n_spam + alpha * n_vocabulary ) parameters_spam [ word ] = p_word_given_spam n_word_given_ham = ham_messages [ word ] . sum () # ham_messages already defined p_word_given_ham = ( n_word_given_ham + alpha ) / ( n_ham + alpha * n_vocabulary ) parameters_ham [ word ] = p_word_given_ham import re def classify ( message ): ''' message: a string ''' message = re . sub ( '\\W' , ' ' , message ) message = message . lower () . split () p_spam_given_message = p_spam p_ham_given_message = p_ham for word in message : if word in parameters_spam : p_spam_given_message *= parameters_spam [ word ] if word in parameters_ham : p_ham_given_message *= parameters_ham [ word ] print ( 'P(Spam|message):' , p_spam_given_message ) print ( 'P(Ham|message):' , p_ham_given_message ) if p_ham_given_message > p_spam_given_message : print ( 'Label: Ham' ) elif p_ham_given_message < p_spam_given_message : print ( 'Label: Spam' ) else : print ( 'Equal proabilities, have a human classify this!' ) classify ( 'WINNER!! This is the secret code to unlock the money: C3421.' ) P(Spam|message): 1.3481290211300841e-25 P(Ham|message): 1.9368049028589875e-27 Label: Spam classify ( \"Sounds good, Tom, then see u there\" ) P(Spam|message): 2.4372375665888117e-25 P(Ham|message): 3.687530435009238e-21 Label: Ham def classify_test_set ( message ): ''' message: a string ''' message = re . sub ( '\\W' , ' ' , message ) message = message . lower () . split () p_spam_given_message = p_spam p_ham_given_message = p_ham for word in message : if word in parameters_spam : p_spam_given_message *= parameters_spam [ word ] if word in parameters_ham : p_ham_given_message *= parameters_ham [ word ] if p_ham_given_message > p_spam_given_message : return 'ham' elif p_spam_given_message > p_ham_given_message : return 'spam' else : return 'needs human classification' test_set [ 'predicted' ] = test_set [ 'SMS' ] . apply ( classify_test_set ) test_set . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS predicted 0 ham Later i guess. I needa do mcat study too. ham 1 ham But i haf enuff space got like 4 mb... ham 2 spam Had your mobile 10 mths? Update to latest Oran... spam 3 ham All sounds good. Fingers . Makes it difficult ... ham 4 ham All done, all handed in. Don't know if mega sh... ham correct = 0 total = test_set . shape [ 0 ] for row in test_set . iterrows (): row = row [ 1 ] if row [ 'Label' ] == row [ 'predicted' ]: correct += 1 print ( 'Correct:' , correct ) print ( 'Incorrect:' , total - correct ) print ( 'Accuracy:' , correct / total ) Correct: 1100 Incorrect: 14 Accuracy: 0.9874326750448833 tags: !NLPIndex","title":"Spam Detection Jupyter Notebook"},{"location":"Knowledge%20Base/All%20Semesters/Semester%203/Natural%20Language%20Processing/Spam%20Detection%20Jupyer%20Notebook.html#spam-detection-jupyter-notebook","text":"import pandas as pd sms_spam = pd . read_csv ( 'SMSSpamCollection' , sep = ' \\t ' , header = None , names = [ 'Label' , 'SMS' ]) print ( sms_spam . shape ) sms_spam . head () (5572, 2) Label SMS 0 ham Go until jurong point, crazy.. Available only ... 1 ham Ok lar... Joking wif u oni... 2 spam Free entry in 2 a wkly comp to win FA Cup fina... 3 ham U dun say so early hor... U c already then say... 4 ham Nah I don't think he goes to usf, he lives aro... sms_spam [ 'Label' ] . value_counts ( normalize = True ) ham 0.865937 spam 0.134063 Name: Label, dtype: float64 # Randomize the dataset data_randomized = sms_spam . sample ( frac = 1 , random_state = 1 ) # Calculate index for split training_test_index = round ( len ( data_randomized ) * 0.8 ) # Split into training and test sets training_set = data_randomized [: training_test_index ] . reset_index ( drop = True ) test_set = data_randomized [ training_test_index :] . reset_index ( drop = True ) print ( training_set . shape ) print ( test_set . shape ) (4458, 2) (1114, 2) training_set [ 'Label' ] . value_counts ( normalize = True ) ham 0.86541 spam 0.13459 Name: Label, dtype: float64 test_set [ 'Label' ] . value_counts ( normalize = True ) ham 0.868043 spam 0.131957 Name: Label, dtype: float64 # Before cleaning training_set . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS 0 ham Yep, by the pretty sculpture 1 ham Yes, princess. Are you going to make me moan? 2 ham Welp apparently he retired # After cleaning training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . replace ( '\\W' , ' ' ) # Removes punctuation training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . lower () training_set . head ( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS 0 ham yep by the pretty sculpture 1 ham yes princess are you going to make me moan 2 ham welp apparently he retired training_set [ 'SMS' ] = training_set [ 'SMS' ] . str . split () vocabulary = [] for sms in training_set [ 'SMS' ]: for word in sms : vocabulary . append ( word ) vocabulary = list ( set ( vocabulary )) len ( vocabulary ) 7783 word_counts_per_sms = { 'secret' : [ 2 , 1 , 1 ], 'prize' : [ 2 , 0 , 1 ], 'claim' : [ 1 , 0 , 1 ], 'now' : [ 1 , 0 , 1 ], 'coming' : [ 0 , 1 , 0 ], 'to' : [ 0 , 1 , 0 ], 'my' : [ 0 , 1 , 0 ], 'party' : [ 0 , 1 , 0 ], 'winner' : [ 0 , 0 , 1 ] } word_counts = pd . DataFrame ( word_counts_per_sms ) word_counts . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } secret prize claim now coming to my party winner 0 2 2 1 1 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 2 1 1 1 1 0 0 0 0 1 word_counts_per_sms = { unique_word : [ 0 ] * len ( training_set [ 'SMS' ]) for unique_word in vocabulary } for index , sms in enumerate ( training_set [ 'SMS' ]): for word in sms : word_counts_per_sms [ word ][ index ] += 1 word_counts = pd . DataFrame ( word_counts_per_sms ) word_counts . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } indyarocks port eg surly trained voda organise vu leg 06 ... imposter gee 4882 reduce sexual nange conducts noworriesloans taxes book 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows \u00d7 7783 columns training_set_clean = pd . concat ([ training_set , word_counts ], axis = 1 ) training_set_clean . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS indyarocks port eg surly trained voda organise vu ... imposter gee 4882 reduce sexual nange conducts noworriesloans taxes book 0 ham [yep, by, the, pretty, sculpture] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 ham [yes, princess, are, you, going, to, make, me,... 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 ham [welp, apparently, he, retired] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 ham [havent] 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 ham [i, forgot, 2, ask, \u00fc, all, smth, there, s, a,... 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 5 rows \u00d7 7785 columns # Isolating spam and ham messages first spam_messages = training_set_clean [ training_set_clean [ 'Label' ] == 'spam' ] ham_messages = training_set_clean [ training_set_clean [ 'Label' ] == 'ham' ] # P(Spam) and P(Ham) p_spam = len ( spam_messages ) / len ( training_set_clean ) p_ham = len ( ham_messages ) / len ( training_set_clean ) # N_Spam n_words_per_spam_message = spam_messages [ 'SMS' ] . apply ( len ) n_spam = n_words_per_spam_message . sum () # N_Ham n_words_per_ham_message = ham_messages [ 'SMS' ] . apply ( len ) n_ham = n_words_per_ham_message . sum () # N_Vocabulary n_vocabulary = len ( vocabulary ) # Laplace smoothing alpha = 1 # Initiate parameters parameters_spam = { unique_word : 0 for unique_word in vocabulary } parameters_ham = { unique_word : 0 for unique_word in vocabulary } # Calculate parameters for word in vocabulary : n_word_given_spam = spam_messages [ word ] . sum () # spam_messages already defined p_word_given_spam = ( n_word_given_spam + alpha ) / ( n_spam + alpha * n_vocabulary ) parameters_spam [ word ] = p_word_given_spam n_word_given_ham = ham_messages [ word ] . sum () # ham_messages already defined p_word_given_ham = ( n_word_given_ham + alpha ) / ( n_ham + alpha * n_vocabulary ) parameters_ham [ word ] = p_word_given_ham import re def classify ( message ): ''' message: a string ''' message = re . sub ( '\\W' , ' ' , message ) message = message . lower () . split () p_spam_given_message = p_spam p_ham_given_message = p_ham for word in message : if word in parameters_spam : p_spam_given_message *= parameters_spam [ word ] if word in parameters_ham : p_ham_given_message *= parameters_ham [ word ] print ( 'P(Spam|message):' , p_spam_given_message ) print ( 'P(Ham|message):' , p_ham_given_message ) if p_ham_given_message > p_spam_given_message : print ( 'Label: Ham' ) elif p_ham_given_message < p_spam_given_message : print ( 'Label: Spam' ) else : print ( 'Equal proabilities, have a human classify this!' ) classify ( 'WINNER!! This is the secret code to unlock the money: C3421.' ) P(Spam|message): 1.3481290211300841e-25 P(Ham|message): 1.9368049028589875e-27 Label: Spam classify ( \"Sounds good, Tom, then see u there\" ) P(Spam|message): 2.4372375665888117e-25 P(Ham|message): 3.687530435009238e-21 Label: Ham def classify_test_set ( message ): ''' message: a string ''' message = re . sub ( '\\W' , ' ' , message ) message = message . lower () . split () p_spam_given_message = p_spam p_ham_given_message = p_ham for word in message : if word in parameters_spam : p_spam_given_message *= parameters_spam [ word ] if word in parameters_ham : p_ham_given_message *= parameters_ham [ word ] if p_ham_given_message > p_spam_given_message : return 'ham' elif p_spam_given_message > p_ham_given_message : return 'spam' else : return 'needs human classification' test_set [ 'predicted' ] = test_set [ 'SMS' ] . apply ( classify_test_set ) test_set . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Label SMS predicted 0 ham Later i guess. I needa do mcat study too. ham 1 ham But i haf enuff space got like 4 mb... ham 2 spam Had your mobile 10 mths? Update to latest Oran... spam 3 ham All sounds good. Fingers . Makes it difficult ... ham 4 ham All done, all handed in. Don't know if mega sh... ham correct = 0 total = test_set . shape [ 0 ] for row in test_set . iterrows (): row = row [ 1 ] if row [ 'Label' ] == row [ 'predicted' ]: correct += 1 print ( 'Correct:' , correct ) print ( 'Incorrect:' , total - correct ) print ( 'Accuracy:' , correct / total ) Correct: 1100 Incorrect: 14 Accuracy: 0.9874326750448833 tags: !NLPIndex","title":"Spam Detection Jupyter Notebook"}]}